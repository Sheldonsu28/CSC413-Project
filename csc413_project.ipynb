{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc413_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECtlZMbXHxLd",
        "outputId": "29f311de-870a-46c4-a37e-713697617cea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/hr\n",
        "!rm -rf /content/1.png\n",
        "!rm -rf /content/hr_temp\n",
        "!rm -rf /content/temp1\n",
        "!rm -rf /content/temp2"
      ],
      "metadata": {
        "id": "FOMahyixf7L6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/hr /content/hr\n",
        "!cp /content/drive/MyDrive/1.png /content/1.png\n",
        "!cp -r /content/drive/MyDrive/hr_temp /content/hr_temp"
      ],
      "metadata": {
        "id": "thHq5jZCNfVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856c0e67-b54c-446c-e3dd-cef078232b63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/1.png': No such file or directory\n",
            "cp: cannot stat '/content/drive/MyDrive/hr_temp': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import cv2 as cv\n",
        "import torch\n",
        "import imageio\n"
      ],
      "metadata": {
        "id": "4TqWmtSSKFKv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "2FJOJzMsc7nt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMk3C0qOHjv6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def calc_output_size(H, kernel_size, padding=0, dilation=1, stride=1):\n",
        "    return ((H + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
        "\n",
        "\n",
        "# Create annotation file\n",
        "def build_index_file(dir=\"hr\"):\n",
        "    file_names = [[filename] for filename in os.listdir(dir)]\n",
        "\n",
        "    np.savetxt(dir + \"/\" + dir + \".csv\",\n",
        "               file_names,\n",
        "               delimiter=\", \",\n",
        "               fmt='% s',\n",
        "               encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def crop_images(w, h, dir=\"hr\"):\n",
        "    file_names = [filename for filename in os.listdir(dir) if '.csv' not in filename]\n",
        "    folder_hr = 'hr'\n",
        "    for file in file_names:\n",
        "        image = cv.imread(os.path.join(dir, file))\n",
        "        img_size = image.shape\n",
        "        x = img_size[1] / 2 - w / 2\n",
        "        y = img_size[0] / 2 - h / 2\n",
        "        crop_img = image[int(y):int(y + h), int(x):int(x + w)]\n",
        "        cv.imwrite(os.path.join(folder_hr, file), crop_img)\n",
        "    build_index_file(dir=\"hr\")\n",
        "\n",
        "\n",
        "def to_var(tensor, device):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "\n",
        "    return Variable(tensor.float()).cuda(device)\n",
        "\n",
        "\n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "def create_image_grid(array, ncols=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_images, channels, cell_h, cell_w = array.shape\n",
        "    if not ncols:\n",
        "        ncols = int(np.sqrt(num_images))\n",
        "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
        "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
        "    for i in range(0, nrows):\n",
        "        for j in range(0, ncols):\n",
        "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
        "                                                                                                                 0)\n",
        "\n",
        "    if channels == 1:\n",
        "        result = result.squeeze()\n",
        "    return result\n",
        "\n",
        "\n",
        "def gan_save_samples(data, iteration, opts):\n",
        "    generated_images = to_data(data)\n",
        "\n",
        "    grid = create_image_grid(generated_images)\n",
        "\n",
        "    # merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
        "    imageio.imwrite(path, grid)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "\n",
        "\n",
        "# a = calc_output_size(256, 9, stride=1, padding=4)\n",
        "# b = calc_output_size(a, 5, stride=1, padding=2)\n",
        "# print(calc_output_size(b, 5, stride=1, padding=2))\n",
        "#crop_images(128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESRGAN"
      ],
      "metadata": {
        "id": "DQHcRoG1c9Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cat\n",
        "from torch import nn\n",
        "from torch import flatten\n",
        "from torch import add\n",
        "\n",
        "\"\"\"\n",
        "Reference:\n",
        "https://arxiv.org/pdf/1809.00219v2.pdf\n",
        "\"\"\"\n",
        "global_beta = 0.2\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, (3, 3), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.RRDB_layers = nn.Sequential(*[RRDB(64, 32, global_beta) for i in range(16)])\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.upSample0 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64 * 4, (1, 1), stride=(stride, stride)),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, channels, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.RRDB_layers(x1)\n",
        "        x3 = add(self.conv1(x2), x1)\n",
        "        x4 = self.upSample0(x3)\n",
        "        x5 = self.conv2(x4)\n",
        "        return self.conv3(x5)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=padding,\n",
        "                      bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.basicBlocks = nn.Sequential(\n",
        "            DiscriminatorBlock(64, 64, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(64, 128, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(128, 128, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(128, 256, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(256, 256, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(256, 512, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=4, stride=2, padding=padding),\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4, 100),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer1 = self.block1(x)\n",
        "        block_out = self.basicBlocks(layer1)\n",
        "        flattened = flatten(block_out, 1)\n",
        "        return self.block2(flattened)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size),\n",
        "                      stride=(stride, stride), padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block1(x)\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, channels, growth_rate, beta):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = DenseBlock(channels, growth_rate)\n",
        "        self.block1 = DenseBlock(channels, growth_rate)\n",
        "        self.block2 = DenseBlock(channels, growth_rate)\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block0(x)\n",
        "        skip0 = self.beta * x1 + x\n",
        "\n",
        "        x2 = self.block1(skip0)\n",
        "        skip1 = self.beta * x2 + skip0\n",
        "\n",
        "        x3 = self.block2(skip1)\n",
        "        return (skip1 + self.beta * x3) * self.beta + x\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Follows the design used in: https://arxiv.org/pdf/1809.00219v2.pdf\n",
        "    Original design: https://arxiv.org/pdf/1608.06993v5.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, growth_rate, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Conv2d(channels, growth_rate, (kernel_size, kernel_size),\n",
        "                               (stride, stride), padding)\n",
        "        self.conv1 = nn.Conv2d(channels + growth_rate, growth_rate, (kernel_size, kernel_size),\n",
        "                               (stride, stride), padding)\n",
        "        self.conv2 = nn.Conv2d(channels + 2 * growth_rate, growth_rate, (kernel_size, kernel_size),\n",
        "                               (stride, stride), padding)\n",
        "        self.conv3 = nn.Conv2d(channels + 3 * growth_rate, growth_rate, (kernel_size, kernel_size),\n",
        "                               (stride, stride), padding)\n",
        "        self.conv4 = nn.Conv2d(channels + 3 * growth_rate, channels, (kernel_size, kernel_size),\n",
        "                               (stride, stride), padding)\n",
        "\n",
        "        self.LReLu = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.LReLu(self.conv0(x))\n",
        "        x2 = self.LReLu(self.conv1(cat((x, x1), 1)))\n",
        "        x3 = self.LReLu(self.conv2(cat((x, x1, x2), 1)))\n",
        "        x4 = self.LReLu(self.conv3(cat((x, x1, x2, x3), 1)))\n",
        "        return self.conv4(cat((x, x1, x2, x4), 1))\n"
      ],
      "metadata": {
        "id": "Dne005dwKjPW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SRCNN"
      ],
      "metadata": {
        "id": "d0jBRIUFdDLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reference:\n",
        "https://arxiv.org/pdf/1501.00092.pdf\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self, channel, f1=9, f2=5, f3=5, n1=64, n2=32):\n",
        "\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.channel = channel\n",
        "        self.block1 = nn.Conv2d(channel, n1, kernel_size=(f1, f1), bias=True, padding=f1 // 2)\n",
        "        self.block2 = nn.Conv2d(n1, n2, kernel_size=(f2, f2), bias=True, padding=f2 // 2)\n",
        "        self.block3 = nn.Conv2d(n2, channel, kernel_size=(f3, f3), bias=True, padding=f3 // 2)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        # self.upSample0 = nn.Sequential(\n",
        "        #     nn.Conv2d(3, 3 * 4, (1, 1)),\n",
        "        #     nn.PixelShuffle(2),\n",
        "        #     nn.ReLU(inplace=True)\n",
        "        # )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.activation(self.block1(x))\n",
        "        x2 = self.activation(self.block2(x1))\n",
        "        return self.block3(x2)\n"
      ],
      "metadata": {
        "id": "lKGPmsnMK3Pm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "PT2mWll0dGrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.cuda import amp\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "class CustomData(Dataset):\n",
        "    def __init__(self, model='SRCNN'):\n",
        "        self.names = np.loadtxt('hr/hr.csv', dtype='str', delimiter=\", \",\n",
        "                                encoding=\"utf-8\")\n",
        "\n",
        "        self.names = [name for name in self.names if \"csv\" not in name]\n",
        "        \n",
        "        self.hrs = [read_image(os.path.join('hr', name)).float() / 255.0 for name in self.names]\n",
        "        if model == 'SRCNN':\n",
        "          self.lr_transform = transforms.Compose([transforms.Resize((64, 64)), \n",
        "                                                  transforms.Resize((128, 128))])\n",
        "        else:\n",
        "          self.lr_transform = transforms.Compose([transforms.Resize((64, 64))])\n",
        "        self.lrs = [self.lr_transform(img) for img in self.hrs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.lrs[idx], self.hrs[idx]\n",
        "\n",
        "\n",
        "def train_SRCNN(args):\n",
        "    \"\"\"\n",
        "    Train SRCNN model\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Using device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "    model = SRCNN(args['out_channels'])\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda(device)\n",
        "\n",
        "    model.train()\n",
        "    params = model.parameters()\n",
        "    optimizer = Adam(params, args['learning_rate'], args['betas'])\n",
        "    # train with mse loss\n",
        "    loss = nn.MSELoss()\n",
        "    dataloader = DataLoader(CustomData(), batch_size=args['batch_size'], shuffle=True, num_workers=0,\n",
        "                            pin_memory=True)\n",
        "    train_iter = iter(dataloader)\n",
        "    train_loss = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    for i in range(args['epochs']):\n",
        "        try:\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        except StopIteration:\n",
        "            train_iter = iter(dataloader)\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with amp.autocast():\n",
        "          g = model(lr)\n",
        "          l = loss(g, hr)\n",
        "        \n",
        "        scaler.scale(l).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # l.backward()\n",
        "        # optimizer.step()\n",
        "        train_loss.append(l.item())\n",
        "\n",
        "        if l.item() < best_loss:\n",
        "          best_loss = l.item()\n",
        "          best_model = model.state_dict()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration {}/{}, training loss: {}'.format(i, args['epochs'], l))\n",
        "\n",
        "    plt.plot([i for i in range(args['epochs'])], train_loss)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('SRCNN.png')\n",
        "    pt_name = \"SRCNN_lr_{}.pt\".format(args['learning_rate'])\n",
        "    torch.save(best_model, pt_name)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    model.load_state_dict(torch.load(pt_name))\n",
        "    with torch.no_grad():\n",
        "        img = read_image('hr/0001.png')\n",
        "        # plt.figure(figsize=(10, 10))\n",
        "        # plt.imshow(img.T.numpy().astype(\"uint8\"))\n",
        "        # plt.axis(\"off\")\n",
        "        lr_transform = transforms.Compose([transforms.Resize((64, 64)), \n",
        "                                                  transforms.Resize((128, 128))])\n",
        "        tensor = lr_transform(torch.tensor(img / 255.))\n",
        "        out = model(to_var(tensor[None, :, :, :], device))\n",
        "        out = out.squeeze(0).cpu().detach()\n",
        "        save_image(out, \"SRCNN_out.png\")\n",
        "\n",
        "\n",
        "def train_ESRGAN(args):\n",
        "    \"\"\"\n",
        "    Train ESRGAN model\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Using device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "    G = Generator(args['out_channels'])\n",
        "    D = Discriminator(args['out_channels'])\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda(device)\n",
        "        D.cuda(device)\n",
        "\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "    params_G = G.parameters()\n",
        "    params_D = D.parameters()\n",
        "    num_params_G = sum(p.numel() for p in G.parameters() if p.requires_grad)\n",
        "    num_params_D = sum(p.numel() for p in D.parameters() if p.requires_grad)\n",
        "    print(\"Number of parameters in G: {}\".format(num_params_G))\n",
        "    print(\"Number of parameters in D: {}\".format(num_params_D))\n",
        "\n",
        "    optimizer_G = Adam(params_G, args['learning_rate'], args['betas'])\n",
        "    optimizer_D = Adam(params_D, args['learning_rate'], args['betas'])\n",
        "\n",
        "    D_loss_func = nn.BCEWithLogitsLoss()\n",
        "    G_pixel_loss_func = nn.MSELoss()\n",
        "    G_adversarial_loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Set up weights\n",
        "    adv_weight = args['adversarial_weight']\n",
        "    pixel_weight = args['pixel_weight']\n",
        "\n",
        "    dataloader = DataLoader(CustomData(model='ESRGAN'), batch_size=args['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    G_pixel_losses = []\n",
        "    G_losses_total = []\n",
        "    G_adversarial_losses = []\n",
        "    D_losses = []\n",
        "    D_fake_losses = []\n",
        "    D_real_losses = []\n",
        "\n",
        "    # Store the best Model\n",
        "    best_G = None\n",
        "    best_D = None\n",
        "    best_G_loss = float('inf')\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    for i in range(args['epochs']):\n",
        "        try:\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(dataloader)\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        # D loss (Relativistic Loss)\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        with amp.autocast():\n",
        "          fake_img = G(lr)\n",
        "\n",
        "          predict_fake = D(fake_img.detach())\n",
        "          predict_real = D(hr)\n",
        "\n",
        "          fake_loss = D_loss_func(predict_real - torch.mean(predict_fake), torch.ones_like(predict_real))\n",
        "          real_loss = D_loss_func(predict_fake - torch.mean(predict_real), torch.zeros_like(predict_fake))\n",
        "\n",
        "          # Code segment from PA4 DCGAN\n",
        "          # ---- Gradient Penalty ----\n",
        "          # if args['gradient_penalty']:\n",
        "            \n",
        "          #   alpha = torch.rand(hr.shape[0], 1, 1, 1)\n",
        "          #   alpha = alpha.expand_as(hr).cuda()\n",
        "          #   interp_images = Variable(alpha * hr.data + (1 - alpha) * fake_img.data,\n",
        "          #                           requires_grad=True).cuda()\n",
        "            \n",
        "          #   D_interp_output = D(interp_images)\n",
        "\n",
        "          #   gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
        "          #                                   grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
        "          #                                   create_graph=True, retain_graph=True)[0]\n",
        "            \n",
        "          #   gradients = gradients /scaler.get_scale()\n",
        "          #   gradients = gradients.view(hr.shape[0], -1)\n",
        "          #   gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "          #   gp = gradients_norm.mean()\n",
        "          # else:\n",
        "          #     gp = 0.0\n",
        "\n",
        "          d_loss = fake_loss + real_loss\n",
        "\n",
        "        scaler.scale(d_loss).backward()\n",
        "        scaler.step(optimizer_D)\n",
        "        # scaler.update()\n",
        "\n",
        "        # d_loss.backward()\n",
        "        # optimizer_D.step()\n",
        "\n",
        "        # G loss\n",
        "        optimizer_G.zero_grad()\n",
        "        with amp.autocast():\n",
        "          fake_img = G(lr)\n",
        "          score_r = D(hr.detach())\n",
        "          score_f = D(fake_img)\n",
        "\n",
        "          adv_loss = G_adversarial_loss_func(score_f - torch.mean(score_r), torch.ones_like(score_r))\n",
        "\n",
        "          g_pixel_loss = G_pixel_loss_func(fake_img, hr)\n",
        "\n",
        "          g_loss = adv_weight * adv_loss + pixel_weight * g_pixel_loss\n",
        "\n",
        "        scaler.scale(g_loss).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "        scaler.update()\n",
        "\n",
        "        # g_loss.backward()\n",
        "        # optimizer_G.step()\n",
        "\n",
        "        G_losses_total.append(g_loss.item())\n",
        "        D_fake_losses.append(fake_loss.item())\n",
        "        D_real_losses.append(real_loss.item())\n",
        "\n",
        "        G_pixel_losses.append(g_pixel_loss.item())\n",
        "        G_adversarial_losses.append(adv_loss.item())\n",
        "        D_losses.append(d_loss.item())\n",
        "\n",
        "        if g_loss.item() < best_G_loss:\n",
        "          best_G_loss = g_loss.item()\n",
        "          best_D = D.state_dict()\n",
        "          best_G = G.state_dict()\n",
        "\n",
        "        # if i % 100 == 0:\n",
        "        print('Iteration {}/{}, G loss: {}, D loss: {}'.format(i, args['epochs'], g_loss, d_loss))\n",
        "\n",
        "    x_axis = [i for i in range(args['epochs'])]\n",
        "    # plt.plot(x_axis, G_pixel_losses, label='G pixel loss')\n",
        "    plt.plot(x_axis, G_adversarial_losses, label='G loss' )\n",
        "    plt.plot(x_axis, D_losses, label='D loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('ESRGAN.png')\n",
        "\n",
        "    G_weights_pt = \"ESRGAN_G_lr_{}.pt\".format(args['learning_rate'])\n",
        "    D_weights_pt = \"ESRGAN_D_lr_{}.pt\".format(args['learning_rate'])\n",
        "    torch.save(best_G, G_weights_pt)\n",
        "    torch.save(best_D, D_weights_pt)\n",
        "\n",
        "    G.load_state_dict(torch.load(G_weights_pt))\n",
        "    D.load_state_dict(torch.load(D_weights_pt))\n",
        "\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    \n",
        "\n",
        "    np.savetxt(\"G_losses_total.csv\", np.asarray(G_losses_total), delimiter=\",\")\n",
        "    np.savetxt(\"D_fake_losses.csv\", np.asarray(D_fake_losses), delimiter=\",\")\n",
        "    np.savetxt(\"D_real_losses.csv\", np.asarray(D_real_losses), delimiter=\",\")\n",
        "\n",
        "    np.savetxt(\"G_pixel_losses.csv\", np.asarray(G_pixel_losses), delimiter=\",\")\n",
        "    np.savetxt(\"G_adversarial_losses.csv\", np.asarray(G_adversarial_losses), delimiter=\",\")\n",
        "    np.savetxt(\"D_losses.csv\", np.asarray(D_losses), delimiter=\",\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img = read_image('hr/0001.png')\n",
        "        # plt.figure(figsize=(10, 10))\n",
        "        # plt.imshow(img.T.numpy().astype(\"uint8\"))\n",
        "        # plt.axis(\"off\")\n",
        "        lr_transform = transforms.Compose([transforms.Resize((64, 64))])\n",
        "        tensor = lr_transform(torch.tensor(img / 255.))\n",
        "        out = G(to_var(tensor[None, :, :, :], device))\n",
        "        \n",
        "        out = out.squeeze(0).cpu().detach()\n",
        "        save_image(out, \"ESRGAN_out.png\")\n"
      ],
      "metadata": {
        "id": "XaTiViEwK5g4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {'out_channels': 3,\n",
        "        'batch_size': 20,\n",
        "        'learning_rate': 1e-4,\n",
        "        'epochs': 1000,\n",
        "        'betas': [0.9, 0.999],\n",
        "        # 'gradient_penalty': True,  # This field is not used in SRCNN training\n",
        "        'pixel_weight': 1.0,       \n",
        "        'adversarial_weight': 0.001 # This field is not used in SRCNN training\n",
        "        }\n"
      ],
      "metadata": {
        "id": "BFIXKKMzSZvk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_SRCNN(args)"
      ],
      "metadata": {
        "id": "sD6HCrCXSdbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "3f70ba65-461e-40fd-fd6e-46a6ecd087e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: Tesla T4\n",
            "Iteration 0/1000, training loss: 0.3227791488170624\n",
            "Iteration 100/1000, training loss: 0.007947875186800957\n",
            "Iteration 200/1000, training loss: 0.0035071843303740025\n",
            "Iteration 300/1000, training loss: 0.0030675113666802645\n",
            "Iteration 400/1000, training loss: 0.0014530635671690106\n",
            "Iteration 500/1000, training loss: 0.003244096180424094\n",
            "Iteration 600/1000, training loss: 0.002804366871714592\n",
            "Iteration 700/1000, training loss: 0.002872740849852562\n",
            "Iteration 800/1000, training loss: 0.003488966729491949\n",
            "Iteration 900/1000, training loss: 0.0011524069122970104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_ESRGAN(args)"
      ],
      "metadata": {
        "id": "rJaNSof0SeZi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35f0f696-bdc9-43b2-d119-716ce36661f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: Tesla T4\n",
            "Number of parameters in G: 10720067\n",
            "Number of parameters in D: 14499401\n",
            "Iteration 0/1000, G loss: 0.16576668620109558, D loss: 1.365358829498291\n",
            "Iteration 1/1000, G loss: 0.1098598763346672, D loss: 0.7451392412185669\n",
            "Iteration 2/1000, G loss: 0.06402970850467682, D loss: 0.3417656123638153\n",
            "Iteration 3/1000, G loss: 0.06244361773133278, D loss: 0.10645346343517303\n",
            "Iteration 4/1000, G loss: 0.04838520288467407, D loss: 0.021149445325136185\n",
            "Iteration 5/1000, G loss: 0.04566560313105583, D loss: 0.00623792689293623\n",
            "Iteration 6/1000, G loss: 0.040545202791690826, D loss: 0.0036262478679418564\n",
            "Iteration 7/1000, G loss: 0.03546980023384094, D loss: 0.00252768793143332\n",
            "Iteration 8/1000, G loss: 0.03761579468846321, D loss: 0.0032378854230046272\n",
            "Iteration 9/1000, G loss: 0.02878071367740631, D loss: 0.0014222918543964624\n",
            "Iteration 10/1000, G loss: 0.03633720427751541, D loss: 0.0015611168928444386\n",
            "Iteration 11/1000, G loss: 0.037422262132167816, D loss: 0.0005202707252465189\n",
            "Iteration 12/1000, G loss: 0.03635990619659424, D loss: 0.00030858698301017284\n",
            "Iteration 13/1000, G loss: 0.03150878846645355, D loss: 0.0002617585123516619\n",
            "Iteration 14/1000, G loss: 0.032061655074357986, D loss: 0.00017957718227989972\n",
            "Iteration 15/1000, G loss: 0.028149381279945374, D loss: 0.0015405139420181513\n",
            "Iteration 16/1000, G loss: 0.02529236115515232, D loss: 0.00029245216865092516\n",
            "Iteration 17/1000, G loss: 0.02947542443871498, D loss: 0.0005923992721363902\n",
            "Iteration 18/1000, G loss: 0.029400385916233063, D loss: 0.0007694120868109167\n",
            "Iteration 19/1000, G loss: 0.027698364108800888, D loss: 0.0019133992027491331\n",
            "Iteration 20/1000, G loss: 0.02424640953540802, D loss: 0.0006915247649885714\n",
            "Iteration 21/1000, G loss: 0.025968536734580994, D loss: 7.490857387892902e-05\n",
            "Iteration 22/1000, G loss: 0.02624233067035675, D loss: 0.000821987516246736\n",
            "Iteration 23/1000, G loss: 0.02753375470638275, D loss: 0.00017139875853899866\n",
            "Iteration 24/1000, G loss: 0.022643152624368668, D loss: 0.00024285231484100223\n",
            "Iteration 25/1000, G loss: 0.02808954194188118, D loss: 0.0001870679552666843\n",
            "Iteration 26/1000, G loss: 0.02791408635675907, D loss: 0.002327143447473645\n",
            "Iteration 27/1000, G loss: 0.02248968370258808, D loss: 0.0001284823811147362\n",
            "Iteration 28/1000, G loss: 0.025720657780766487, D loss: 0.002299252664670348\n",
            "Iteration 29/1000, G loss: 0.02587287127971649, D loss: 0.00013632966147270054\n",
            "Iteration 30/1000, G loss: 0.02721412293612957, D loss: 0.00015048828208819032\n",
            "Iteration 31/1000, G loss: 0.02303842455148697, D loss: 9.121358743868768e-05\n",
            "Iteration 32/1000, G loss: 0.02368045784533024, D loss: 0.00015433989756274968\n",
            "Iteration 33/1000, G loss: 0.0260995551943779, D loss: 5.683305789716542e-05\n",
            "Iteration 34/1000, G loss: 0.02970733866095543, D loss: 0.00032696721609681845\n",
            "Iteration 35/1000, G loss: 0.02098892815411091, D loss: 0.0005357663030736148\n",
            "Iteration 36/1000, G loss: 0.02348184585571289, D loss: 0.0002682355116121471\n",
            "Iteration 37/1000, G loss: 0.022391019389033318, D loss: 0.00010688559268601239\n",
            "Iteration 38/1000, G loss: 0.023152921348810196, D loss: 0.0005934758228249848\n",
            "Iteration 39/1000, G loss: 0.020430300384759903, D loss: 0.0010978961363434792\n",
            "Iteration 40/1000, G loss: 0.022702058777213097, D loss: 0.0010253553045913577\n",
            "Iteration 41/1000, G loss: 0.02198665961623192, D loss: 0.0007838821620680392\n",
            "Iteration 42/1000, G loss: 0.02423487789928913, D loss: 0.0018358794040977955\n",
            "Iteration 43/1000, G loss: 0.01831916719675064, D loss: 0.003097696229815483\n",
            "Iteration 44/1000, G loss: 0.02085617370903492, D loss: 0.012341712601482868\n",
            "Iteration 45/1000, G loss: 0.02086663991212845, D loss: 0.009715135209262371\n",
            "Iteration 46/1000, G loss: 0.01917247101664543, D loss: 0.00287324539385736\n",
            "Iteration 47/1000, G loss: 0.022290635854005814, D loss: 0.0016201313119381666\n",
            "Iteration 48/1000, G loss: 0.022096790373325348, D loss: 0.0004757549031637609\n",
            "Iteration 49/1000, G loss: 0.019416283816099167, D loss: 0.0013247625902295113\n",
            "Iteration 50/1000, G loss: 0.02466295287013054, D loss: 0.00040153003646992147\n",
            "Iteration 51/1000, G loss: 0.02095230482518673, D loss: 0.0013533489545807242\n",
            "Iteration 52/1000, G loss: 0.02031584456562996, D loss: 0.003968279343098402\n",
            "Iteration 53/1000, G loss: 0.02034623920917511, D loss: 0.0022331932559609413\n",
            "Iteration 54/1000, G loss: 0.021960048004984856, D loss: 0.008117010816931725\n",
            "Iteration 55/1000, G loss: 0.019089680165052414, D loss: 0.0015485838521271944\n",
            "Iteration 56/1000, G loss: 0.021073445677757263, D loss: 0.0004391190013848245\n",
            "Iteration 57/1000, G loss: 0.02013566344976425, D loss: 0.0024417974054813385\n",
            "Iteration 58/1000, G loss: 0.02059025503695011, D loss: 0.00548681290820241\n",
            "Iteration 59/1000, G loss: 0.022199735045433044, D loss: 0.02250397577881813\n",
            "Iteration 60/1000, G loss: 0.017620015889406204, D loss: 0.008321750909090042\n",
            "Iteration 61/1000, G loss: 0.021033016964793205, D loss: 0.04975208267569542\n",
            "Iteration 62/1000, G loss: 0.022772768512368202, D loss: 0.08034342527389526\n",
            "Iteration 63/1000, G loss: 0.020282521843910217, D loss: 0.002081468468531966\n",
            "Iteration 64/1000, G loss: 0.017682628706097603, D loss: 0.04881613329052925\n",
            "Iteration 65/1000, G loss: 0.015908975154161453, D loss: 0.3802648186683655\n",
            "Iteration 66/1000, G loss: 0.013804750517010689, D loss: 0.8072900772094727\n",
            "Iteration 67/1000, G loss: 0.011394796893000603, D loss: 1.5606553554534912\n",
            "Iteration 68/1000, G loss: 0.00983470305800438, D loss: 2.5008702278137207\n",
            "Iteration 69/1000, G loss: 0.012592529878020287, D loss: 4.350664138793945\n",
            "Iteration 70/1000, G loss: 0.015919942408800125, D loss: 1.2895616292953491\n",
            "Iteration 71/1000, G loss: 0.018801115453243256, D loss: 0.13388347625732422\n",
            "Iteration 72/1000, G loss: 0.016024954617023468, D loss: 0.018545007333159447\n",
            "Iteration 73/1000, G loss: 0.0196811705827713, D loss: 0.09574075788259506\n",
            "Iteration 74/1000, G loss: 0.01634894497692585, D loss: 0.03524643927812576\n",
            "Iteration 75/1000, G loss: 0.016318175941705704, D loss: 0.006510513368993998\n",
            "Iteration 76/1000, G loss: 0.01749289408326149, D loss: 0.001659574219956994\n",
            "Iteration 77/1000, G loss: 0.01868027076125145, D loss: 0.007506068795919418\n",
            "Iteration 78/1000, G loss: 0.015504224225878716, D loss: 0.00890979915857315\n",
            "Iteration 79/1000, G loss: 0.01506161317229271, D loss: 0.012557053938508034\n",
            "Iteration 80/1000, G loss: 0.013278232887387276, D loss: 0.02822634018957615\n",
            "Iteration 81/1000, G loss: 0.014496352523565292, D loss: 0.016556641086935997\n",
            "Iteration 82/1000, G loss: 0.012743495404720306, D loss: 0.05375000089406967\n",
            "Iteration 83/1000, G loss: 0.015389813110232353, D loss: 0.044274330139160156\n",
            "Iteration 84/1000, G loss: 0.013241374865174294, D loss: 0.030558574944734573\n",
            "Iteration 85/1000, G loss: 0.013209789991378784, D loss: 0.05439719185233116\n",
            "Iteration 86/1000, G loss: 0.01675797626376152, D loss: 0.13060002028942108\n",
            "Iteration 87/1000, G loss: 0.012552990578114986, D loss: 0.12251463532447815\n",
            "Iteration 88/1000, G loss: 0.014832548797130585, D loss: 0.5874319076538086\n",
            "Iteration 89/1000, G loss: 0.015625091269612312, D loss: 0.10484466701745987\n",
            "Iteration 90/1000, G loss: 0.015199698507785797, D loss: 0.2364349514245987\n",
            "Iteration 91/1000, G loss: 0.013665021397173405, D loss: 0.4334757626056671\n",
            "Iteration 92/1000, G loss: 0.01525229774415493, D loss: 0.6816538572311401\n",
            "Iteration 93/1000, G loss: 0.015580866485834122, D loss: 0.06266796588897705\n",
            "Iteration 94/1000, G loss: 0.016597401350736618, D loss: 0.01618039980530739\n",
            "Iteration 95/1000, G loss: 0.01642119139432907, D loss: 0.01389368437230587\n",
            "Iteration 96/1000, G loss: 0.01636354997754097, D loss: 0.03341047838330269\n",
            "Iteration 97/1000, G loss: 0.014856627210974693, D loss: 0.0163640808314085\n",
            "Iteration 98/1000, G loss: 0.01583952270448208, D loss: 0.09579542279243469\n",
            "Iteration 99/1000, G loss: 0.014341587200760841, D loss: 0.39268356561660767\n",
            "Iteration 100/1000, G loss: 0.012077970430254936, D loss: 0.7071224451065063\n",
            "Iteration 101/1000, G loss: 0.012618141248822212, D loss: 0.3350394368171692\n",
            "Iteration 102/1000, G loss: 0.01738395355641842, D loss: 0.18311427533626556\n",
            "Iteration 103/1000, G loss: 0.01600358448922634, D loss: 0.45969539880752563\n",
            "Iteration 104/1000, G loss: 0.02141275443136692, D loss: 0.04265506565570831\n",
            "Iteration 105/1000, G loss: 0.014978554099798203, D loss: 0.01619315706193447\n",
            "Iteration 106/1000, G loss: 0.018591545522212982, D loss: 0.03251513093709946\n",
            "Iteration 107/1000, G loss: 0.015306860208511353, D loss: 0.1525450199842453\n",
            "Iteration 108/1000, G loss: 0.016588695347309113, D loss: 0.06467632949352264\n",
            "Iteration 109/1000, G loss: 0.01737343892455101, D loss: 0.0022871685214340687\n",
            "Iteration 110/1000, G loss: 0.017502687871456146, D loss: 0.042149558663368225\n",
            "Iteration 111/1000, G loss: 0.017743723466992378, D loss: 0.01075606420636177\n",
            "Iteration 112/1000, G loss: 0.01694757491350174, D loss: 0.04862936586141586\n",
            "Iteration 113/1000, G loss: 0.015801453962922096, D loss: 0.06633204966783524\n",
            "Iteration 114/1000, G loss: 0.015503435395658016, D loss: 0.03214893117547035\n",
            "Iteration 115/1000, G loss: 0.01477866806089878, D loss: 0.2144784927368164\n",
            "Iteration 116/1000, G loss: 0.013794127851724625, D loss: 0.25576961040496826\n",
            "Iteration 117/1000, G loss: 0.013137211091816425, D loss: 0.14372648298740387\n",
            "Iteration 118/1000, G loss: 0.016201142221689224, D loss: 2.97135329246521\n",
            "Iteration 119/1000, G loss: 0.018335141241550446, D loss: 0.3938862681388855\n",
            "Iteration 120/1000, G loss: 0.017237603664398193, D loss: 0.21372821927070618\n",
            "Iteration 121/1000, G loss: 0.016100052744150162, D loss: 0.27251413464546204\n",
            "Iteration 122/1000, G loss: 0.015612410381436348, D loss: 0.11601412296295166\n",
            "Iteration 123/1000, G loss: 0.012965332716703415, D loss: 0.12272335588932037\n",
            "Iteration 124/1000, G loss: 0.01273312233388424, D loss: 0.9007581472396851\n",
            "Iteration 125/1000, G loss: 0.012268801219761372, D loss: 0.2289506494998932\n",
            "Iteration 126/1000, G loss: 0.014299148693680763, D loss: 0.08468236029148102\n",
            "Iteration 127/1000, G loss: 0.013005244545638561, D loss: 0.04191425070166588\n",
            "Iteration 128/1000, G loss: 0.01675708219408989, D loss: 0.17139580845832825\n",
            "Iteration 129/1000, G loss: 0.016700170934200287, D loss: 0.05149548873305321\n",
            "Iteration 130/1000, G loss: 0.015258442610502243, D loss: 0.013741554692387581\n",
            "Iteration 131/1000, G loss: 0.013915970921516418, D loss: 0.010384608991444111\n",
            "Iteration 132/1000, G loss: 0.017248664051294327, D loss: 0.013451467268168926\n",
            "Iteration 133/1000, G loss: 0.01915603131055832, D loss: 0.0009478413267061114\n",
            "Iteration 134/1000, G loss: 0.0185212604701519, D loss: 0.0012520186137408018\n",
            "Iteration 135/1000, G loss: 0.014717573300004005, D loss: 0.028353555127978325\n",
            "Iteration 136/1000, G loss: 0.01762225665152073, D loss: 0.003277949523180723\n",
            "Iteration 137/1000, G loss: 0.016202256083488464, D loss: 0.003288164734840393\n",
            "Iteration 138/1000, G loss: 0.01486384216696024, D loss: 0.004041195381432772\n",
            "Iteration 139/1000, G loss: 0.013864288106560707, D loss: 0.03948502615094185\n",
            "Iteration 140/1000, G loss: 0.013636274263262749, D loss: 0.01791911944746971\n",
            "Iteration 141/1000, G loss: 0.013855693861842155, D loss: 0.34119081497192383\n",
            "Iteration 142/1000, G loss: 0.013977586291730404, D loss: 0.038295578211545944\n",
            "Iteration 143/1000, G loss: 0.012248259037733078, D loss: 0.16831916570663452\n",
            "Iteration 144/1000, G loss: 0.014985949732363224, D loss: 0.18719659745693207\n",
            "Iteration 145/1000, G loss: 0.013285867869853973, D loss: 0.26759546995162964\n",
            "Iteration 146/1000, G loss: 0.016504110768437386, D loss: 0.10010120272636414\n",
            "Iteration 147/1000, G loss: 0.015334989875555038, D loss: 0.0048842113465070724\n",
            "Iteration 148/1000, G loss: 0.013915707357227802, D loss: 0.01631450466811657\n",
            "Iteration 149/1000, G loss: 0.013761322945356369, D loss: 0.06787782162427902\n",
            "Iteration 150/1000, G loss: 0.018266839906573296, D loss: 0.43867772817611694\n",
            "Iteration 151/1000, G loss: 0.017280137166380882, D loss: 0.016849800944328308\n",
            "Iteration 152/1000, G loss: 0.016106493771076202, D loss: 0.030235396698117256\n",
            "Iteration 153/1000, G loss: 0.01689860410988331, D loss: 0.008084416389465332\n",
            "Iteration 154/1000, G loss: 0.017437301576137543, D loss: 0.4165670573711395\n",
            "Iteration 155/1000, G loss: 0.020861530676484108, D loss: 0.020185932517051697\n",
            "Iteration 156/1000, G loss: 0.021115083247423172, D loss: 0.00018323904077988118\n",
            "Iteration 157/1000, G loss: 0.01583549752831459, D loss: 0.008261297829449177\n",
            "Iteration 158/1000, G loss: 0.015541594475507736, D loss: 0.014524612575769424\n",
            "Iteration 159/1000, G loss: 0.014591616578400135, D loss: 0.09295277297496796\n",
            "Iteration 160/1000, G loss: 0.0171094611287117, D loss: 0.848435640335083\n",
            "Iteration 161/1000, G loss: 0.017410460859537125, D loss: 0.19876840710639954\n",
            "Iteration 162/1000, G loss: 0.019210878759622574, D loss: 0.04657959192991257\n",
            "Iteration 163/1000, G loss: 0.018852178007364273, D loss: 1.348024845123291\n",
            "Iteration 164/1000, G loss: 0.017607981339097023, D loss: 0.5299254655838013\n",
            "Iteration 165/1000, G loss: 0.01632242649793625, D loss: 0.07529126107692719\n",
            "Iteration 166/1000, G loss: 0.019115621224045753, D loss: 0.012028983794152737\n",
            "Iteration 167/1000, G loss: 0.015294279903173447, D loss: 0.04410940036177635\n",
            "Iteration 168/1000, G loss: 0.015853265300393105, D loss: 0.06016930937767029\n",
            "Iteration 169/1000, G loss: 0.0165560320019722, D loss: 1.0639960765838623\n",
            "Iteration 170/1000, G loss: 0.019071921706199646, D loss: 0.004562768153846264\n",
            "Iteration 171/1000, G loss: 0.016350656747817993, D loss: 0.013160407543182373\n",
            "Iteration 172/1000, G loss: 0.016029521822929382, D loss: 0.0387934148311615\n",
            "Iteration 173/1000, G loss: 0.0164317786693573, D loss: 0.23379294574260712\n",
            "Iteration 174/1000, G loss: 0.020474081858992577, D loss: 0.012154458090662956\n",
            "Iteration 175/1000, G loss: 0.01629854179918766, D loss: 0.13771550357341766\n",
            "Iteration 176/1000, G loss: 0.016469962894916534, D loss: 0.0070116398856043816\n",
            "Iteration 177/1000, G loss: 0.01876882091164589, D loss: 0.0041710794903337955\n",
            "Iteration 178/1000, G loss: 0.017423417419195175, D loss: 0.010498380288481712\n",
            "Iteration 179/1000, G loss: 0.015058780089020729, D loss: 0.1249728873372078\n",
            "Iteration 180/1000, G loss: 0.016622547060251236, D loss: 0.05431004613637924\n",
            "Iteration 181/1000, G loss: 0.012695837765932083, D loss: 0.47379136085510254\n",
            "Iteration 182/1000, G loss: 0.013979483395814896, D loss: 0.7265453338623047\n",
            "Iteration 183/1000, G loss: 0.014912286773324013, D loss: 0.18088820576667786\n",
            "Iteration 184/1000, G loss: 0.01521205808967352, D loss: 0.05222725123167038\n",
            "Iteration 185/1000, G loss: 0.016695642843842506, D loss: 0.032856568694114685\n",
            "Iteration 186/1000, G loss: 0.015816133469343185, D loss: 0.003591713961213827\n",
            "Iteration 187/1000, G loss: 0.016256805509328842, D loss: 0.023065052926540375\n",
            "Iteration 188/1000, G loss: 0.018100835382938385, D loss: 0.14446303248405457\n",
            "Iteration 189/1000, G loss: 0.016112059354782104, D loss: 0.2245056927204132\n",
            "Iteration 190/1000, G loss: 0.014799006283283234, D loss: 0.13780477643013\n",
            "Iteration 191/1000, G loss: 0.015816643834114075, D loss: 0.47981584072113037\n",
            "Iteration 192/1000, G loss: 0.01729349046945572, D loss: 0.08724333345890045\n",
            "Iteration 193/1000, G loss: 0.016121935099363327, D loss: 0.4219781756401062\n",
            "Iteration 194/1000, G loss: 0.017445402219891548, D loss: 0.26210129261016846\n",
            "Iteration 195/1000, G loss: 0.008745613507926464, D loss: 4.27435302734375\n",
            "Iteration 196/1000, G loss: 0.012908246368169785, D loss: 12.390693664550781\n",
            "Iteration 197/1000, G loss: 0.010574661195278168, D loss: 1.6538060903549194\n",
            "Iteration 198/1000, G loss: 0.009924616664648056, D loss: 0.5759304761886597\n",
            "Iteration 199/1000, G loss: 0.01038656197488308, D loss: 1.1562960147857666\n",
            "Iteration 200/1000, G loss: 0.011898398399353027, D loss: 1.2146600484848022\n",
            "Iteration 201/1000, G loss: 0.011767668649554253, D loss: 0.5513750910758972\n",
            "Iteration 202/1000, G loss: 0.009732164442539215, D loss: 0.6388421058654785\n",
            "Iteration 203/1000, G loss: 0.010558299720287323, D loss: 0.300359845161438\n",
            "Iteration 204/1000, G loss: 0.009710727259516716, D loss: 0.34650543332099915\n",
            "Iteration 205/1000, G loss: 0.011407395824790001, D loss: 0.24917006492614746\n",
            "Iteration 206/1000, G loss: 0.009844793006777763, D loss: 0.4274669289588928\n",
            "Iteration 207/1000, G loss: 0.010707766748964787, D loss: 0.14042703807353973\n",
            "Iteration 208/1000, G loss: 0.010333179496228695, D loss: 0.1470632553100586\n",
            "Iteration 209/1000, G loss: 0.010353464633226395, D loss: 0.43727922439575195\n",
            "Iteration 210/1000, G loss: 0.00993239413946867, D loss: 0.11989268660545349\n",
            "Iteration 211/1000, G loss: 0.01131288893520832, D loss: 0.24528078734874725\n",
            "Iteration 212/1000, G loss: 0.010078789666295052, D loss: 0.2146649956703186\n",
            "Iteration 213/1000, G loss: 0.014077816158533096, D loss: 0.2104574590921402\n",
            "Iteration 214/1000, G loss: 0.012060767039656639, D loss: 0.46859845519065857\n",
            "Iteration 215/1000, G loss: 0.012465514242649078, D loss: 0.16763749718666077\n",
            "Iteration 216/1000, G loss: 0.01254931092262268, D loss: 0.049416955560445786\n",
            "Iteration 217/1000, G loss: 0.014797784388065338, D loss: 0.09063668549060822\n",
            "Iteration 218/1000, G loss: 0.010373104363679886, D loss: 0.09945095330476761\n",
            "Iteration 219/1000, G loss: 0.012948711402714252, D loss: 0.006385105662047863\n",
            "Iteration 220/1000, G loss: 0.01108538918197155, D loss: 0.05622919648885727\n",
            "Iteration 221/1000, G loss: 0.012232000008225441, D loss: 0.7641012668609619\n",
            "Iteration 222/1000, G loss: 0.014121752232313156, D loss: 0.048501163721084595\n",
            "Iteration 223/1000, G loss: 0.011332856491208076, D loss: 0.12316276133060455\n",
            "Iteration 224/1000, G loss: 0.013172630220651627, D loss: 0.026340389624238014\n",
            "Iteration 225/1000, G loss: 0.012055632658302784, D loss: 0.8134603500366211\n",
            "Iteration 226/1000, G loss: 0.011638404801487923, D loss: 0.23217995464801788\n",
            "Iteration 227/1000, G loss: 0.01307649165391922, D loss: 0.10353952646255493\n",
            "Iteration 228/1000, G loss: 0.01426110602915287, D loss: 0.05489078536629677\n",
            "Iteration 229/1000, G loss: 0.014708762057125568, D loss: 0.02338230423629284\n",
            "Iteration 230/1000, G loss: 0.015206433832645416, D loss: 0.03320727497339249\n",
            "Iteration 231/1000, G loss: 0.013920054771006107, D loss: 0.06007958948612213\n",
            "Iteration 232/1000, G loss: 0.01245591789484024, D loss: 0.21670123934745789\n",
            "Iteration 233/1000, G loss: 0.016302024945616722, D loss: 0.08080798387527466\n",
            "Iteration 234/1000, G loss: 0.014827940613031387, D loss: 1.3150379657745361\n",
            "Iteration 235/1000, G loss: 0.013644369319081306, D loss: 0.005302176345139742\n",
            "Iteration 236/1000, G loss: 0.014729572460055351, D loss: 0.010997830890119076\n",
            "Iteration 237/1000, G loss: 0.01162989903241396, D loss: 0.044361747801303864\n",
            "Iteration 238/1000, G loss: 0.01081656664609909, D loss: 0.05103769153356552\n",
            "Iteration 239/1000, G loss: 0.011710116639733315, D loss: 0.06235777214169502\n",
            "Iteration 240/1000, G loss: 0.013913556933403015, D loss: 0.014153139665722847\n",
            "Iteration 241/1000, G loss: 0.01150467898696661, D loss: 0.016294412314891815\n",
            "Iteration 242/1000, G loss: 0.013358371332287788, D loss: 0.04548512026667595\n",
            "Iteration 243/1000, G loss: 0.015569603070616722, D loss: 0.004806003998965025\n",
            "Iteration 244/1000, G loss: 0.013472985476255417, D loss: 0.009651859290897846\n",
            "Iteration 245/1000, G loss: 0.011051543056964874, D loss: 0.0898137167096138\n",
            "Iteration 246/1000, G loss: 0.015656691044569016, D loss: 0.639421820640564\n",
            "Iteration 247/1000, G loss: 0.012203514575958252, D loss: 7.389001846313477\n",
            "Iteration 248/1000, G loss: 0.011816846206784248, D loss: 2.2897439002990723\n",
            "Iteration 249/1000, G loss: 0.00841308943927288, D loss: 1.9951813220977783\n",
            "Iteration 250/1000, G loss: 0.006779103074222803, D loss: 0.9071310758590698\n",
            "Iteration 251/1000, G loss: 0.008820288814604282, D loss: 0.7802849411964417\n",
            "Iteration 252/1000, G loss: 0.006808960810303688, D loss: 1.1994409561157227\n",
            "Iteration 253/1000, G loss: 0.007972978986799717, D loss: 0.5783957839012146\n",
            "Iteration 254/1000, G loss: 0.00875561311841011, D loss: 0.7352257370948792\n",
            "Iteration 255/1000, G loss: 0.0067790113389492035, D loss: 0.9119054079055786\n",
            "Iteration 256/1000, G loss: 0.00817587599158287, D loss: 0.6290991306304932\n",
            "Iteration 257/1000, G loss: 0.00679679773747921, D loss: 0.6392718553543091\n",
            "Iteration 258/1000, G loss: 0.009398109279572964, D loss: 0.822796106338501\n",
            "Iteration 259/1000, G loss: 0.007958674803376198, D loss: 0.8193085193634033\n",
            "Iteration 260/1000, G loss: 0.00823544804006815, D loss: 0.7005971670150757\n",
            "Iteration 261/1000, G loss: 0.006637174636125565, D loss: 0.4730488061904907\n",
            "Iteration 262/1000, G loss: 0.0072019267827272415, D loss: 0.8333616852760315\n",
            "Iteration 263/1000, G loss: 0.00835484266281128, D loss: 0.6911557912826538\n",
            "Iteration 264/1000, G loss: 0.007268326357007027, D loss: 0.41955938935279846\n",
            "Iteration 265/1000, G loss: 0.008743878453969955, D loss: 0.886210560798645\n",
            "Iteration 266/1000, G loss: 0.007639969699084759, D loss: 1.235958456993103\n",
            "Iteration 267/1000, G loss: 0.007676607929170132, D loss: 0.6330400705337524\n",
            "Iteration 268/1000, G loss: 0.007621665485203266, D loss: 1.3211805820465088\n",
            "Iteration 269/1000, G loss: 0.008091326802968979, D loss: 1.544966697692871\n",
            "Iteration 270/1000, G loss: 0.009472286328673363, D loss: 0.6975245475769043\n",
            "Iteration 271/1000, G loss: 0.008020326495170593, D loss: 0.36398667097091675\n",
            "Iteration 272/1000, G loss: 0.008271103724837303, D loss: 0.5946686267852783\n",
            "Iteration 273/1000, G loss: 0.008647989481687546, D loss: 0.2661738991737366\n",
            "Iteration 274/1000, G loss: 0.007198610343039036, D loss: 0.6391350030899048\n",
            "Iteration 275/1000, G loss: 0.008052732795476913, D loss: 1.3519028425216675\n",
            "Iteration 276/1000, G loss: 0.007871072739362717, D loss: 1.3584645986557007\n",
            "Iteration 277/1000, G loss: 0.010968735441565514, D loss: 0.17119160294532776\n",
            "Iteration 278/1000, G loss: 0.009610412642359734, D loss: 0.129899799823761\n",
            "Iteration 279/1000, G loss: 0.010150889866054058, D loss: 0.3207360506057739\n",
            "Iteration 280/1000, G loss: 0.009928872808814049, D loss: 0.21206265687942505\n",
            "Iteration 281/1000, G loss: 0.011514660902321339, D loss: 0.07119151949882507\n",
            "Iteration 282/1000, G loss: 0.00942506268620491, D loss: 1.9362719058990479\n",
            "Iteration 283/1000, G loss: 0.007662951946258545, D loss: 0.7896779775619507\n",
            "Iteration 284/1000, G loss: 0.010017742402851582, D loss: 1.5926852226257324\n",
            "Iteration 285/1000, G loss: 0.008431987836956978, D loss: 0.9315080046653748\n",
            "Iteration 286/1000, G loss: 0.010905000381171703, D loss: 0.24860775470733643\n",
            "Iteration 287/1000, G loss: 0.00926287192851305, D loss: 0.3294294774532318\n",
            "Iteration 288/1000, G loss: 0.010122985579073429, D loss: 0.26325884461402893\n",
            "Iteration 289/1000, G loss: 0.009415436536073685, D loss: 0.12068309634923935\n",
            "Iteration 290/1000, G loss: 0.010316245257854462, D loss: 0.29706817865371704\n",
            "Iteration 291/1000, G loss: 0.009146654047071934, D loss: 0.21718715131282806\n",
            "Iteration 292/1000, G loss: 0.011916188523173332, D loss: 0.11185140907764435\n",
            "Iteration 293/1000, G loss: 0.012203197926282883, D loss: 0.020667212083935738\n",
            "Iteration 294/1000, G loss: 0.010283686220645905, D loss: 0.49227532744407654\n",
            "Iteration 295/1000, G loss: 0.009352643974125385, D loss: 0.27844059467315674\n",
            "Iteration 296/1000, G loss: 0.01049682404845953, D loss: 0.531804621219635\n",
            "Iteration 297/1000, G loss: 0.010528940707445145, D loss: 1.4397368431091309\n",
            "Iteration 298/1000, G loss: 0.011947087943553925, D loss: 0.38602569699287415\n",
            "Iteration 299/1000, G loss: 0.009949556551873684, D loss: 1.3637897968292236\n",
            "Iteration 300/1000, G loss: 0.011688599362969398, D loss: 0.5504743456840515\n",
            "Iteration 301/1000, G loss: 0.011368142440915108, D loss: 0.10773757100105286\n",
            "Iteration 302/1000, G loss: 0.014018969610333443, D loss: 0.03892398998141289\n",
            "Iteration 303/1000, G loss: 0.012569580227136612, D loss: 0.3250669240951538\n",
            "Iteration 304/1000, G loss: 0.009491919539868832, D loss: 0.2588847875595093\n",
            "Iteration 305/1000, G loss: 0.009320657700300217, D loss: 0.7170532941818237\n",
            "Iteration 306/1000, G loss: 0.01282978430390358, D loss: 0.07325925678014755\n",
            "Iteration 307/1000, G loss: 0.010634901002049446, D loss: 0.17596064507961273\n",
            "Iteration 308/1000, G loss: 0.009891843423247337, D loss: 0.8556594848632812\n",
            "Iteration 309/1000, G loss: 0.011987289413809776, D loss: 0.17621579766273499\n",
            "Iteration 310/1000, G loss: 0.010331662371754646, D loss: 0.8412336111068726\n",
            "Iteration 311/1000, G loss: 0.013206209056079388, D loss: 0.04038938134908676\n",
            "Iteration 312/1000, G loss: 0.012072402983903885, D loss: 0.032875146716833115\n",
            "Iteration 313/1000, G loss: 0.012669220566749573, D loss: 0.2925461232662201\n",
            "Iteration 314/1000, G loss: 0.013554703444242477, D loss: 0.044403448700904846\n",
            "Iteration 315/1000, G loss: 0.01311237458139658, D loss: 0.019224204123020172\n",
            "Iteration 316/1000, G loss: 0.011828874237835407, D loss: 0.25867149233818054\n",
            "Iteration 317/1000, G loss: 0.011845270171761513, D loss: 0.6889665722846985\n",
            "Iteration 318/1000, G loss: 0.012051410973072052, D loss: 0.04118315875530243\n",
            "Iteration 319/1000, G loss: 0.00987369567155838, D loss: 0.23425345122814178\n",
            "Iteration 320/1000, G loss: 0.010412532836198807, D loss: 1.2951364517211914\n",
            "Iteration 321/1000, G loss: 0.00893790740519762, D loss: 2.670729160308838\n",
            "Iteration 322/1000, G loss: 0.012245378457009792, D loss: 0.5501595139503479\n",
            "Iteration 323/1000, G loss: 0.011004633270204067, D loss: 0.5747036933898926\n",
            "Iteration 324/1000, G loss: 0.011958751827478409, D loss: 0.3803020715713501\n",
            "Iteration 325/1000, G loss: 0.012129362672567368, D loss: 0.035864315927028656\n",
            "Iteration 326/1000, G loss: 0.009804772213101387, D loss: 0.15000873804092407\n",
            "Iteration 327/1000, G loss: 0.013168268837034702, D loss: 0.010220815427601337\n",
            "Iteration 328/1000, G loss: 0.012683156877756119, D loss: 0.04999009147286415\n",
            "Iteration 329/1000, G loss: 0.012550710700452328, D loss: 0.07184698432683945\n",
            "Iteration 330/1000, G loss: 0.014829842373728752, D loss: 0.009333307854831219\n",
            "Iteration 331/1000, G loss: 0.012447884306311607, D loss: 0.02538953348994255\n",
            "Iteration 332/1000, G loss: 0.012289805337786674, D loss: 0.015010382980108261\n",
            "Iteration 333/1000, G loss: 0.008244980126619339, D loss: 0.5019158124923706\n",
            "Iteration 334/1000, G loss: 0.011086894199252129, D loss: 0.20391294360160828\n",
            "Iteration 335/1000, G loss: 0.010768099687993526, D loss: 1.217405080795288\n",
            "Iteration 336/1000, G loss: 0.012033944018185139, D loss: 0.560906171798706\n",
            "Iteration 337/1000, G loss: 0.015455031767487526, D loss: 0.11672446131706238\n",
            "Iteration 338/1000, G loss: 0.013864883221685886, D loss: 0.18775153160095215\n",
            "Iteration 339/1000, G loss: 0.01455326471477747, D loss: 0.2975820302963257\n",
            "Iteration 340/1000, G loss: 0.01604774221777916, D loss: 0.017985928803682327\n",
            "Iteration 341/1000, G loss: 0.020925551652908325, D loss: 0.0003220267244614661\n",
            "Iteration 342/1000, G loss: 0.01797039434313774, D loss: 0.0006719352677464485\n",
            "Iteration 343/1000, G loss: 0.016636591404676437, D loss: 0.0001548627478769049\n",
            "Iteration 344/1000, G loss: 0.013956641778349876, D loss: 0.008808678947389126\n",
            "Iteration 345/1000, G loss: 0.01831486076116562, D loss: 0.0021053357049822807\n",
            "Iteration 346/1000, G loss: 0.0133583415299654, D loss: 0.05717010051012039\n",
            "Iteration 347/1000, G loss: 0.017981335520744324, D loss: 0.003099642926827073\n",
            "Iteration 348/1000, G loss: 0.01441759243607521, D loss: 1.111543893814087\n",
            "Iteration 349/1000, G loss: 0.008857304230332375, D loss: 2.0185675621032715\n",
            "Iteration 350/1000, G loss: 0.010642151348292828, D loss: 0.2067873477935791\n",
            "Iteration 351/1000, G loss: 0.009248584508895874, D loss: 0.37320518493652344\n",
            "Iteration 352/1000, G loss: 0.009195609949529171, D loss: 0.6512230634689331\n",
            "Iteration 353/1000, G loss: 0.010644214227795601, D loss: 0.46519434452056885\n",
            "Iteration 354/1000, G loss: 0.012318374589085579, D loss: 0.4925052523612976\n",
            "Iteration 355/1000, G loss: 0.01049700379371643, D loss: 0.29384922981262207\n",
            "Iteration 356/1000, G loss: 0.010713592171669006, D loss: 1.216590166091919\n",
            "Iteration 357/1000, G loss: 0.01162646897137165, D loss: 1.9421656131744385\n",
            "Iteration 358/1000, G loss: 0.008871853351593018, D loss: 4.53856897354126\n",
            "Iteration 359/1000, G loss: 0.008949117735028267, D loss: 0.8187288045883179\n",
            "Iteration 360/1000, G loss: 0.009013441391289234, D loss: 0.5741063356399536\n",
            "Iteration 361/1000, G loss: 0.010756352916359901, D loss: 0.4088972806930542\n",
            "Iteration 362/1000, G loss: 0.007783064618706703, D loss: 0.34296050667762756\n",
            "Iteration 363/1000, G loss: 0.008412616327404976, D loss: 0.3091244399547577\n",
            "Iteration 364/1000, G loss: 0.011342351324856281, D loss: 0.06963274627923965\n",
            "Iteration 365/1000, G loss: 0.00988764688372612, D loss: 0.1947018802165985\n",
            "Iteration 366/1000, G loss: 0.007677716668695211, D loss: 0.16202989220619202\n",
            "Iteration 367/1000, G loss: 0.009882409125566483, D loss: 0.0773249939084053\n",
            "Iteration 368/1000, G loss: 0.009547712281346321, D loss: 0.05007323622703552\n",
            "Iteration 369/1000, G loss: 0.010217132046818733, D loss: 0.0760926604270935\n",
            "Iteration 370/1000, G loss: 0.009482180699706078, D loss: 0.4775775671005249\n",
            "Iteration 371/1000, G loss: 0.009014785289764404, D loss: 0.23164397478103638\n",
            "Iteration 372/1000, G loss: 0.009975593537092209, D loss: 0.18417179584503174\n",
            "Iteration 373/1000, G loss: 0.008899210020899773, D loss: 0.6801576018333435\n",
            "Iteration 374/1000, G loss: 0.007965564727783203, D loss: 0.7912126779556274\n",
            "Iteration 375/1000, G loss: 0.006393784657120705, D loss: 1.0891544818878174\n",
            "Iteration 376/1000, G loss: 0.0056091491132974625, D loss: 2.7270143032073975\n",
            "Iteration 377/1000, G loss: 0.007450758013874292, D loss: 2.148252010345459\n",
            "Iteration 378/1000, G loss: 0.009769751690328121, D loss: 1.2347967624664307\n",
            "Iteration 379/1000, G loss: 0.007690179627388716, D loss: 0.570658266544342\n",
            "Iteration 380/1000, G loss: 0.010352415032684803, D loss: 0.9171414375305176\n",
            "Iteration 381/1000, G loss: 0.008592975325882435, D loss: 1.0885436534881592\n",
            "Iteration 382/1000, G loss: 0.008906054310500622, D loss: 0.590954065322876\n",
            "Iteration 383/1000, G loss: 0.00849969033151865, D loss: 0.8656051754951477\n",
            "Iteration 384/1000, G loss: 0.007590034976601601, D loss: 0.6923712491989136\n",
            "Iteration 385/1000, G loss: 0.007975427433848381, D loss: 0.7746850252151489\n",
            "Iteration 386/1000, G loss: 0.008886491879820824, D loss: 0.41193199157714844\n",
            "Iteration 387/1000, G loss: 0.008776649832725525, D loss: 0.3281577229499817\n",
            "Iteration 388/1000, G loss: 0.00939115323126316, D loss: 0.17881354689598083\n",
            "Iteration 389/1000, G loss: 0.010072676464915276, D loss: 0.08765965700149536\n",
            "Iteration 390/1000, G loss: 0.009064569137990475, D loss: 0.1798403263092041\n",
            "Iteration 391/1000, G loss: 0.008265999145805836, D loss: 0.06976302713155746\n",
            "Iteration 392/1000, G loss: 0.011325551196932793, D loss: 0.11507579684257507\n",
            "Iteration 393/1000, G loss: 0.009480733424425125, D loss: 0.31992465257644653\n",
            "Iteration 394/1000, G loss: 0.009214691817760468, D loss: 0.5555461049079895\n",
            "Iteration 395/1000, G loss: 0.009355303831398487, D loss: 0.4923488199710846\n",
            "Iteration 396/1000, G loss: 0.010826613754034042, D loss: 0.21676331758499146\n",
            "Iteration 397/1000, G loss: 0.010242614895105362, D loss: 0.47468042373657227\n",
            "Iteration 398/1000, G loss: 0.010498613119125366, D loss: 0.5194936394691467\n",
            "Iteration 399/1000, G loss: 0.01271443534642458, D loss: 0.39600199460983276\n",
            "Iteration 400/1000, G loss: 0.010594221763312817, D loss: 0.11091925948858261\n",
            "Iteration 401/1000, G loss: 0.010884908959269524, D loss: 0.19251438975334167\n",
            "Iteration 402/1000, G loss: 0.009834028780460358, D loss: 0.725510835647583\n",
            "Iteration 403/1000, G loss: 0.011345327831804752, D loss: 0.5034530162811279\n",
            "Iteration 404/1000, G loss: 0.014662791043519974, D loss: 0.018522975966334343\n",
            "Iteration 405/1000, G loss: 0.014531513676047325, D loss: 0.04675142094492912\n",
            "Iteration 406/1000, G loss: 0.014169284142553806, D loss: 0.00833648256957531\n",
            "Iteration 407/1000, G loss: 0.011037057265639305, D loss: 0.07787700742483139\n",
            "Iteration 408/1000, G loss: 0.01393519714474678, D loss: 0.005357983522117138\n",
            "Iteration 409/1000, G loss: 0.013722371309995651, D loss: 0.024317990988492966\n",
            "Iteration 410/1000, G loss: 0.01239059492945671, D loss: 0.763879656791687\n",
            "Iteration 411/1000, G loss: 0.012003198266029358, D loss: 1.6882643699645996\n",
            "Iteration 412/1000, G loss: 0.01100903283804655, D loss: 0.6018757224082947\n",
            "Iteration 413/1000, G loss: 0.013949434272944927, D loss: 0.5091033577919006\n",
            "Iteration 414/1000, G loss: 0.010248241946101189, D loss: 0.4172605276107788\n",
            "Iteration 415/1000, G loss: 0.009466965682804585, D loss: 1.2339482307434082\n",
            "Iteration 416/1000, G loss: 0.010528558865189552, D loss: 0.3098459243774414\n",
            "Iteration 417/1000, G loss: 0.009371716529130936, D loss: 1.5723177194595337\n",
            "Iteration 418/1000, G loss: 0.010290920734405518, D loss: 0.6207064390182495\n",
            "Iteration 419/1000, G loss: 0.012431539595127106, D loss: 0.12521761655807495\n",
            "Iteration 420/1000, G loss: 0.012996887788176537, D loss: 0.2672816514968872\n",
            "Iteration 421/1000, G loss: 0.014429943636059761, D loss: 0.05997075140476227\n",
            "Iteration 422/1000, G loss: 0.015282808803021908, D loss: 0.03826403617858887\n",
            "Iteration 423/1000, G loss: 0.01674521341919899, D loss: 0.039385780692100525\n",
            "Iteration 424/1000, G loss: 0.015803229063749313, D loss: 0.032680220901966095\n",
            "Iteration 425/1000, G loss: 0.01654159091413021, D loss: 0.03967219218611717\n",
            "Iteration 426/1000, G loss: 0.01792651228606701, D loss: 0.004947672598063946\n",
            "Iteration 427/1000, G loss: 0.01699632778763771, D loss: 0.013694404624402523\n",
            "Iteration 428/1000, G loss: 0.014821726828813553, D loss: 0.365964412689209\n",
            "Iteration 429/1000, G loss: 0.012942425906658173, D loss: 0.3982948660850525\n",
            "Iteration 430/1000, G loss: 0.009516991674900055, D loss: 1.294961929321289\n",
            "Iteration 431/1000, G loss: 0.008370309136807919, D loss: 2.061898708343506\n",
            "Iteration 432/1000, G loss: 0.00812247022986412, D loss: 2.54368257522583\n",
            "Iteration 433/1000, G loss: 0.007452554069459438, D loss: 2.941282272338867\n",
            "Iteration 434/1000, G loss: 0.007094874512404203, D loss: 2.9838457107543945\n",
            "Iteration 435/1000, G loss: 0.010051417164504528, D loss: 1.257314920425415\n",
            "Iteration 436/1000, G loss: 0.007898582145571709, D loss: 0.7062869071960449\n",
            "Iteration 437/1000, G loss: 0.010594947263598442, D loss: 0.49984848499298096\n",
            "Iteration 438/1000, G loss: 0.008868970908224583, D loss: 0.25797349214553833\n",
            "Iteration 439/1000, G loss: 0.007985382340848446, D loss: 0.5107893943786621\n",
            "Iteration 440/1000, G loss: 0.011336240917444229, D loss: 0.1351250410079956\n",
            "Iteration 441/1000, G loss: 0.008882414549589157, D loss: 0.2577875256538391\n",
            "Iteration 442/1000, G loss: 0.011935997754335403, D loss: 0.12032226473093033\n",
            "Iteration 443/1000, G loss: 0.013625027611851692, D loss: 0.17393115162849426\n",
            "Iteration 444/1000, G loss: 0.01074962131679058, D loss: 0.051250312477350235\n",
            "Iteration 445/1000, G loss: 0.015536347404122353, D loss: 0.060205746442079544\n",
            "Iteration 446/1000, G loss: 0.012915071099996567, D loss: 0.028473451733589172\n",
            "Iteration 447/1000, G loss: 0.011352010071277618, D loss: 0.07037296146154404\n",
            "Iteration 448/1000, G loss: 0.012060053646564484, D loss: 0.08133731037378311\n",
            "Iteration 449/1000, G loss: 0.008945758454501629, D loss: 0.29602137207984924\n",
            "Iteration 450/1000, G loss: 0.014338074252009392, D loss: 0.017526499927043915\n",
            "Iteration 451/1000, G loss: 0.012467015534639359, D loss: 0.30006587505340576\n",
            "Iteration 452/1000, G loss: 0.01174163818359375, D loss: 0.06209063529968262\n",
            "Iteration 453/1000, G loss: 0.009185459464788437, D loss: 0.5013285875320435\n",
            "Iteration 454/1000, G loss: 0.012327082455158234, D loss: 0.040589749813079834\n",
            "Iteration 455/1000, G loss: 0.012805568054318428, D loss: 0.016846343874931335\n",
            "Iteration 456/1000, G loss: 0.01073718536645174, D loss: 0.21740558743476868\n",
            "Iteration 457/1000, G loss: 0.012533203698694706, D loss: 0.510688066482544\n",
            "Iteration 458/1000, G loss: 0.01056082732975483, D loss: 0.09833554923534393\n",
            "Iteration 459/1000, G loss: 0.008729463443160057, D loss: 0.26706796884536743\n",
            "Iteration 460/1000, G loss: 0.007579418830573559, D loss: 0.7551554441452026\n",
            "Iteration 461/1000, G loss: 0.008808585815131664, D loss: 0.4126920700073242\n",
            "Iteration 462/1000, G loss: 0.005322558805346489, D loss: 1.4573302268981934\n",
            "Iteration 463/1000, G loss: 0.00776740862056613, D loss: 0.43030303716659546\n",
            "Iteration 464/1000, G loss: 0.01142740435898304, D loss: 0.11962820589542389\n",
            "Iteration 465/1000, G loss: 0.00935009028762579, D loss: 0.6971017122268677\n",
            "Iteration 466/1000, G loss: 0.0084412582218647, D loss: 0.698728084564209\n",
            "Iteration 467/1000, G loss: 0.009305987507104874, D loss: 0.44965022802352905\n",
            "Iteration 468/1000, G loss: 0.008249413222074509, D loss: 0.755035936832428\n",
            "Iteration 469/1000, G loss: 0.011703191325068474, D loss: 0.15753427147865295\n",
            "Iteration 470/1000, G loss: 0.013362265191972256, D loss: 0.4259461760520935\n",
            "Iteration 471/1000, G loss: 0.01212526485323906, D loss: 0.5279783010482788\n",
            "Iteration 472/1000, G loss: 0.009443729184567928, D loss: 2.115473985671997\n",
            "Iteration 473/1000, G loss: 0.011597593314945698, D loss: 0.06583854556083679\n",
            "Iteration 474/1000, G loss: 0.011898227035999298, D loss: 0.2579047977924347\n",
            "Iteration 475/1000, G loss: 0.011645665392279625, D loss: 0.2634572982788086\n",
            "Iteration 476/1000, G loss: 0.00908940751105547, D loss: 0.5331329107284546\n",
            "Iteration 477/1000, G loss: 0.007740435190498829, D loss: 0.5401488542556763\n",
            "Iteration 478/1000, G loss: 0.01011062040925026, D loss: 0.7474539279937744\n",
            "Iteration 479/1000, G loss: 0.010488749481737614, D loss: 0.7147805690765381\n",
            "Iteration 480/1000, G loss: 0.010321706533432007, D loss: 0.5692638158798218\n",
            "Iteration 481/1000, G loss: 0.011230707168579102, D loss: 0.22886303067207336\n",
            "Iteration 482/1000, G loss: 0.011095370166003704, D loss: 0.17611169815063477\n",
            "Iteration 483/1000, G loss: 0.011607227846980095, D loss: 1.4065266847610474\n",
            "Iteration 484/1000, G loss: 0.012807006016373634, D loss: 0.03192588686943054\n",
            "Iteration 485/1000, G loss: 0.013534650206565857, D loss: 0.0309089794754982\n",
            "Iteration 486/1000, G loss: 0.009935539215803146, D loss: 0.3104513883590698\n",
            "Iteration 487/1000, G loss: 0.011489339172840118, D loss: 0.15569546818733215\n",
            "Iteration 488/1000, G loss: 0.01010015420615673, D loss: 0.30837976932525635\n",
            "Iteration 489/1000, G loss: 0.011536261066794395, D loss: 0.7094205021858215\n",
            "Iteration 490/1000, G loss: 0.009721366688609123, D loss: 2.4438366889953613\n",
            "Iteration 491/1000, G loss: 0.01454400084912777, D loss: 0.37644803524017334\n",
            "Iteration 492/1000, G loss: 0.010191431269049644, D loss: 0.9781608581542969\n",
            "Iteration 493/1000, G loss: 0.008446156978607178, D loss: 0.5067963004112244\n",
            "Iteration 494/1000, G loss: 0.009608732536435127, D loss: 0.9471690654754639\n",
            "Iteration 495/1000, G loss: 0.009380626492202282, D loss: 0.9782036542892456\n",
            "Iteration 496/1000, G loss: 0.010790995322167873, D loss: 0.8703511953353882\n",
            "Iteration 497/1000, G loss: 0.007439457345753908, D loss: 2.053445339202881\n",
            "Iteration 498/1000, G loss: 0.00771443173289299, D loss: 0.570229709148407\n",
            "Iteration 499/1000, G loss: 0.00969417579472065, D loss: 0.37175482511520386\n",
            "Iteration 500/1000, G loss: 0.00867752730846405, D loss: 0.5160186290740967\n",
            "Iteration 501/1000, G loss: 0.008471944369375706, D loss: 0.30304858088493347\n",
            "Iteration 502/1000, G loss: 0.008396228775382042, D loss: 0.3261381983757019\n",
            "Iteration 503/1000, G loss: 0.011077337898314, D loss: 0.19452247023582458\n",
            "Iteration 504/1000, G loss: 0.012095257639884949, D loss: 0.11019430309534073\n",
            "Iteration 505/1000, G loss: 0.013760806992650032, D loss: 0.20846818387508392\n",
            "Iteration 506/1000, G loss: 0.014615898951888084, D loss: 0.07375186681747437\n",
            "Iteration 507/1000, G loss: 0.013149645179510117, D loss: 0.20618322491645813\n",
            "Iteration 508/1000, G loss: 0.012985470704734325, D loss: 0.05656427890062332\n",
            "Iteration 509/1000, G loss: 0.012328055687248707, D loss: 0.3395015001296997\n",
            "Iteration 510/1000, G loss: 0.013249989598989487, D loss: 1.1434671878814697\n",
            "Iteration 511/1000, G loss: 0.018469523638486862, D loss: 0.3774975538253784\n",
            "Iteration 512/1000, G loss: 0.019232189282774925, D loss: 0.0018510024528950453\n",
            "Iteration 513/1000, G loss: 0.015123154036700726, D loss: 0.007510259747505188\n",
            "Iteration 514/1000, G loss: 0.013862080872058868, D loss: 0.09701944142580032\n",
            "Iteration 515/1000, G loss: 0.01587677001953125, D loss: 0.002509388606995344\n",
            "Iteration 516/1000, G loss: 0.014800582081079483, D loss: 0.0025760571006685495\n",
            "Iteration 517/1000, G loss: 0.014450466260313988, D loss: 2.485957622528076\n",
            "Iteration 518/1000, G loss: 0.01223521027714014, D loss: 4.350563049316406\n",
            "Iteration 519/1000, G loss: 0.01502882782369852, D loss: 0.05874279886484146\n",
            "Iteration 520/1000, G loss: 0.014858603477478027, D loss: 0.026892121881246567\n",
            "Iteration 521/1000, G loss: 0.011346668004989624, D loss: 0.8707576394081116\n",
            "Iteration 522/1000, G loss: 0.011689100414514542, D loss: 0.7434201836585999\n",
            "Iteration 523/1000, G loss: 0.009839968755841255, D loss: 0.582983136177063\n",
            "Iteration 524/1000, G loss: 0.010565847158432007, D loss: 1.7626769542694092\n",
            "Iteration 525/1000, G loss: 0.010298008099198341, D loss: 1.4025897979736328\n",
            "Iteration 526/1000, G loss: 0.010803730227053165, D loss: 1.367823839187622\n",
            "Iteration 527/1000, G loss: 0.010430492460727692, D loss: 1.0518810749053955\n",
            "Iteration 528/1000, G loss: 0.00984587799757719, D loss: 0.7700350284576416\n",
            "Iteration 529/1000, G loss: 0.009285587817430496, D loss: 0.7719526290893555\n",
            "Iteration 530/1000, G loss: 0.00768657773733139, D loss: 0.903400182723999\n",
            "Iteration 531/1000, G loss: 0.009986512362957, D loss: 0.23763880133628845\n",
            "Iteration 532/1000, G loss: 0.009767943993210793, D loss: 0.28712767362594604\n",
            "Iteration 533/1000, G loss: 0.009919209405779839, D loss: 0.10872432589530945\n",
            "Iteration 534/1000, G loss: 0.01102390605956316, D loss: 0.06593647599220276\n",
            "Iteration 535/1000, G loss: 0.011380480602383614, D loss: 0.06575005501508713\n",
            "Iteration 536/1000, G loss: 0.012334241531789303, D loss: 0.01629633828997612\n",
            "Iteration 537/1000, G loss: 0.010722288861870766, D loss: 0.05770514905452728\n",
            "Iteration 538/1000, G loss: 0.009322032332420349, D loss: 0.25169020891189575\n",
            "Iteration 539/1000, G loss: 0.012022515758872032, D loss: 0.06248713284730911\n",
            "Iteration 540/1000, G loss: 0.011293618008494377, D loss: 0.31442713737487793\n",
            "Iteration 541/1000, G loss: 0.011381620541214943, D loss: 0.08760237693786621\n",
            "Iteration 542/1000, G loss: 0.013638943433761597, D loss: 0.09112825989723206\n",
            "Iteration 543/1000, G loss: 0.012051473371684551, D loss: 0.8937922716140747\n",
            "Iteration 544/1000, G loss: 0.01571689546108246, D loss: 0.8277875185012817\n",
            "Iteration 545/1000, G loss: 0.023392492905259132, D loss: 0.0001148461306001991\n",
            "Iteration 546/1000, G loss: 0.023755691945552826, D loss: 0.00011605308100115508\n",
            "Iteration 547/1000, G loss: 0.028614813461899757, D loss: 3.6001029002363794e-06\n",
            "Iteration 548/1000, G loss: 0.020431354641914368, D loss: 0.0020369566045701504\n",
            "Iteration 549/1000, G loss: 0.01620362512767315, D loss: 0.11948740482330322\n",
            "Iteration 550/1000, G loss: 0.017559444531798363, D loss: 0.10031375288963318\n",
            "Iteration 551/1000, G loss: 0.01659097708761692, D loss: 1.993204116821289\n",
            "Iteration 552/1000, G loss: 0.013408485800027847, D loss: 0.8590290546417236\n",
            "Iteration 553/1000, G loss: 0.01662490889430046, D loss: 0.04335878789424896\n",
            "Iteration 554/1000, G loss: 0.014179989695549011, D loss: 0.1845153123140335\n",
            "Iteration 555/1000, G loss: 0.013516433537006378, D loss: 0.9161149263381958\n",
            "Iteration 556/1000, G loss: 0.010443350300192833, D loss: 3.9460575580596924\n",
            "Iteration 557/1000, G loss: 0.01075300108641386, D loss: 5.097478866577148\n",
            "Iteration 558/1000, G loss: 0.010018780827522278, D loss: 1.1007335186004639\n",
            "Iteration 559/1000, G loss: 0.011358417570590973, D loss: 1.133501410484314\n",
            "Iteration 560/1000, G loss: 0.01209340151399374, D loss: 0.19585804641246796\n",
            "Iteration 561/1000, G loss: 0.010649153962731361, D loss: 0.5184307098388672\n",
            "Iteration 562/1000, G loss: 0.012954935431480408, D loss: 0.0768975019454956\n",
            "Iteration 563/1000, G loss: 0.016251543536782265, D loss: 0.003888070583343506\n",
            "Iteration 564/1000, G loss: 0.009426330216228962, D loss: 0.2616639733314514\n",
            "Iteration 565/1000, G loss: 0.012874559499323368, D loss: 0.028284046798944473\n",
            "Iteration 566/1000, G loss: 0.011645188555121422, D loss: 0.08739835768938065\n",
            "Iteration 567/1000, G loss: 0.014329385943710804, D loss: 0.002759854309260845\n",
            "Iteration 568/1000, G loss: 0.012963013723492622, D loss: 0.013768389821052551\n",
            "Iteration 569/1000, G loss: 0.010687543079257011, D loss: 0.05098773539066315\n",
            "Iteration 570/1000, G loss: 0.009373973123729229, D loss: 0.33951568603515625\n",
            "Iteration 571/1000, G loss: 0.009334366768598557, D loss: 0.2618885636329651\n",
            "Iteration 572/1000, G loss: 0.010282737202942371, D loss: 0.035009272396564484\n",
            "Iteration 573/1000, G loss: 0.009486548602581024, D loss: 0.31274327635765076\n",
            "Iteration 574/1000, G loss: 0.011442743241786957, D loss: 0.10031275451183319\n",
            "Iteration 575/1000, G loss: 0.008200093172490597, D loss: 0.43726223707199097\n",
            "Iteration 576/1000, G loss: 0.008080463856458664, D loss: 0.9047774076461792\n",
            "Iteration 577/1000, G loss: 0.007073612883687019, D loss: 0.9256422519683838\n",
            "Iteration 578/1000, G loss: 0.006870989687740803, D loss: 2.4903063774108887\n",
            "Iteration 579/1000, G loss: 0.011093162931501865, D loss: 0.049855150282382965\n",
            "Iteration 580/1000, G loss: 0.008307598531246185, D loss: 0.7342284917831421\n",
            "Iteration 581/1000, G loss: 0.009706346318125725, D loss: 0.3101882338523865\n",
            "Iteration 582/1000, G loss: 0.007784445304423571, D loss: 0.4520825147628784\n",
            "Iteration 583/1000, G loss: 0.01101968064904213, D loss: 0.028655223548412323\n",
            "Iteration 584/1000, G loss: 0.008664118126034737, D loss: 0.4322771430015564\n",
            "Iteration 585/1000, G loss: 0.008953746408224106, D loss: 0.17761962115764618\n",
            "Iteration 586/1000, G loss: 0.006483311299234629, D loss: 0.8748326301574707\n",
            "Iteration 587/1000, G loss: 0.008407650515437126, D loss: 1.184303879737854\n",
            "Iteration 588/1000, G loss: 0.008503837510943413, D loss: 0.36090144515037537\n",
            "Iteration 589/1000, G loss: 0.009415863081812859, D loss: 1.7756538391113281\n",
            "Iteration 590/1000, G loss: 0.009554504416882992, D loss: 0.6187273263931274\n",
            "Iteration 591/1000, G loss: 0.010596157982945442, D loss: 1.2636125087738037\n",
            "Iteration 592/1000, G loss: 0.010276083834469318, D loss: 0.3018914759159088\n",
            "Iteration 593/1000, G loss: 0.012395283207297325, D loss: 0.11922070384025574\n",
            "Iteration 594/1000, G loss: 0.011035118252038956, D loss: 0.28621935844421387\n",
            "Iteration 595/1000, G loss: 0.010317351669073105, D loss: 0.9610841870307922\n",
            "Iteration 596/1000, G loss: 0.008656631223857403, D loss: 0.6059640049934387\n",
            "Iteration 597/1000, G loss: 0.01116139255464077, D loss: 0.8053870797157288\n",
            "Iteration 598/1000, G loss: 0.009352212771773338, D loss: 1.3829457759857178\n",
            "Iteration 599/1000, G loss: 0.011754663661122322, D loss: 2.8680293560028076\n",
            "Iteration 600/1000, G loss: 0.012993884272873402, D loss: 0.8112813234329224\n",
            "Iteration 601/1000, G loss: 0.01566983386874199, D loss: 0.1290809065103531\n",
            "Iteration 602/1000, G loss: 0.01510775089263916, D loss: 0.0037234085611999035\n",
            "Iteration 603/1000, G loss: 0.013116042129695415, D loss: 0.020793195813894272\n",
            "Iteration 604/1000, G loss: 0.01306161005049944, D loss: 0.009081687778234482\n",
            "Iteration 605/1000, G loss: 0.014063820242881775, D loss: 0.007924946025013924\n",
            "Iteration 606/1000, G loss: 0.015492690727114677, D loss: 0.008761824108660221\n",
            "Iteration 607/1000, G loss: 0.014392111450433731, D loss: 0.052428074181079865\n",
            "Iteration 608/1000, G loss: 0.012434862554073334, D loss: 0.2621433138847351\n",
            "Iteration 609/1000, G loss: 0.015015838667750359, D loss: 0.1660400778055191\n",
            "Iteration 610/1000, G loss: 0.015441625379025936, D loss: 0.022250842303037643\n",
            "Iteration 611/1000, G loss: 0.013662661425769329, D loss: 0.06396166235208511\n",
            "Iteration 612/1000, G loss: 0.016454409807920456, D loss: 0.005453265272080898\n",
            "Iteration 613/1000, G loss: 0.012104872614145279, D loss: 0.09158441424369812\n",
            "Iteration 614/1000, G loss: 0.012686165049672127, D loss: 0.22729474306106567\n",
            "Iteration 615/1000, G loss: 0.013283371925354004, D loss: 0.5666494369506836\n",
            "Iteration 616/1000, G loss: 0.012944439426064491, D loss: 0.7204692363739014\n",
            "Iteration 617/1000, G loss: 0.012519443407654762, D loss: 0.22071939706802368\n",
            "Iteration 618/1000, G loss: 0.015604998916387558, D loss: 0.006929540075361729\n",
            "Iteration 619/1000, G loss: 0.01606227457523346, D loss: 0.03635985776782036\n",
            "Iteration 620/1000, G loss: 0.015474829822778702, D loss: 0.04987386241555214\n",
            "Iteration 621/1000, G loss: 0.014764498919248581, D loss: 1.9821245670318604\n",
            "Iteration 622/1000, G loss: 0.013304434716701508, D loss: 1.1863691806793213\n",
            "Iteration 623/1000, G loss: 0.010589475743472576, D loss: 0.9426531791687012\n",
            "Iteration 624/1000, G loss: 0.013283236883580685, D loss: 1.820314645767212\n",
            "Iteration 625/1000, G loss: 0.013238157145678997, D loss: 1.4399116039276123\n",
            "Iteration 626/1000, G loss: 0.009570222347974777, D loss: 3.2650952339172363\n",
            "Iteration 627/1000, G loss: 0.008282755501568317, D loss: 4.981590270996094\n",
            "Iteration 628/1000, G loss: 0.008853502571582794, D loss: 2.5708117485046387\n",
            "Iteration 629/1000, G loss: 0.006501181051135063, D loss: 2.3199758529663086\n",
            "Iteration 630/1000, G loss: 0.007281116675585508, D loss: 2.387153148651123\n",
            "Iteration 631/1000, G loss: 0.00675287377089262, D loss: 1.4795697927474976\n",
            "Iteration 632/1000, G loss: 0.008883219212293625, D loss: 1.3582888841629028\n",
            "Iteration 633/1000, G loss: 0.009142129682004452, D loss: 1.4229247570037842\n",
            "Iteration 634/1000, G loss: 0.007507176138460636, D loss: 1.1957931518554688\n",
            "Iteration 635/1000, G loss: 0.00912043172866106, D loss: 0.934536337852478\n",
            "Iteration 636/1000, G loss: 0.008653929457068443, D loss: 0.6961949467658997\n",
            "Iteration 637/1000, G loss: 0.008175202645361423, D loss: 0.7655271291732788\n",
            "Iteration 638/1000, G loss: 0.006859384942799807, D loss: 0.6392754316329956\n",
            "Iteration 639/1000, G loss: 0.00729846628382802, D loss: 0.7636305093765259\n",
            "Iteration 640/1000, G loss: 0.006270593032240868, D loss: 1.028771162033081\n",
            "Iteration 641/1000, G loss: 0.008286301977932453, D loss: 0.7396963834762573\n",
            "Iteration 642/1000, G loss: 0.006493667606264353, D loss: 0.6591160297393799\n",
            "Iteration 643/1000, G loss: 0.008144432678818703, D loss: 0.624849796295166\n",
            "Iteration 644/1000, G loss: 0.006147958803921938, D loss: 0.689566969871521\n",
            "Iteration 645/1000, G loss: 0.009820486418902874, D loss: 0.5670883655548096\n",
            "Iteration 646/1000, G loss: 0.007441980764269829, D loss: 0.40441617369651794\n",
            "Iteration 647/1000, G loss: 0.007523265201598406, D loss: 0.5339711904525757\n",
            "Iteration 648/1000, G loss: 0.006163109093904495, D loss: 0.6160402894020081\n",
            "Iteration 649/1000, G loss: 0.0070107136853039265, D loss: 0.6695258617401123\n",
            "Iteration 650/1000, G loss: 0.006032283417880535, D loss: 0.4699883460998535\n",
            "Iteration 651/1000, G loss: 0.0064828768372535706, D loss: 0.6167095899581909\n",
            "Iteration 652/1000, G loss: 0.008713406510651112, D loss: 0.23108434677124023\n",
            "Iteration 653/1000, G loss: 0.006720209959894419, D loss: 0.5004397034645081\n",
            "Iteration 654/1000, G loss: 0.005860829725861549, D loss: 0.5299406051635742\n",
            "Iteration 655/1000, G loss: 0.0063629308715462685, D loss: 1.2616186141967773\n",
            "Iteration 656/1000, G loss: 0.007386535406112671, D loss: 0.33155667781829834\n",
            "Iteration 657/1000, G loss: 0.00806288979947567, D loss: 0.1149011105298996\n",
            "Iteration 658/1000, G loss: 0.008078842423856258, D loss: 0.2342599332332611\n",
            "Iteration 659/1000, G loss: 0.0074849557131528854, D loss: 0.09540865570306778\n",
            "Iteration 660/1000, G loss: 0.00821276567876339, D loss: 0.07692816853523254\n",
            "Iteration 661/1000, G loss: 0.009836571291089058, D loss: 0.02245384082198143\n",
            "Iteration 662/1000, G loss: 0.007872145622968674, D loss: 0.16088958084583282\n",
            "Iteration 663/1000, G loss: 0.007098904345184565, D loss: 0.8262450695037842\n",
            "Iteration 664/1000, G loss: 0.007014896720647812, D loss: 0.9679862260818481\n",
            "Iteration 665/1000, G loss: 0.005591949913650751, D loss: 3.404975414276123\n",
            "Iteration 666/1000, G loss: 0.0055974447168409824, D loss: 2.5409891605377197\n",
            "Iteration 667/1000, G loss: 0.005896568298339844, D loss: 2.7860145568847656\n",
            "Iteration 668/1000, G loss: 0.006913032382726669, D loss: 1.5268925428390503\n",
            "Iteration 669/1000, G loss: 0.00629670824855566, D loss: 0.7373431921005249\n",
            "Iteration 670/1000, G loss: 0.006313926540315151, D loss: 0.7135242223739624\n",
            "Iteration 671/1000, G loss: 0.0065239714458584785, D loss: 0.7503605484962463\n",
            "Iteration 672/1000, G loss: 0.007930782623589039, D loss: 0.16390587389469147\n",
            "Iteration 673/1000, G loss: 0.007375551387667656, D loss: 0.4972792863845825\n",
            "Iteration 674/1000, G loss: 0.00870553683489561, D loss: 0.3813115358352661\n",
            "Iteration 675/1000, G loss: 0.010215519927442074, D loss: 0.48505204916000366\n",
            "Iteration 676/1000, G loss: 0.006336531136184931, D loss: 1.261757254600525\n",
            "Iteration 677/1000, G loss: 0.00845225341618061, D loss: 0.6090221405029297\n",
            "Iteration 678/1000, G loss: 0.007619514130055904, D loss: 0.8037364482879639\n",
            "Iteration 679/1000, G loss: 0.0060839056968688965, D loss: 1.428950548171997\n",
            "Iteration 680/1000, G loss: 0.007067444734275341, D loss: 1.409321665763855\n",
            "Iteration 681/1000, G loss: 0.007302064914256334, D loss: 1.3472702503204346\n",
            "Iteration 682/1000, G loss: 0.005944699048995972, D loss: 0.43611568212509155\n",
            "Iteration 683/1000, G loss: 0.005790564697235823, D loss: 0.5913493037223816\n",
            "Iteration 684/1000, G loss: 0.005954735912382603, D loss: 0.2841561436653137\n",
            "Iteration 685/1000, G loss: 0.006557907909154892, D loss: 0.378792941570282\n",
            "Iteration 686/1000, G loss: 0.008969949558377266, D loss: 0.27840957045555115\n",
            "Iteration 687/1000, G loss: 0.006174461450427771, D loss: 0.3701735734939575\n",
            "Iteration 688/1000, G loss: 0.007417548447847366, D loss: 0.2858811020851135\n",
            "Iteration 689/1000, G loss: 0.007892321795225143, D loss: 0.11116183549165726\n",
            "Iteration 690/1000, G loss: 0.008964728564023972, D loss: 0.0656367763876915\n",
            "Iteration 691/1000, G loss: 0.008679456077516079, D loss: 0.08339905738830566\n",
            "Iteration 692/1000, G loss: 0.007100397255271673, D loss: 0.06729152798652649\n",
            "Iteration 693/1000, G loss: 0.010285262949764729, D loss: 0.0633358284831047\n",
            "Iteration 694/1000, G loss: 0.009077893570065498, D loss: 0.06187719106674194\n",
            "Iteration 695/1000, G loss: 0.0070131514221429825, D loss: 0.1981813609600067\n",
            "Iteration 696/1000, G loss: 0.009219834581017494, D loss: 0.03068978525698185\n",
            "Iteration 697/1000, G loss: 0.009831098839640617, D loss: 0.03276646137237549\n",
            "Iteration 698/1000, G loss: 0.0070758601650595665, D loss: 0.07799218595027924\n",
            "Iteration 699/1000, G loss: 0.008670834824442863, D loss: 0.09372086822986603\n",
            "Iteration 700/1000, G loss: 0.007995860651135445, D loss: 0.5963457822799683\n",
            "Iteration 701/1000, G loss: 0.0052451784722507, D loss: 1.366461992263794\n",
            "Iteration 702/1000, G loss: 0.006251961924135685, D loss: 0.8066058158874512\n",
            "Iteration 703/1000, G loss: 0.008723851293325424, D loss: 0.36726152896881104\n",
            "Iteration 704/1000, G loss: 0.007840963080525398, D loss: 0.33967897295951843\n",
            "Iteration 705/1000, G loss: 0.01234026812016964, D loss: 0.04374819993972778\n",
            "Iteration 706/1000, G loss: 0.010583995841443539, D loss: 0.04975363612174988\n",
            "Iteration 707/1000, G loss: 0.01055895909667015, D loss: 0.052034445106983185\n",
            "Iteration 708/1000, G loss: 0.010357322171330452, D loss: 0.033451247960329056\n",
            "Iteration 709/1000, G loss: 0.011494828388094902, D loss: 0.015750616788864136\n",
            "Iteration 710/1000, G loss: 0.012739432975649834, D loss: 0.0045806411653757095\n",
            "Iteration 711/1000, G loss: 0.012004848569631577, D loss: 0.003940776456147432\n",
            "Iteration 712/1000, G loss: 0.014496946707367897, D loss: 0.002175759756937623\n",
            "Iteration 713/1000, G loss: 0.015029137954115868, D loss: 0.0007587886648252606\n",
            "Iteration 714/1000, G loss: 0.01498214527964592, D loss: 0.004489381797611713\n",
            "Iteration 715/1000, G loss: 0.015886379405856133, D loss: 0.00129723793361336\n",
            "Iteration 716/1000, G loss: 0.013645274564623833, D loss: 0.013443533331155777\n",
            "Iteration 717/1000, G loss: 0.012401820160448551, D loss: 0.018900034949183464\n",
            "Iteration 718/1000, G loss: 0.01451406441628933, D loss: 0.008191331289708614\n",
            "Iteration 719/1000, G loss: 0.013721773400902748, D loss: 0.09611250460147858\n",
            "Iteration 720/1000, G loss: 0.013081557117402554, D loss: 0.0045928568579256535\n",
            "Iteration 721/1000, G loss: 0.010558055713772774, D loss: 0.3716509938240051\n",
            "Iteration 722/1000, G loss: 0.01282519195228815, D loss: 2.0243825912475586\n",
            "Iteration 723/1000, G loss: 0.01155486423522234, D loss: 0.2538970708847046\n",
            "Iteration 724/1000, G loss: 0.010725126601755619, D loss: 0.144362211227417\n",
            "Iteration 725/1000, G loss: 0.012727871537208557, D loss: 0.03719337284564972\n",
            "Iteration 726/1000, G loss: 0.011530129238963127, D loss: 0.04256798326969147\n",
            "Iteration 727/1000, G loss: 0.009233195334672928, D loss: 0.8536568880081177\n",
            "Iteration 728/1000, G loss: 0.011438990011811256, D loss: 0.18568310141563416\n",
            "Iteration 729/1000, G loss: 0.010891749523580074, D loss: 0.09850531816482544\n",
            "Iteration 730/1000, G loss: 0.009175240062177181, D loss: 0.7422935366630554\n",
            "Iteration 731/1000, G loss: 0.00831589475274086, D loss: 1.6010311841964722\n",
            "Iteration 732/1000, G loss: 0.0066542066633701324, D loss: 8.830562591552734\n",
            "Iteration 733/1000, G loss: 0.011926940642297268, D loss: 4.320488929748535\n",
            "Iteration 734/1000, G loss: 0.0074263871647417545, D loss: 4.7836503982543945\n",
            "Iteration 735/1000, G loss: 0.006404820363968611, D loss: 2.6360065937042236\n",
            "Iteration 736/1000, G loss: 0.008479176089167595, D loss: 2.368391990661621\n",
            "Iteration 737/1000, G loss: 0.00874592550098896, D loss: 2.0246429443359375\n",
            "Iteration 738/1000, G loss: 0.0055687082931399345, D loss: 1.846734642982483\n",
            "Iteration 739/1000, G loss: 0.004380526952445507, D loss: 1.330464482307434\n",
            "Iteration 740/1000, G loss: 0.006596023682504892, D loss: 1.4957082271575928\n",
            "Iteration 741/1000, G loss: 0.005755271762609482, D loss: 1.6009401082992554\n",
            "Iteration 742/1000, G loss: 0.006786125712096691, D loss: 1.0141632556915283\n",
            "Iteration 743/1000, G loss: 0.005830892361700535, D loss: 1.2004992961883545\n",
            "Iteration 744/1000, G loss: 0.007257592864334583, D loss: 1.4560754299163818\n",
            "Iteration 745/1000, G loss: 0.006604648195207119, D loss: 1.0247790813446045\n",
            "Iteration 746/1000, G loss: 0.006626655347645283, D loss: 0.8719463348388672\n",
            "Iteration 747/1000, G loss: 0.004227294120937586, D loss: 1.0703543424606323\n",
            "Iteration 748/1000, G loss: 0.005315825343132019, D loss: 0.8581070899963379\n",
            "Iteration 749/1000, G loss: 0.005166176240891218, D loss: 0.8364291787147522\n",
            "Iteration 750/1000, G loss: 0.005166214890778065, D loss: 1.0128726959228516\n",
            "Iteration 751/1000, G loss: 0.00641462579369545, D loss: 0.639454185962677\n",
            "Iteration 752/1000, G loss: 0.004828021861612797, D loss: 0.7452285289764404\n",
            "Iteration 753/1000, G loss: 0.006845568772405386, D loss: 0.7354623675346375\n",
            "Iteration 754/1000, G loss: 0.005632808897644281, D loss: 0.722866952419281\n",
            "Iteration 755/1000, G loss: 0.005705943331122398, D loss: 0.5367763042449951\n",
            "Iteration 756/1000, G loss: 0.005695359315723181, D loss: 0.7480382323265076\n",
            "Iteration 757/1000, G loss: 0.005448666401207447, D loss: 0.7929460406303406\n",
            "Iteration 758/1000, G loss: 0.005013748072087765, D loss: 0.7549307346343994\n",
            "Iteration 759/1000, G loss: 0.004755156580358744, D loss: 1.0927900075912476\n",
            "Iteration 760/1000, G loss: 0.003593308385461569, D loss: 1.4573320150375366\n",
            "Iteration 761/1000, G loss: 0.004735238384455442, D loss: 1.4744393825531006\n",
            "Iteration 762/1000, G loss: 0.00513843260705471, D loss: 1.8694379329681396\n",
            "Iteration 763/1000, G loss: 0.0026386824902147055, D loss: 2.6816213130950928\n",
            "Iteration 764/1000, G loss: 0.005000608507543802, D loss: 2.9756076335906982\n",
            "Iteration 765/1000, G loss: 0.0031210656743496656, D loss: 1.8768424987792969\n",
            "Iteration 766/1000, G loss: 0.003305251244455576, D loss: 2.2563962936401367\n",
            "Iteration 767/1000, G loss: 0.0034000049345195293, D loss: 1.5438206195831299\n",
            "Iteration 768/1000, G loss: 0.004816832020878792, D loss: 1.3886946439743042\n",
            "Iteration 769/1000, G loss: 0.0033791379537433386, D loss: 1.416724681854248\n",
            "Iteration 770/1000, G loss: 0.004060598090291023, D loss: 1.5070080757141113\n",
            "Iteration 771/1000, G loss: 0.003919660113751888, D loss: 1.3390991687774658\n",
            "Iteration 772/1000, G loss: 0.0036552157253026962, D loss: 1.3034963607788086\n",
            "Iteration 773/1000, G loss: 0.005454188212752342, D loss: 1.1264081001281738\n",
            "Iteration 774/1000, G loss: 0.004503015894442797, D loss: 1.1033201217651367\n",
            "Iteration 775/1000, G loss: 0.0044311052188277245, D loss: 1.2751113176345825\n",
            "Iteration 776/1000, G loss: 0.00485039409250021, D loss: 1.134801983833313\n",
            "Iteration 777/1000, G loss: 0.004978294484317303, D loss: 1.235339879989624\n",
            "Iteration 778/1000, G loss: 0.004627544432878494, D loss: 0.9692867398262024\n",
            "Iteration 779/1000, G loss: 0.0038759708404541016, D loss: 1.1955034732818604\n",
            "Iteration 780/1000, G loss: 0.004600133281201124, D loss: 1.1088449954986572\n",
            "Iteration 781/1000, G loss: 0.0048657855950295925, D loss: 0.9574376344680786\n",
            "Iteration 782/1000, G loss: 0.006492793560028076, D loss: 0.8164358139038086\n",
            "Iteration 783/1000, G loss: 0.004964721389114857, D loss: 0.9336361289024353\n",
            "Iteration 784/1000, G loss: 0.00732730096206069, D loss: 0.7175674438476562\n",
            "Iteration 785/1000, G loss: 0.0057219089940190315, D loss: 0.6604856252670288\n",
            "Iteration 786/1000, G loss: 0.005108869634568691, D loss: 0.682091474533081\n",
            "Iteration 787/1000, G loss: 0.004001450724899769, D loss: 0.6973882913589478\n",
            "Iteration 788/1000, G loss: 0.004142446909099817, D loss: 0.7154520750045776\n",
            "Iteration 789/1000, G loss: 0.00691814161837101, D loss: 0.2714484632015228\n",
            "Iteration 790/1000, G loss: 0.004921386018395424, D loss: 0.4395831227302551\n",
            "Iteration 791/1000, G loss: 0.0048554930835962296, D loss: 0.264374703168869\n",
            "Iteration 792/1000, G loss: 0.006332291290163994, D loss: 0.16373051702976227\n",
            "Iteration 793/1000, G loss: 0.004902111366391182, D loss: 0.48317641019821167\n",
            "Iteration 794/1000, G loss: 0.007168252021074295, D loss: 0.3032790720462799\n",
            "Iteration 795/1000, G loss: 0.003750581294298172, D loss: 0.6510069370269775\n",
            "Iteration 796/1000, G loss: 0.0059692589566111565, D loss: 0.7726140022277832\n",
            "Iteration 797/1000, G loss: 0.004297328647226095, D loss: 0.8728595972061157\n",
            "Iteration 798/1000, G loss: 0.007075553759932518, D loss: 1.1236439943313599\n",
            "Iteration 799/1000, G loss: 0.004459013231098652, D loss: 1.9382314682006836\n",
            "Iteration 800/1000, G loss: 0.0060536181554198265, D loss: 0.8866303563117981\n",
            "Iteration 801/1000, G loss: 0.006541578099131584, D loss: 1.4280591011047363\n",
            "Iteration 802/1000, G loss: 0.00540222879499197, D loss: 2.637442111968994\n",
            "Iteration 803/1000, G loss: 0.005802905187010765, D loss: 0.9234241247177124\n",
            "Iteration 804/1000, G loss: 0.006214948371052742, D loss: 0.4913860261440277\n",
            "Iteration 805/1000, G loss: 0.008471996523439884, D loss: 0.45329731702804565\n",
            "Iteration 806/1000, G loss: 0.005639420822262764, D loss: 0.42186403274536133\n",
            "Iteration 807/1000, G loss: 0.008343768306076527, D loss: 0.28790438175201416\n",
            "Iteration 808/1000, G loss: 0.005124937742948532, D loss: 0.4124734401702881\n",
            "Iteration 809/1000, G loss: 0.0061510587111115456, D loss: 0.23339414596557617\n",
            "Iteration 810/1000, G loss: 0.007109101861715317, D loss: 0.13064147531986237\n",
            "Iteration 811/1000, G loss: 0.007030703593045473, D loss: 0.10183659940958023\n",
            "Iteration 812/1000, G loss: 0.006841525435447693, D loss: 0.07808880507946014\n",
            "Iteration 813/1000, G loss: 0.0073536476120352745, D loss: 0.056086309254169464\n",
            "Iteration 814/1000, G loss: 0.008304571732878685, D loss: 0.2032327502965927\n",
            "Iteration 815/1000, G loss: 0.00720862066373229, D loss: 0.35842522978782654\n",
            "Iteration 816/1000, G loss: 0.007928671315312386, D loss: 0.20542305707931519\n",
            "Iteration 817/1000, G loss: 0.006252301391214132, D loss: 0.5263240337371826\n",
            "Iteration 818/1000, G loss: 0.007586784195154905, D loss: 0.3685709238052368\n",
            "Iteration 819/1000, G loss: 0.0082117710262537, D loss: 0.34016990661621094\n",
            "Iteration 820/1000, G loss: 0.008332166820764542, D loss: 0.18389147520065308\n",
            "Iteration 821/1000, G loss: 0.008522062562406063, D loss: 0.32926735281944275\n",
            "Iteration 822/1000, G loss: 0.008137423545122147, D loss: 0.09698102623224258\n",
            "Iteration 823/1000, G loss: 0.01104067638516426, D loss: 0.031766653060913086\n",
            "Iteration 824/1000, G loss: 0.008681162260472775, D loss: 0.060859836637973785\n",
            "Iteration 825/1000, G loss: 0.007004112936556339, D loss: 0.21726877987384796\n",
            "Iteration 826/1000, G loss: 0.006271658930927515, D loss: 0.5637299418449402\n",
            "Iteration 827/1000, G loss: 0.0059859538450837135, D loss: 0.6950197815895081\n",
            "Iteration 828/1000, G loss: 0.005402020178735256, D loss: 1.7486908435821533\n",
            "Iteration 829/1000, G loss: 0.006415525451302528, D loss: 0.6176972389221191\n",
            "Iteration 830/1000, G loss: 0.005713142454624176, D loss: 0.6386170983314514\n",
            "Iteration 831/1000, G loss: 0.009303003549575806, D loss: 0.25918853282928467\n",
            "Iteration 832/1000, G loss: 0.00840759091079235, D loss: 0.2990301847457886\n",
            "Iteration 833/1000, G loss: 0.0061265816912055016, D loss: 0.4221445322036743\n",
            "Iteration 834/1000, G loss: 0.0072370185516774654, D loss: 0.6518010497093201\n",
            "Iteration 835/1000, G loss: 0.007118789944797754, D loss: 0.9290454983711243\n",
            "Iteration 836/1000, G loss: 0.007273209281265736, D loss: 1.1281964778900146\n",
            "Iteration 837/1000, G loss: 0.00940386950969696, D loss: 0.34527507424354553\n",
            "Iteration 838/1000, G loss: 0.007204720750451088, D loss: 1.236043930053711\n",
            "Iteration 839/1000, G loss: 0.009369309060275555, D loss: 1.6436039209365845\n",
            "Iteration 840/1000, G loss: 0.009230054914951324, D loss: 0.8033114671707153\n",
            "Iteration 841/1000, G loss: 0.009787255898118019, D loss: 0.6799659729003906\n",
            "Iteration 842/1000, G loss: 0.010003464296460152, D loss: 0.2727473974227905\n",
            "Iteration 843/1000, G loss: 0.008203964680433273, D loss: 1.3905038833618164\n",
            "Iteration 844/1000, G loss: 0.008572203107178211, D loss: 0.3574061691761017\n",
            "Iteration 845/1000, G loss: 0.009717275388538837, D loss: 0.11715717613697052\n",
            "Iteration 846/1000, G loss: 0.006566554307937622, D loss: 0.6039904356002808\n",
            "Iteration 847/1000, G loss: 0.007553356699645519, D loss: 0.8834875226020813\n",
            "Iteration 848/1000, G loss: 0.010665040463209152, D loss: 0.032115429639816284\n",
            "Iteration 849/1000, G loss: 0.006862628273665905, D loss: 1.8366198539733887\n",
            "Iteration 850/1000, G loss: 0.008832534775137901, D loss: 0.5432695746421814\n",
            "Iteration 851/1000, G loss: 0.006966579705476761, D loss: 0.1416178047657013\n",
            "Iteration 852/1000, G loss: 0.007207700051367283, D loss: 1.881672978401184\n",
            "Iteration 853/1000, G loss: 0.006725594401359558, D loss: 0.6577022075653076\n",
            "Iteration 854/1000, G loss: 0.008628208190202713, D loss: 0.6640625\n",
            "Iteration 855/1000, G loss: 0.005973261781036854, D loss: 0.9859708547592163\n",
            "Iteration 856/1000, G loss: 0.007659325376152992, D loss: 0.975745677947998\n",
            "Iteration 857/1000, G loss: 0.00544002465903759, D loss: 0.5724079608917236\n",
            "Iteration 858/1000, G loss: 0.0062344865873456, D loss: 0.708457887172699\n",
            "Iteration 859/1000, G loss: 0.006127985194325447, D loss: 0.7679324746131897\n",
            "Iteration 860/1000, G loss: 0.006874305661767721, D loss: 0.890744686126709\n",
            "Iteration 861/1000, G loss: 0.005620845127850771, D loss: 1.193725824356079\n",
            "Iteration 862/1000, G loss: 0.005312927532941103, D loss: 1.2140350341796875\n",
            "Iteration 863/1000, G loss: 0.00623382069170475, D loss: 1.34633207321167\n",
            "Iteration 864/1000, G loss: 0.005889611318707466, D loss: 0.7594060897827148\n",
            "Iteration 865/1000, G loss: 0.0046247453428804874, D loss: 0.8219645023345947\n",
            "Iteration 866/1000, G loss: 0.005704107694327831, D loss: 0.7343485355377197\n",
            "Iteration 867/1000, G loss: 0.0055688610300421715, D loss: 0.7019799947738647\n",
            "Iteration 868/1000, G loss: 0.006232523359358311, D loss: 0.45818892121315\n",
            "Iteration 869/1000, G loss: 0.005462867673486471, D loss: 0.9695333242416382\n",
            "Iteration 870/1000, G loss: 0.008951341733336449, D loss: 0.25812140107154846\n",
            "Iteration 871/1000, G loss: 0.008250545710325241, D loss: 0.8269119262695312\n",
            "Iteration 872/1000, G loss: 0.010958933271467686, D loss: 0.029214704409241676\n",
            "Iteration 873/1000, G loss: 0.013551058247685432, D loss: 0.0010930527932941914\n",
            "Iteration 874/1000, G loss: 0.009969969280064106, D loss: 0.06198639050126076\n",
            "Iteration 875/1000, G loss: 0.01106223650276661, D loss: 0.009661395102739334\n",
            "Iteration 876/1000, G loss: 0.011882759630680084, D loss: 0.28232598304748535\n",
            "Iteration 877/1000, G loss: 0.010210830718278885, D loss: 0.13757941126823425\n",
            "Iteration 878/1000, G loss: 0.009064040146768093, D loss: 3.094191551208496\n",
            "Iteration 879/1000, G loss: 0.007107479032129049, D loss: 7.33613395690918\n",
            "Iteration 880/1000, G loss: 0.008895565755665302, D loss: 4.722188949584961\n",
            "Iteration 881/1000, G loss: 0.007351627107709646, D loss: 2.149510383605957\n",
            "Iteration 882/1000, G loss: 0.009022139012813568, D loss: 0.30020251870155334\n",
            "Iteration 883/1000, G loss: 0.006729723419994116, D loss: 1.0852806568145752\n",
            "Iteration 884/1000, G loss: 0.005639688111841679, D loss: 1.1655089855194092\n",
            "Iteration 885/1000, G loss: 0.006731108296662569, D loss: 0.6885924339294434\n",
            "Iteration 886/1000, G loss: 0.005871371366083622, D loss: 0.6463603377342224\n",
            "Iteration 887/1000, G loss: 0.004974484443664551, D loss: 1.3453853130340576\n",
            "Iteration 888/1000, G loss: 0.00557568809017539, D loss: 0.9730515480041504\n",
            "Iteration 889/1000, G loss: 0.004455310292541981, D loss: 1.045863389968872\n",
            "Iteration 890/1000, G loss: 0.0045203715562820435, D loss: 1.706599473953247\n",
            "Iteration 891/1000, G loss: 0.0052088405936956406, D loss: 1.5064449310302734\n",
            "Iteration 892/1000, G loss: 0.006681966129690409, D loss: 1.1303656101226807\n",
            "Iteration 893/1000, G loss: 0.0048206341452896595, D loss: 1.3535058498382568\n",
            "Iteration 894/1000, G loss: 0.005561148747801781, D loss: 1.8240973949432373\n",
            "Iteration 895/1000, G loss: 0.005720572080463171, D loss: 1.399027705192566\n",
            "Iteration 896/1000, G loss: 0.0050836969166994095, D loss: 1.8222174644470215\n",
            "Iteration 897/1000, G loss: 0.0045266831293702126, D loss: 1.5118231773376465\n",
            "Iteration 898/1000, G loss: 0.003547203028574586, D loss: 1.6442968845367432\n",
            "Iteration 899/1000, G loss: 0.003964991308748722, D loss: 1.7745375633239746\n",
            "Iteration 900/1000, G loss: 0.004036790691316128, D loss: 1.6233994960784912\n",
            "Iteration 901/1000, G loss: 0.004933967720717192, D loss: 1.5889966487884521\n",
            "Iteration 902/1000, G loss: 0.004539831075817347, D loss: 1.5033340454101562\n",
            "Iteration 903/1000, G loss: 0.004645520821213722, D loss: 1.2516701221466064\n",
            "Iteration 904/1000, G loss: 0.003878287272527814, D loss: 1.072128415107727\n",
            "Iteration 905/1000, G loss: 0.005570495035499334, D loss: 0.8632047772407532\n",
            "Iteration 906/1000, G loss: 0.0035773878917098045, D loss: 1.1018654108047485\n",
            "Iteration 907/1000, G loss: 0.003968747332692146, D loss: 0.7868334054946899\n",
            "Iteration 908/1000, G loss: 0.005063369870185852, D loss: 0.6727908849716187\n",
            "Iteration 909/1000, G loss: 0.00475452933460474, D loss: 0.44342347979545593\n",
            "Iteration 910/1000, G loss: 0.00583026185631752, D loss: 0.2529440224170685\n",
            "Iteration 911/1000, G loss: 0.007446550764143467, D loss: 0.2127571403980255\n",
            "Iteration 912/1000, G loss: 0.00701551791280508, D loss: 0.38338175415992737\n",
            "Iteration 913/1000, G loss: 0.006012770347297192, D loss: 0.2277550995349884\n",
            "Iteration 914/1000, G loss: 0.006632576696574688, D loss: 0.2853371500968933\n",
            "Iteration 915/1000, G loss: 0.006094790063798428, D loss: 0.7969352006912231\n",
            "Iteration 916/1000, G loss: 0.00822995975613594, D loss: 0.05232027918100357\n",
            "Iteration 917/1000, G loss: 0.008579237386584282, D loss: 0.05341147258877754\n",
            "Iteration 918/1000, G loss: 0.010082559660077095, D loss: 0.013302822597324848\n",
            "Iteration 919/1000, G loss: 0.010273318737745285, D loss: 0.030781369656324387\n",
            "Iteration 920/1000, G loss: 0.010535413399338722, D loss: 0.09041368216276169\n",
            "Iteration 921/1000, G loss: 0.009578775614500046, D loss: 0.012964836321771145\n",
            "Iteration 922/1000, G loss: 0.0077985478565096855, D loss: 0.03846713900566101\n",
            "Iteration 923/1000, G loss: 0.007548092864453793, D loss: 0.7984846830368042\n",
            "Iteration 924/1000, G loss: 0.007557331584393978, D loss: 0.2852710485458374\n",
            "Iteration 925/1000, G loss: 0.007406833115965128, D loss: 0.3372507691383362\n",
            "Iteration 926/1000, G loss: 0.007370766717940569, D loss: 0.3123393654823303\n",
            "Iteration 927/1000, G loss: 0.008146977983415127, D loss: 0.30831336975097656\n",
            "Iteration 928/1000, G loss: 0.0069488040171563625, D loss: 1.442664384841919\n",
            "Iteration 929/1000, G loss: 0.008099645376205444, D loss: 0.5020884275436401\n",
            "Iteration 930/1000, G loss: 0.006988855078816414, D loss: 1.0099973678588867\n",
            "Iteration 931/1000, G loss: 0.007498966529965401, D loss: 0.38229048252105713\n",
            "Iteration 932/1000, G loss: 0.005482155364006758, D loss: 2.4126780033111572\n",
            "Iteration 933/1000, G loss: 0.006280453409999609, D loss: 1.1828458309173584\n",
            "Iteration 934/1000, G loss: 0.005857856944203377, D loss: 0.5389735698699951\n",
            "Iteration 935/1000, G loss: 0.005505074746906757, D loss: 0.3297441303730011\n",
            "Iteration 936/1000, G loss: 0.006183503661304712, D loss: 0.7747766971588135\n",
            "Iteration 937/1000, G loss: 0.008471615612506866, D loss: 0.13412398099899292\n",
            "Iteration 938/1000, G loss: 0.006392939016222954, D loss: 0.2045908272266388\n",
            "Iteration 939/1000, G loss: 0.00683099078014493, D loss: 0.6422711610794067\n",
            "Iteration 940/1000, G loss: 0.005630698055028915, D loss: 0.2609047293663025\n",
            "Iteration 941/1000, G loss: 0.010037563741207123, D loss: 0.05920737236738205\n",
            "Iteration 942/1000, G loss: 0.01057453453540802, D loss: 0.020045701414346695\n",
            "Iteration 943/1000, G loss: 0.009315218776464462, D loss: 0.04947277158498764\n",
            "Iteration 944/1000, G loss: 0.008793102577328682, D loss: 0.09393072128295898\n",
            "Iteration 945/1000, G loss: 0.006549307610839605, D loss: 0.38355064392089844\n",
            "Iteration 946/1000, G loss: 0.009587393142282963, D loss: 0.09239985793828964\n",
            "Iteration 947/1000, G loss: 0.008385105058550835, D loss: 0.04979677498340607\n",
            "Iteration 948/1000, G loss: 0.0065236277878284454, D loss: 0.09582521766424179\n",
            "Iteration 949/1000, G loss: 0.006392449140548706, D loss: 0.1358378529548645\n",
            "Iteration 950/1000, G loss: 0.006353556178510189, D loss: 0.06685771048069\n",
            "Iteration 951/1000, G loss: 0.007195834070444107, D loss: 0.07215180993080139\n",
            "Iteration 952/1000, G loss: 0.006067296490073204, D loss: 0.22735263407230377\n",
            "Iteration 953/1000, G loss: 0.006734916009008884, D loss: 0.1877257078886032\n",
            "Iteration 954/1000, G loss: 0.0063286107033491135, D loss: 0.3157896101474762\n",
            "Iteration 955/1000, G loss: 0.0042896149680018425, D loss: 1.0465366840362549\n",
            "Iteration 956/1000, G loss: 0.005631175823509693, D loss: 2.411875009536743\n",
            "Iteration 957/1000, G loss: 0.0039973994717001915, D loss: 4.958460807800293\n",
            "Iteration 958/1000, G loss: 0.003828618209809065, D loss: 4.197479248046875\n",
            "Iteration 959/1000, G loss: 0.004306970164179802, D loss: 3.590439558029175\n",
            "Iteration 960/1000, G loss: 0.0043470426462590694, D loss: 1.467648983001709\n",
            "Iteration 961/1000, G loss: 0.005694352090358734, D loss: 1.284443736076355\n",
            "Iteration 962/1000, G loss: 0.0037943387869745493, D loss: 1.5586808919906616\n",
            "Iteration 963/1000, G loss: 0.00378927169367671, D loss: 1.548182725906372\n",
            "Iteration 964/1000, G loss: 0.004509138874709606, D loss: 1.4156808853149414\n",
            "Iteration 965/1000, G loss: 0.004079005680978298, D loss: 1.427839756011963\n",
            "Iteration 966/1000, G loss: 0.004573464393615723, D loss: 1.3468947410583496\n",
            "Iteration 967/1000, G loss: 0.0037095765583217144, D loss: 1.2437433004379272\n",
            "Iteration 968/1000, G loss: 0.004079188220202923, D loss: 1.3802969455718994\n",
            "Iteration 969/1000, G loss: 0.005147980526089668, D loss: 1.1508811712265015\n",
            "Iteration 970/1000, G loss: 0.004345481749624014, D loss: 1.2137733697891235\n",
            "Iteration 971/1000, G loss: 0.004550502635538578, D loss: 1.3985153436660767\n",
            "Iteration 972/1000, G loss: 0.004307379946112633, D loss: 1.247538685798645\n",
            "Iteration 973/1000, G loss: 0.005821786355227232, D loss: 1.287956953048706\n",
            "Iteration 974/1000, G loss: 0.005015963222831488, D loss: 1.3069982528686523\n",
            "Iteration 975/1000, G loss: 0.004606758709996939, D loss: 1.2448256015777588\n",
            "Iteration 976/1000, G loss: 0.00648714741691947, D loss: 1.121046543121338\n",
            "Iteration 977/1000, G loss: 0.004305326379835606, D loss: 1.1455409526824951\n",
            "Iteration 978/1000, G loss: 0.004474329762160778, D loss: 1.0330839157104492\n",
            "Iteration 979/1000, G loss: 0.004430731292814016, D loss: 1.0151989459991455\n",
            "Iteration 980/1000, G loss: 0.004085991531610489, D loss: 1.0363473892211914\n",
            "Iteration 981/1000, G loss: 0.003187410533428192, D loss: 0.8296511769294739\n",
            "Iteration 982/1000, G loss: 0.0041846479289233685, D loss: 1.1415529251098633\n",
            "Iteration 983/1000, G loss: 0.004414285533130169, D loss: 0.5054745078086853\n",
            "Iteration 984/1000, G loss: 0.005620734766125679, D loss: 0.8459442257881165\n",
            "Iteration 985/1000, G loss: 0.006345413159579039, D loss: 0.75606369972229\n",
            "Iteration 986/1000, G loss: 0.0052547575905919075, D loss: 0.6017411947250366\n",
            "Iteration 987/1000, G loss: 0.0058804284781217575, D loss: 0.4031369686126709\n",
            "Iteration 988/1000, G loss: 0.004110673442482948, D loss: 0.43397918343544006\n",
            "Iteration 989/1000, G loss: 0.005670091137290001, D loss: 0.20758973062038422\n",
            "Iteration 990/1000, G loss: 0.004506599623709917, D loss: 0.360819935798645\n",
            "Iteration 991/1000, G loss: 0.004734276793897152, D loss: 0.5947837233543396\n",
            "Iteration 992/1000, G loss: 0.004802956711500883, D loss: 0.48599401116371155\n",
            "Iteration 993/1000, G loss: 0.004496502690017223, D loss: 0.7228007912635803\n",
            "Iteration 994/1000, G loss: 0.004263767972588539, D loss: 0.8070660829544067\n",
            "Iteration 995/1000, G loss: 0.004314507823437452, D loss: 0.7433933615684509\n",
            "Iteration 996/1000, G loss: 0.0056082638911902905, D loss: 1.091879963874817\n",
            "Iteration 997/1000, G loss: 0.0036452175118029118, D loss: 2.581483840942383\n",
            "Iteration 998/1000, G loss: 0.003689774312078953, D loss: 2.490602493286133\n",
            "Iteration 999/1000, G loss: 0.00349436909891665, D loss: 2.354633092880249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:294: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOx9d7wcVd3+c2brbek3jYQUwCSkAiEYBKnSFRRUQFARfwiC8qovvryKIL6KgB2pUToKiHRDCzWhp5CENNJIuan33tzk1r3bzu+PmTNzZubM7Mzuzpab83w+yd2dPTN7dsr3Od9OKKWQkJCQkJCwQin3BCQkJCQkKhOSICQkJCQkhJAEISEhISEhhCQICQkJCQkhJEFISEhISAgRLvcEiokhQ4bQsWPHlnsaEhISElWDxYsXt1BKG0Wf9SmCGDt2LBYtWlTuaUhISEhUDQghm50+kyYmCQkJCQkhJEFISEhISAghCUJCQkJCQog+5YOQkJCQECGVSqGpqQmJRKLcUykb4vE4Ro0ahUgk4nkfSRASEhJ9Hk1NTWhoaMDYsWNBCCn3dEoOSilaW1vR1NSEcePGed5PmpgkJCT6PBKJBAYPHrxfkgMAEEIwePBg3xpUYBoEIeQ+AGcB2E0pnaJtexzABG3IAAB7KaUzBPtuAtABIAMgTSmdGdQ8JSQk9g/sr+TAkM/vD1KDeADAafwGSunXKaUzNFJ4EsBTLvufoI2V5CCxX6I3ncG/Fm2FLMkvUS4ERhCU0vkA9og+IyqVfQ3Ao0F9v4REteMvr67DT/+9HC+v3FnuqUgUAbt27cKFF16I8ePH44gjjsDs2bPx9NNP28Zt2rQJU6ZMKcMM7SiXD+JYALsopescPqcAXiGELCaEXOZ2IELIZYSQRYSQRc3NzUWfqIREubCrvRcA0JFIl3kmEoWCUopzzjkHn//857Fx40YsXrwYjz32GJqamso9NVeUiyAugLv2cAyl9HAApwO4khDyeaeBlNI5lNKZlNKZjY3CciISElUJZlpS9nPbeV/A66+/jmg0issvv1zfNmbMGPzgBz9w3S+RSOCSSy7B1KlTcdhhh+GNN94AAKxcuRKzZs3CjBkzMG3aNKxbtw5dXV0488wzMX36dEyZMgWPP/54wfMueZgrISQM4CsAjnAaQyndpv3dTQh5GsAsAPNLM0MJicpAlhGEjDUsKm58fiVWbW8v6jEPHdkPN3xxsuPnK1euxOGHH+77uHfccQcIIfj444+xZs0anHLKKVi7di3uvvtuXH311fjGN76BZDKJTCaDF154ASNHjsTcuXMBAPv27cv79zCU49Y7GcAaSqlQtyKE1BFCGthrAKcAWFHC+UlIVASymm9aahB9D1deeSWmT5+OI4880nXc22+/jYsuuggAMHHiRIwZMwZr167F7NmzcdNNN+GWW27B5s2bUVNTg6lTp2LevHn4n//5HyxYsAD9+/cveJ5Bhrk+CuB4AEMIIU0AbqCU3gvgfFjMS4SQkQD+Tik9A8AwAE9rIVlhAP+klL4U1DwlJCoVTIPY38Mziw23lX5QmDx5Mp588kn9/R133IGWlhbMnJlfkOaFF16Io446CnPnzsUZZ5yBe+65ByeeeCKWLFmCF154Addddx1OOukkXH/99QXNO8gopgsopSMopRFK6SiNHEAp/Tal9G7L2O0aOYBSupFSOl37N5lS+pug5ighUcmgugZR3nlIFI4TTzwRiUQCd911l76tu7s7537HHnss/vGPfwAA1q5diy1btmDChAnYuHEjxo8fjx/+8Ic4++yzsXz5cmzfvh21tbW46KKLcM0112DJkiUFz1uW2pCQqFBkpZO6z4AQgmeeeQY/+tGPcOutt6KxsRF1dXW45ZZbXPf7/ve/jyuuuAJTp05FOBzGAw88gFgshn/96194+OGHEYlEMHz4cPzsZz/DwoULcc0110BRFEQiERMZ5T3vvpSEM3PmTCobBkn0FXzv4UV4eeUu3H3R4ThtyohyT6eqsXr1akyaNKnc0yg7ROeBELLYKSFZxkdISFQosvraTWoQEuWBJAgJiQqFkQdR5olI7LeQBCEhUaGQYa4S5YYkCAmJCgWViXISZYa89SQkKhRMg5B5EBLlgiQICYkKhQxzlSg3JEFISFQoZKJc30IoFMKMGTMwefJkTJ8+HX/4wx+QzWZt4yqp3LdMlJOQqFBIDaJvoaamBkuXLgUA7N69GxdeeCHa29tx4403lnlmzpAahIREhSKT7TtJrBJmDB06FHPmzMHtt9/u2jFwvyv3LSEh4Q19qMhBZeHFa4GdHxf3mMOnAqff7GuX8ePHI5PJYPfu3Rg2bJhwzP5Y7ltCQsIDmIlJEsX+iz5b7ltCQqIw6AQByRBFhc+VflDYuHEjQqEQhg4d6nvfUpX7lgQhIVGhWLJlLwCpQfRFNDc34/LLL8dVV13lmufCyn2feOKJjuW+t2zZguXLl2PixIkYNGgQLrroIgwYMAB///vfC56nJAgJiQqH5Ie+gZ6eHsyYMQOpVArhcBgXX3wxfvzjH7vuI8t9FxGy3LdEX8LYa1Vn4wOXHInjJ/g3Q0gYkOW+Vchy3xISfQx9ZwknUW2QBCEhUemQDCFRJgRGEISQ+wghuwkhK7htvySEbCOELNX+neGw72mEkE8IIesJIdcGNUcJiWqAjGIqDvqSOT0f5PP7g9QgHgBwmmD7nyilM7R/L1g/JISEANwB4HQAhwK4gBByaIDzlJCoaOzncq0oiMfjaG1t3W9JglKK1tZWxONxX/sFFsVEKZ1PCBmbx66zAKynlG4EAELIYwDOBrCqeLOTkKge7KcyragYNWoUmpqa0NzcXO6plA3xeByjRo3ytU85wlyvIoR8E8AiAD+hlLZZPj8AwFbufROAo5wORgi5DMBlAHDggQcWeaoSEuWH5IfCEYlEMG7cuHJPo+pQaif1XQAOAjADwA4Afyj0gJTSOZTSmZTSmY2NjYUeTkKi4rC/mkUkyo+SEgSldBelNEMpzQL4G1RzkhXbAIzm3o/StklI7JeQ9CBRLpSUIAghI7i3XwawQjBsIYBDCCHjCCFRAOcDeK4U85OQqERIBUKiXAjMB0EIeRTA8QCGEEKaANwA4HhCyAyoi6JNAL6njR0J4O+U0jMopWlCyFUAXgYQAnAfpXRlUPOUkKh8SIaQKA+CjGK6QLD5Xoex2wGcwb1/AYAtBFZCYn+E1CAkygWZSS0hUeGQ/CBRLkiCkJCocEgNQqJckAQhIVHhkKU2JMoFSRASEhUOqUFIlAuSICQkKhySHyTKBUkQEhIVDplJLVEuSIKQkJCQkBBCEoSEhISEhBCSICQkKhzSwiRRLkiCkJCocMgwV4lyQRKED4y9di6ufuyjck9DYj+D1CAkygVJEB6xZIva1+jZpdvLPBOJ/QF85JIkCIlyQRKER3zlznfLPQUhtu7pxp/mrZWhkH0M/OWUV1aiXJAEUeX41v0f4i+vrcP2fYlyT0WiiOBJQZK/RLkgCcIDEqmMp3FvrNmNsdfOxZ6uZMAzMrC3OwUAiIbkpeyrkPQgUS5IqeIBrR4F/n3vfAoA+HjbviCnYwIjL4WU7CslSgAqbUwSFQBJEB5g1SDGXjsXrZ29tnGxsHo6ez1qHMVAj/ZdUob0LZhMTPLqSpQJkiA8IJnO2rbtbLfb/KOMIATjgwJbaEozdfXg/Y2t2N3h7jMyKRDy2kqUCZIgPEAk8GPhkOM2EaEEDbnKrB6cP+d9nHuXe1Qcfz3llZUoFwIjCELIfYSQ3YSQFdy23xFC1hBClhNCniaEDHDYdxMh5GNCyFJCyKKg5ugVInNSJmt/bJmJKZkpPUFIKVIdYL6FrXt6cowTv5aQKCWC1CAeAHCaZds8AFMopdMArAXwvy77n0ApnUEpnRnQ/DzhiUVbcemDdo5KZ+0kEC2DD4JBypDqQCrj/0pJ7VCiXAiMICil8wHssWx7hVKa1t6+D2BUUN9fLLy8cpf+etKIfvrrtOBBZ6GmpfRBMMhVZnXAq3YpNQiJSkA5fRDfAfCiw2cUwCuEkMWEkMvcDkIIuYwQsogQsqi5ubnok+TX5gNrI/rrtGZienbpNjz24RYAhgZR8T6I7j25x0gEgpR2b/gJS5b8IFEulIUgCCE/B5AG8A+HIcdQSg8HcDqAKwkhn3c6FqV0DqV0JqV0ZmNjY1Hn2drZa8ppGFgX1V+ntZXg1Y8txbVPfQwACGtPvcA9ETg8rzI3vAHcOg5Y92qg85EQI6XdN+EciY0mwpcqhESZUHKCIIR8G8BZAL5BHWoIUEq3aX93A3gawKySTZDDl+98F7vaDQf1oFqDIEROakJUgsiU4YH2/I1bP9T+vh/UVCRcwMyPkRwqhMyTk6gElJQgCCGnAfgpgC9RSrsdxtQRQhrYawCnAFghGhs0tuwxT5HXIFICgggxDaIMKoT/75Sp1+UA0yBCuQiCfy0ZQqJMCDLM9VEA7wGYQAhpIoRcCuB2AA0A5mkhrHdrY0cSQl7Qdh0G4G1CyDIAHwKYSyl9Kah5+kFjQ0x/nRFEMWW1J7kcGoR/VMMc+x6YkzqSy8RkKvctr5VEeRAO6sCU0gsEm+91GLsdwBna640Apgc1r0IwamCN/toartiTzOjOaZH5KWhIGVIdSKXVC+VLgwhwPhISbgiMIPoazpkxEp87aAgOHlqP9bs7bSQw6XpDyUmVIVHOf6y8NDGVA8mMmiOTW4MQv5aQKCVkqQ0H3PHGev31W9ccjz+ffxiiYQV3X3QEAHcSKAtBSCFSFUhqGkQ4lIOgpZNaogIgCcIBv3v5EwDASROHYszgOn17RHuw3cxIoiS6oOH9G6tL3HT1pjH22rn4+4KN5Z5KUcAy8EMkl4mpuq6TRN+EJIgciEfNRfmY7TidoXj4/c3CfcpRi8m3IzOHgKoUsOZLD7y7qbwTKRL0dYWfRDmpHkqUCZIgcqAmYiYIZjtOZykedBBala1BVBcYj/UVGZnV67O7j+srv1eiuiEJIgesBKFrENksmjvsVV4B6YMoJhTCstOD/4EbmjuDX617PLzMg5CoBEiCECDNCXhWX8n6/vpnV5rG8cinYmfh6NtSJGiCWLZ1L076w1u49+1PA/0er7/DlAfRx6+tROVCEoQA63Z36q+tzugoF57YlRSX9RaVAg8a5aj/VAqw8x/079vU2gUAWNYUbD9xjxYmqUFIVAQkQQjwzvoW/bXVXBQL5z5lFW1iqjJpw1bcQZcvYd/jp8pqPsjnMlXXFZPoS5AEYQGlFO9vNMphW4U98RD9UwYFos8myhkaRLBikl3mXOGnhYL9jly+DlPLUckQEmWCJAgL5n68A6+uNpoE5eNPKIVD1QrvX1ld0kbXIAKeNvseLwuAQuBd0+NfVtc1k+g7kARhwY69CdN7LyYlK8qx4vP9nVWSB8FW9kGTLi2Vicmrk9q0TzBzkZDIBVmLyYJ4RCWEJy6fjQVrm3HpMeN9H6MsGoTfVWaVSB1mYupIpHOMLPR71L+5iugVCmr5KyFRyZAEYUFHryqIph7QH0eOHeRpn37xMMY11mPZ1r0AytQwqI9KHJ5sOxIpNMQjLqML/55KMTGZi/X10YsrUfGQJiYLOhNphBXiy7R0/IShePyyz+rvqyLktGpMTHw+QHAolYnJcx6EdFJLVACkBmFBZ28a9fGw55XkpBH98LuvTkMsbGRcl2PF11eFCK+NBRnqyoioZCYmH6U2+uillagCSA3Cgs5EGvUxd9788mEH6K//fflsnRxmjhkIoMJ9EFXGJDwppIMkCO3QSuAmJumklqgeSIKwoKM3N0Hcet40/XUdN/bfVxyNkycN85UHQSnFix/vwHouezsf+Jed1WdiClKDYGVTgicIr+NkqQ2J8kOamCzoTKTREHc/LWEXM4RC/GkQTyxqwk+fXA4A2HTzmZ73s6KvOjJ5E1OQzv9SZb97Ffayo5xEJSBQDYIQch8hZDchZAW3bRAhZB4hZJ32d6DDvt/SxqwjhHwryHny6PSgQbj5JxRCfD3QG1oK0xwY1u3qRGunUV121fZ2tGm9FKoZvDYWZBl1Zr4K2nfPfs+WPd2m6+UGyQ8S5ULQJqYHAJxm2XYtgNcopYcAeE17bwIhZBCAGwAcBWAWgBuciKTYUJ3U+YdSKoo/DaJYpR1++uRynPrn+fr7M25bgHPufKcoxy4nTE7qAJfSpSrpwR/9Ow8uch7Xx8Jcv/jXt3HC798s9zQkfCJQgqCUzgewx7L5bAAPaq8fBHCOYNdTAcyjlO6hlLYBmAc70QSCjkQa9bFQ7oEOIIT4EjK8zXvhJuup8oeWTrPGsLm1u6DjVQJ4v4Nbm9dCwTSIoJs98cJ+o0e/UzmCHoqNj7ftw6ctXeWehoRPlMNJPYxSukN7vRPAMMGYAwBs5d43adtsIIRcRghZRAhZ1NzcXPDkkumMKWTVL/yamHh3xlfvfs/XdzW15UMA1SVseFIIspUr+54gI6UAs2aQSIvLxQMyD0KiMlDWKCaqLqcKuv0ppXMopTMppTMbGxsLnlMyk7U1CfIDv07qQjJ3j7nljbz3rZpEOe5cfun24ExmzEmdCbgULy/43QpB8rdQVSReSvRJlIMgdhFCRgCA9ne3YMw2AKO596O0bYEjlaGIhPIXngohvh7ofMIq9/WksHiz2By1eke77+NVMngTUzJdAg0iYBOT13vDnAchGUKiPCgHQTwHgEUlfQvAs4IxLwM4hRAyUHNOn6JtCxSZLEUmSxEN5TYxXXv6RNx90RG27cSnBhHycAVaO3sx/cZXdFK4+rGPcO5dYnPU2XfkWGVXmbApVV2rdKmc1HnkQfQFH4REdSLoMNdHAbwHYAIhpIkQcimAmwF8gRCyDsDJ2nsQQmYSQv4OAJTSPQD+D8BC7d+vtG2BgpkZIuHcq/rLjzsIp00Zbtvu1wfhxcT0yqpd2NeTwiPvbwEAV2dfMp3tUytOq2P6+WXbg/meTGn6TnjOg+BeSxOTRLkQaKIcpfQCh49OEoxdBOC73Pv7ANwX0NSEYE7QqJdlvQP8+iC8mJhYNNK4IXUAgMb6mGuE0nMBCdFywHouP9nZgS9OL/73MA0iaI3Fs4nJ5IPoOwzx4ad7MGuctyrJEuWHJ0lICKkjhCja688QQr5ECAmm7nIZwWzchTmp/YW5euEiVgaC9aoYUBt1HX/1Y0tN75dsacPYa+dik0nzqBIntcXtEFQtvbTmnA5c+8qjpVwf4gf8/pVPyj0FCR/wKgnnA4gTQg4A8AqAi6EmwfUp6CamAjQIQohNqLnBy4qSDSGaUPc7vScXNwEAFqwrPAy41LDVXwoo+kr3QQRccSMfDaIvmQzdytRIVB68ihpCKe0G8BUAd1JKvwpgcnDTKg90DaJAE5OfB9pP8tdGrSwH8bn6Z19BCMFzy0oSDFY0WE0+QYkXwwdRukQ5r+hLPoigy6lLFBeeCYIQMhvANwDM1bbln01WoTCc1PkTREgxm5j+/OpaPPzeJsfx1rBKkQBhmx79cCvSmSwUn9MzmuEQbNmj+S6q5Dm1EmhQ1VZLFsWUx7iH39/cJ+pqAcFXy5UoLryKmv8C8L8AnqaUriSEjAdQQJZWZSKZVh/LaBHzIP786jr84tmVjuPTFpvG4s1ttjF85Muujt48NAh1f5NiVCWrUqvADkq+sAS5oFfr+ZiYAODT1r5RpkJqENUFT1FMlNK3ALwFAJqzuoVS+sMgJ1YOJIvig/C3CrWWdjjv7vcwaUQ/XHH8QfjS9JG28dvaenyv/plPJOh+y0HAqkEE9QtKlwfhNczVqlkGMZvSQ2oQ1QWvUUz/JIT0I4TUAVgBYBUh5Jpgp1Z6dCRSAICGQqq5+syDEPkgVu9oxw8f/Uh/zx9vT1evbyHJm5h0VMlzajMxBbQCTZcqDyJPDaJqVD4BeFK0Xb50L5B1rkklUV54XSofSilth1p59UUA46BGMvUptHWrBDGwthCC8LYK7exNI5ulmDN/Y86xvJBs70n7XoVldYLwtVtFoFQ5AHq576CL9eXRMKjawWvJtp/166HAE9+ybq1IvL2uBbs7EuWeRknhlSAiWt7DOQCeo5SmUM1LGgfs7VYdgbnyDNzgJQ8ik6WYcsPL+NnTH3s6Ju+naE+kfNvhjSgmgFTZZbOGDAdloUjrPogKKbXRh0xMfA0t4e9Y/XzpJlMALrr3A5x717vlnkZJ4ZUg7gGwCUAdgPmEkDEA+lZVOABtXaoG0b8mfw2CeCjW15VMAwAeW7g1Z/c6wFz1s70n5XtOTOiZhW11qBNWgR2UDbtUDYPydVLzb9OZLObM34BEyp9p5rXVu/Dt+z/0tU8x0JFI669fXb0LF9/7QcnnUCxs3dNT7imUFJ4IglJ6G6X0AErpGVTFZgAnBDy3kqOtO4n6WLjgct+5HJE9SePB7uxNoy7qHjHMm5j29aQcheShI/oJtzOhly5R3+Viwh7mGsz3pCqsFpNtP263pz7ahpteWIPbXlvn6xiXPrgIb37SXPLEu9Yuc2vVBetaSvr9xUBfSlb0A69O6v6EkD+yxjyEkD9A1Sb6FPZ2JzGgAP8DYA5zdbJndyfNK7/6uLsWkeIEeyKVdVz71zgQDbNQ9QZYLjso2KOYAtYgStgwyN9+xo69mubQnvCvTQKlT7w7O8A+HqVCX0pW9AOvS+X7AHQA+Jr2rx3A/UFNqlxo605hYAH+B8DspHbqgNbVmza975cjaopPprMe8wcnHqy/rnUgCLZq7fFpkqgEZCk1+R0KtTAt3rwHmwU5BaXzQRTupGbhyvkKrXmrduKfH2zJb+c8EHSXvlKgLxVM9AOv1VwPopSey72/kRCy1HF0laIYGgTRwly7etP4+4JPhWOsgjqXBsE/YMl01tTQiFV4BZwJ4uWVu9TvTWYQc59+xSGTpQgRgrT2gBaay8H6aGy6+Uzb9wAVFOZqdVJzr5mJMV+zx+WPLAEAXHjUgXntvz9ifyUIrxpEDyHkGPaGEPI5AH3OW1McDUJ9eH//yif406trhWP2dptNA7kc1XwUU286axJifGZqbdT9OCanZh6C9vU1u0reeD5DqSn3oVgGppXb95neMxIO2tacr5OaF1DsdARdWLBYOHnSMNREQhjRP17uqeSN/ZQfPGsQlwN4iBDSX3vfBqMrXJ9BW3eyoBwIwHh4rWYkALjikcUIhxTURMy8nCuslrfD96YziHH78w5rUZHBJVuM0h1t3Un0t43wju88sAiAffUdJLKaBsFQrCCmt9e1YPJI42yUTIPIo2EQYCYDds1L1W2vUGQpxcFD69HWXb31pKQG4QJK6TJK6XQA0wBMo5QeBuDEQGdWYqQzWXQk0gXlQABGpq9oNf/iip14ftl2JNNZk9YwckAcN39lquMxeSe1tWMcr0GIoq8eeW+z/rq1M1mVeRD8byxWmKu1nAqLYhJltm9o7ixaP+x8Wo4CZi2S3WN+hFaqjBFs6SxFSCFVXeq7D7hR8oKveE5KabuWUQ0APw5gPmXD3p7Cs6gBY4UbjziHrqayFIPrDSIKEYLzZ9ntwUxI8NFHyUzWJMRMGoSAIFq5KqAtpoqg1fGwZik1hbYWS4N4eeVO0/uMg5N6d0cCJ/3hLdz4vHPBRT+wCv7fvrhaPM7yXmRi8rOoXd601/vgIiOTzSKskKou1Cc1CP+o3qstAMuiHlhXHB+Emy07nckiFhabiXiwVW1vKouTJw3DyZOGIunigxAVGWzpNGLQ95ji0f3d8OWKA89oq0+GYt10H3xqbnFu+CDM45i/6P2NrUX5XutZvOctcakV6zz4SDbdxORhWZvNUvSmM2VdAaczTIMoRNyUF0GHP1cqCrlieZ0xQsgEQshS7l87IeS/LGOOJ4Ts48ZcX8A8PYHVYSrUxMTs5dZcB/5hTmeoSZg7FaDrSWbQk8xgd0cvYhEF0bCiOqm5Y/Fqu6hMOe+Y7urNHea6anu7kAzKFaqYoWaCCGoWTpnUbHuxhJvXlaj1GvD7sfWEl2Nd/fhSTLjuJV+NqYqNTJYiHCJIpKsvzJphP+UHd4IghHRoAtz6rwOAvRa1B1BKP6GUzqCUzgBwBIBuAE8Lhi5g4yilv8rnu/xgbxEK9QHGw8vKaTB0cuUGPty0B+GQopsK2N8vHDrMvE8yjS/d/jZaOnsRD4cQC4c0DcK7D4K/sc12dDuZvLxyJ864bQGeWWruOpfN0rLZsLNZatKwglJkGAFaHb96ocMimUe8zt8q0DMCJ7WXYz2/bDuA8ppIVB+Egs2t3WWbQ6GQJiYBKKUNlNJ+gn8NlFKvEVBuOAnABq10R1nBIiyKFebabVmt7+NqKHUk0ohwAodpHX/75kzTPu09KazbrbYZjUUUxMIKetMZ8E3ochFEL6dB9OZYwa3XvuuTnZ36tn8vbsL4n71Qtho0VhNTUA8qK0NiPTwz8xXLfO519laiMjmp9UQ57+dCNLRUWkUmS20O6morXSEJojw4H8CjDp/NJoQsI4S8SAgJvP81K4KXK6s5F9hzYFWnrWURwiGiJ305rU75wnw9yQxqoiF092Ys9fXdfRDdHEGkMtTVhi9yhfzzA5W7P23ptH9YIDY0d2LstXPx7gbn2jwZqmoQvztvGoBgVP0trd1GeRSLIGAEWywHq1dBY81xMGuN/o7lNLZUWmHaogUC1ZdXUG3zLRbKRhCEkCiALwF4QvDxEgBjtNDavwJ4xuU4l7EaUc3NzXnPh5WwiEUKOyVM2GeyFNGQglvPVQWbtQprJKTowtrJSc1rHa1daiHBrmTa5LDM5aTuSTpoDS7hQHe/tQFb93TjR48vxbpdnaZj3xT+O/DoBY77+gFrr/rk4m2OY7KaBnHalOEAClt5Ou179eNGcyarIE1oZrmiVZH1amKyahDaNU+ms3pfcT9kKdIWnErBFBssiolHltKqkrqm+2Lbkv2myVE5NYjTASyhlO6yfqCF03Zqr1+A2o9iiOgglNI5lNKZlNKZjY2NeU+G2ecLaTcKGKUgUpksRg2swZQD1GSsPS5JQk5f2Z5II64RVmtnL+piYWQp8MGnrcJ9hSYmH/H7fCG8v76+Dk9/tA0dWsIfOz8Xhl8HPnnB8/lh85UAACAASURBVDHdUKflinQn7UmFDBmqkmA+ZhXbsRwkKl+O2rpyL58GIfaF3PDcCtz0whrhGDdsbbObCPmFRpBIZylCIStBoMoIQv07lWwE/nYC8NYt5Z1QiVBOgrgADuYlQshwoklaQsgsqPMsTpyhA1KZLEJFiNVmu2eyaomI/prTu7nDXPKYEKIv4t00iEGaT6RfPKILCV4u5Mqk9gN+GomUWVIGUeiP1Y7iI74opabQXNVJzdvd8/8+66rcyDMxvt9mYtKIMVQkDSJfJzVzor+93jDH+SHLXzyzwratVCYmkQ9CnXsVEYR2/ocRrTLBjmVlnE3pUBaC0HpbfwHAU9y2ywkhl2tvzwOwghCyDMBtAM6nAXu1kulswQIWMARZKqMKtn5aIb4mywqOCPaxor0nhUFaQt1fLpiBfd328s65nNR+wM9in8UkFgRBsPnyGsTjC7di5q9fxeodaj4mc1L7Ce10glU7YA5ongxtJibtd3+4aQ9+M3dV3t/NkK+TmgkofnOh/phiZYfngjXQANB+R1VpENr5ZxuqaO6FoCwEQSntopQOppTu47bdTSm9W3t9O6V0MqV0OqX0s5TSwPv8JdPZggUsYGgQ6WwWCiGoi4ahEOAjriYSAzPpOCkt+3pSSKUpTps8HEMb4jhgYI1tDG9CKqYGYSMIJ1+GR/z1tXW2FqtslcznZ7yzQVUUT//LAqQyWfSkMqiJhHyFdjrBKTKIX93uau81+Sp4YvybQ3VeP8hSioZYGFccf5D7OFuYq4ggChNSuaLaigWRBpGpNg2CBTHoIrN65l4Iyh3FVDFIWpLX8gXzQaQzauSGohA0xCNYsmWvZRz0JbuTWas9kUIqk0VEI66LjhpjGzNmUK3+OmIhuIOH1guO6u3G7kgUlyD+MG+trQeBKDmND//d3NqFrt406mJhroJp8XwQqbTh+D3l0GE6Qb64wijD4SW50A8oVa99zr7lNjKzjy+UIKzJnEGB5UHwqFYntaFBVEkp3QIhCUJDMm0uf5Ev2Eo3zanVouQ7jh8cexy096SQzBj9HxSFYNa4QaYxfC8JNq4uGsLCn5+MEya4Oe3t38k7qa3O7b159MLOBSb01uzswL1vq6tznixTGYpFm9tQGw0X3CQHEBBEVs1Kb+1KYuKIfrq84suxuznQ8wGlVO8Z4meuuoDidixUvpaKIIR5EFmgmlbhxnnPoxBWFUMShAZeEBcCk5Naey2q78STgkiDGFQXRXsijVTG7Bux9o7g/ReM4GKREBobYrZVGwDP1Vyt9um2ruKXauaF4O2vq/2Vw9w1eOR9NQfj1dW7DA2iiFFM6Qw1wpu5xcEg7npZNYhCzwOrAcXPReReE9WESqQypqtXaKJboVqhV6S1AJA7Ljxc31Z9GoT2V19EVc/cC4EkCA2povkgjDBXlhPx0RZxJU0jisn+2djBtehIpJHKUNO8rH4GXvlghMDGuBKeQGvhN1lj5FsDJogDB6ud8Xiy5LUYRqiFPJZWckllsnokD39e+d7e971j9jsc9n/z8v7+d9e3YM3ODuzrSZnm4iXL+c43N+CcO94xja0GDYJSikQqi1hEwYkTh+rbqy6KSTcxMQ1Cmpj2KyQzxSEIwmkQLDRy4vAG2zhe5RZFMR0wsBard7SjrTtp8o1Y/Qx8+GXakuzn2p4zh3SxaRBFbvby8PubMWf+Bv19g6YZ8UXx+JLogEqkhQSzWYVuMpPVI5miYQV/10qd6OHERU7b3r4vob/mjy3SikTNgNbs7DA1HCrcB1Fc85kIzZ29SGayOGBAjaVkCqpKg9CDBPYzE1Mx6in1Cai9nosd5qq+/tfls3H/25uwsaUTzy5Vi6dFwgoXxWQ8OGGFIJ2lmLdKdZRSak7ei1jUDX7fAwfX4vOfacTVJx0CAFi82VzSOhd2c7kaVoJoseRxFAprXD7TtkTmtj9+bbo6hpCimph6khnURY0EycYGtWM3E95BZhrzUxH9IidyKmYUUxChy1aw8G4rQaSzWVCarZqeAexUSxPTfopkprh5EOlsFmwx3C8ewdUnH4I/f32G3jkuwsX28w/OG/99PJ684mhTbH6cK/9hvS15JSEWCuGh78zCEWMGAjBnCNsg0C7mzDd6E1ijZvjVbxBg1jBes+rVzsGM0QMAMILI/zusBLG8aZ+e5xAJGUmSbJhTIlkxUnJ4DcFNg7AWcDT5IAqcRqIEBMH6rAyuj5lMqbN/+zoeeGdT4N9fLNhNTJIg9isUKw+CcUw6Yy9QRohRMiLs0A9i9KBaHDFmIG6/8DB9Wyxs2MStgps3IxHL9EV9sSsVzH/CO6mZE5WZnbyEh7qBCd0GLfLrZ09/jD/OWwtANTHx5kHASKSzwppl7hVOiYiin8Q0CEaOorGFElVvnr/DD9gipT4Wtpk8/71ki2iXigR77PIhiM2tXQHMqDSQBKGhWJnUhNMgROaSVJaZNIjuPxA5qaePMgQDr0G42cWthHTk2EG2MX5U+qPGDcL7/3uSjz3yBzv1fOQVM4GwOj6Kh/BQN7Bz9+2jx+rbWMHAaEjhNAgjPwKwK1vWXh88fv/yJxh77dycwnsbl1nv5qS230PFC3P1U6crX3Rqi5SGuN2aHS5WAcQSwAgz9uek/s/y7Tjud2/izU92BzW1QCEJQkOqSE5q3cSUoUInMSuQFgkpuuAT1fnhBQPf39qtfo5Vllz/xUNdZpr74YxFQkUJ/fUCpiXwJqaFm/aYtimkwEQ5iwYBGJFekZCiXwcmDNi5tpaAdwsP/dsC1Uwnsu/zfaF/8+Up+muxiUn9a703+KGFhrl29qZtZeiLjU5Og7CCkOox01DdxKRv8bTfx9vUYhHfvn+hHkRSTZAEoaFYUUxGqQ0KkWxlQiesGAJJRCS8oORj9N2EglWDqOGIJRdEDu36WMhkCgsSzMzGO+R3aH6PkGJoEPnKxEQqgwVr1UJ3A2qM6Cj2+1QTk/o9GYuTun+NhSBcbPesAGFzRy+e+chcxvzB9zbrryeP7I/rzpwEwCxqXl21Cw++u0knQmsqCz+2UCf1A+9uwrRfvlLQMXKhszcNhRjnhYf3u7P8YLJdf8I8nnue4Besd+57UqmQUUwaUkWOYrK+ZmCr0aH9YvrDLupBwWsQ5ugP7wTBE8+zV34Ob69vgfK6eN/LH1li29ZYHwtEg/jpv+2VMNfsaEcilRGa2yJF8EH84pkVeGJxEwDoFXYBg4gjnImJfQUzMVnNI25lsmsiIbQhhWueWI4PN+3B6EE1OGKM3dTHg/9N331oEQDg2tMnArCbmIqZSR00duzrwV9fXw9AvAiiVRQJxK6RrvV4NDGZnt0SlVcvJqQGoaHYeRCAuFPcuUeMwi3nTsV3jxmn33SiEh/8yp2PNnHXIJzndejIfrjyhIMxvF9c+Lm1FAIADKmPFa0PAsPm1i78a1GTbfu63Z247pkVQg2BmeLUEhX5PWRrdnborxs4cwdzikfDxJQFDxjaHutbYcy1A05gSXYbW1THZLtLJJlegFAga25+cY1pDAAcMWagJYrJ/VyUu63nym3trp/3liCKqljQCUK/At7OLX/9MtZywlUASRAaeotc7lt9bf88pBB8/cgDEQ4puhmBj1Ji4AU2X+4h7XKTuQlz/XgOQ0TazqD6qL569wtKKfYIsq/delsvb9or1BB4H0QxRB6/EGC+j0hIsTUlSlkSDxmufmyp47EZQTBSdxPS7JS7raT5a1oXC/vKgyhmz+n73/kUX7nzHV/7iMxKQ+pj+utsFQlMdqoVZM0bcoB/rty0/0qFJAgNxXZSA7m7kLF7TKRB8PvOPmiw/trtoXfLnNb7XzsIo7DAlFQXDZu0oONdi/+Z8cgHW3D4/83D+t3mXtYpF6EQVhT87uVPbNvNPgh/D9m/Fm3Fl25/2ySEeVMiO59hRdF/K/sOFuUjInAn1EbMXfKY7ZrXAn919mT996jf53w83oZNKRWamP40by2m3vCybV8Wpvs/p030PH8n3Pj8KltF4lyICwiCL3P+mWGiasPBYvte+wJlU0sXbnlpjSuZGxoEg0cfBPdoF5OwSwVJEBqK1zDIeO1a6gLGTRcX+CB4DWLSiH76a7YKeeTSo/DutSfmMUOmSZjndsAAe6+JGssDfurk4Z6/5bXVaifZLXvMMeBudlgRSQEwOfP9PmM//fdyLG/aZ9rGLwRYT4SQYoQdM6HOBKzo+jghEjYLfXaNr39WzRyfOLwB35w9FoBxr7iRHk/QWUqFTuq/vLYOHb1pPPTeJpOQ40OqKwV8D4pSNSxieG9DK46++XU8t2y7aft3H1qEu97cgM2t3Y77ZvPUIHgZIH0QVYp0JossLbwjG2Cp0pqDIHQntWCF6qR93PTlqTjuM404ctxAjNSE+tPfPxpXnuDegMb+5eablSchBquJwGs59D1dSbz5STMAdbXOh6a6hfrxpHiUVtZcIYaQLKQW0wrOHs5rEExIqX2v1W1M8LK5+rkvrKY69tvX7lI1KdM59dkEKZuFaeFqJZbrn12JN9c26+9THvusB+WrEK2Y+eS8UpuYVm5XFwrW5l2MtNweV3auFZ8+CGt5kWqDJAgYK8XiRDGJX4vgZmJy0j4mjeiHB78zy0Qqhx04ENec6s2M4FTuW/Qw2wnCm6nl7reMInxhRTHVNEpZvoePEOId8+zB4ov3KYTY2obmA15TZGaksEJsJqa07iMS3xfdyTTGXjsXD723yTRHHuznMpLhfz27P7wK6IxVgxCci0SS91dp5jOBBjF6kKExBuXLZvfUNadOMObHaRCldqLrfgQbiYu3m8dYiMFjFBP/E6UPokrBVpFF8UHwVVpzMIQRxRRsRPiwfjHBVvPNKkrAsxKmV1MLbzqIhomJIKwaBP/beVMIIwh+BaYUEObKIxpW8PpPjgNgEARvYmLCgP11Wji0aY2F7nzDIESr5pehZpLhp69HMTnM85Zzp5reWwVqrnORdNEgxg8x7P9BiS1GEDO12mCAWYOgJRaYjF6tjyU7r27Pq2FiYgThbe78/S5K8rzyn0vw4sc7PB2rHJB5EAB6M+qqppiJcuprj05qHzZuv1j1q1NN83DSTEQahHX+XomML+EQUhTd1AHY7c786lw0T97slI8PQoRIiGB8Yz3qY2GLiUnzQWjfkc5BEKyyLk+ATiYmUT8LNtJJ0NdZso+z1EwSov34rzfmb7/m/LYspQh5LMKSzdKcCx+GjECD+d5x4/HJzg509KbAtaQvCdi9Y30G9DpLnpzU/giC15hFtb3mLt+Buct3YNPNZ3o6XqlRNg2CELKJEPIxIWQpIWSR4HNCCLmNELKeELKcEHK46DjFgN4ToAjOPD8+iLOmjwBQHNOWE2qjYVOpDh225jn2m9eaQeyVyKwkwAtQa/0f/pjmc6f95a6JohTHLMF6aoQUotufVROT+jn7DiYURNcnk6V60b6kiRDN41wd0DmimKwEEc10mQhmV3sv/rVoq+PxmVYomn/I4vz2ily5F6axLBucu64j+tfg0cs+i4G10aJog35gJLuJt2/d0+NYIl/XMnz6IHgNwroIK3eeiheUW4M4gVLqlH9+OoBDtH9HAbhL+1t0FNXERMwCzQ23nDsNPz9jUtGT0bzBfHPySTx//voMxCMKRg+qNY3x6qSOWkqDJF01CIO8RNqXtbGSH6HS6VDNlvkgwgrRiVHhNYgsc1I7r8AveWAh5msO4c5etTUsn0vBwI4lvMI5fBCsftHXZo7CjiUv4OGWm/FNegPmw7Dp//Tfy0378Ifiy7pYwW/zI6cyWQqvFVx0H4jg+0MKAS2x09bRB6Ftv+Bv7wOAcDWfr4mJN91a791qCHutZB/E2QAeoireBzCAEDIiiC/SCSJUuC/Aj4kpElIwuF7kHwgOTk5q3oF2zmEH4LQp9lPt1cTEnwNKqekh4TOaAbPWxp8tUQMhAvecASuufvQj27YLZo3WiY4/dlix94PIuGgQ87loIcBo32kvjaH+Zc2IjjnYyGnRfRAOv4kRxK3nTccFQ9QigNPIWvFgAdJ6tzz7fcjmeU/kj1CWPer5mF6EWkcihWQ661KRVv3tq3a4Z1oXG+x82GeT+zcZwt2fk5rXzK1OaqHTurMZePf2iqmjUk6CoABeIYQsJoRcJvj8AAC8/tykbTOBEHIZIWQRIWRRc3Oz9WNPMFTxwlfyuWoxVQwoxe2vr9NLU3uJ0faqQfArpSw1m5WeXGIus7GMy1HgH5iQrkGYfRR+HpuV2+0C6LdfmSb0b4S4Bk7WlqNeNEsnYciONVQjiB9/wVj958qD4CughrX4+zR1J2n+lnPVILR7/dTQIkT/c6XrMXl4icSZ+stXcN7d77oSRJZSx8VKUPjTqyq52k1Mufe1aRAe597UZuRWWJ3UwsrMz10FvPJzYLu9Nlo5UE6COIZSejhUU9KVhJDP53MQSukcSulMSunMxkbvmb48egMyMRVqOjrviFH4Py3rtlgwpkfx+1fUByaTpa5lxBm8+iD4VabVxOQGvow2k2kmDcJnFFMufg5ZoqZCVhOTbiLJfR1ve22dehyriUmbbyZLEY8ott8D5NYgACAE9dykfTyybuHbXn6TCF7LrS9v2qf/dtFzsK2tx7ySz5auLpPV3+blntKr6/o0Ma3eYWjMNg1CtChLdpn/lhllIwhK6Tbt724ATwOYZRmyDcBo7v0obVvRUdwwV+N1oQrE7786HRdrWbdB4j/Ld+C1NWpDk2eu/JzjuLhHExPPNaqJydvDxMfIO/kg/Dj2cp1+fmUdMpmYzE5qL/fFA+9uEn5pliMb60reWvuJL8fREAtjAFd1lhFEIpv7GizatAdjr52LNTtVDSoSIuhnqUgbyrPGlp9YfubXEhFEJmvRINa/ltd8vILve2FdsHghPXsUk7dFD58cZzXPCcvOhDWTc7q4PeDzRVkIghBSRwhpYK8BnAJghWXYcwC+qUUzfRbAPkppIAHDbPVc7GJ9uaKYygpO0L6qlcUA7C0ueVijapzAr8gy1F2D+OPXpuuv+Vae7DyGrE5qy6Fe/HgH2gRFAb1gE9cKMqwY/SB0oa4R24EWZ70rLLJG92dkqU1Q6t+njbnk/oX6Z0P7xcxRXRpBZDw8ss8uVUtJvMFls3/485Ox+len6WPy1iB8RTE5fxeFxR+mBJsL1NJhCNxuS0dALz8pXxMTzwmeNIgQI4hge8B7Rbk0iGEA3iaELAPwIYC5lNKXCCGXE0Iu18a8AGAjgPUA/gbg+0FNprhRTMbr8kQnucOYkXFzHjDQXodJBK/nh18pZSmQzDibD2YfNBiTR6plPvgVtChRjpmYmtq68eIvTsbqZ27BFf9YgjNvW4AV2+wx9blWu+akNeN7rXWUDh8zEE9//2jXYzE4RaqIWtBar8V7G1u5+ZjHhnUTUy5BapQMYeW0wyGCeCSEmmgI1505Cf/47lEIhwgI/EcR5aNBiPImKKVmDTsabOE+viIy/xrwaGLSNQh/tZh4jdda7ltIEGGtmVWFaBBlCXOllG4EMF2w/W7uNQXg3XtWAJJ51NxxgpektEoA7zhm5RlEBfsA4MHvzMo7Vj5LKVo7xSv8S48ZhxH9a/D8Vcfgons/wLsbDAHJTh2fZMU6yr21thnfCC0Eli4E8E9s35fA1+95Dyu5FTJgJhz+mCLo1W6JMX/eB8H3CHeDVX4yE0Mma18wuOVBWM+3rkHQ3Pco+y0ffKrG9POa8XePHQ8AePOT3QjlQRB+Wr7yZUysoNz/AAAlWFHEhzzzrw/+2QueSI/qBME2eDt3Zn+c+TOxiUnr15LJTysuNio5zLVkcCtJ4BeMZH4UfgLjOhbmGF16sBu8h1OzO7QH5o3/Pl64z3GfacQJE4Z6/g5eiGSzFNdYYvUZmOagKMSWzBfSTUxcFJOWKPfzp63WSKBL0CeaN1kBwDPfd/av6N/B5VrwiV5es4e3WcpJM+GTyWbNgjKdxPTF12IUaRb2l7DKrBBlGoS7IKWUosPSpEjUNjakKHkRRG6tzPichf6KovkotZiYaLBOakYK/eJhE0F41Yhs1VzzMDF50iBCTIPYv01MFQXWY1jU4MQvGEFcHX4aX191VcHHCwzcg9yRSKM2GiqKBgXYTUxO4HtmW1eZRrE+74lyVge2NSx3uot/xfQdllpMXu31W/d0Y/Fmc6VQJgTSVh/Ep/MxavMz+HX4PqzW8gH473HSIHJFMS3a3GYLJRbNP6yQvAjCLQ9i655ubOJKZjOCEGsQtCRRTJRSvLW2GV0aKQzvH0enS5c/J9gSHj1qEKZKxi5hrr99cbV6/+pO6srQIMqdSV0RYOGVNV5TRF0QK0KyXZBgq7asiSBSRfntDJmsal+mVCxQ7vv2TLy3oRUnHzpM3xbhhPlXDjsATW3qSnzpVqNJTa5aTJksNZmkJgxvwCKLwM6FsEL0B5n99epL2tpm7yfASi3YnNSagOF/zvD+cf13W8+bVyf1B5+22raJiD8cIrpfww/cCOLYW98wvWdaakiQX0SppXlV1r/Q9oKH3tuMG55bic9pCYrD+sWxsdl/CKleaoMwH4S3/UwBGy6Jcve8tRGXHD0Ow5kGkakMH4TUIGBoENYGOfmgWKvwoGBNBgNUDUJYrylPZCjVW5WmTIXs1L8HNzbg52ceajLp8XbyU6cM1x22/ENF4O5QtK7QupMZTBze4Gvu4RDRvzNLKRTi3Zdk9XkARrE2mwbBiJp7BMc3Go5aG0HoJhj3uVgdsICbBlFcgrDNhWWXl9HEtExbYOxqVwXukPqYXn/Ljz+FDZ2od8Hzti/vj7NHMWXtY5kPokKc1JUtzUqEnmQGCilOmGvFE4R2w/Klllduby8KOTKo9XpUocC0sx9/4TO6LToiKP1gymomBKJFu0JgM+HwsD6AiVQG/SwFB3MhpCh63oZdqLujJ2k3O2Q4c1U4hwaRSmf1kFonDSKXYLL6HwCzdqYfT1H07Oxc2N1u2MN9EYRm1nE6h6UwMbVoIdBhLVO+f01Ed577icjasqdbO462wauJiVeSbCYm8/vP3fw6Euw0pJx7t5cSlS3NSoTuZAa10XBRoo7Uh8H7jVdqKMRuYgKAYw8Z4u9AOUojM6HUpZkZBtRGdEevqPQDL8RCISK8Fgohul1bhEzGrkFYK9LmQiREdGfisx9tMz3EC356Am6/8DDHfXtEGkTGEEam5DRG1JyYTGWyGN4vjsF1UVz/xUNNx2EE4dRTnIFPCNN/k+B8R8OKZw3iP8uN9CM/1Vw3t3ajX1xcTZhaS214FLh+sa9HPR8diTTqomHEwooelOInMu/etz8FAOhrG4/7ZrMUp09RW/Wu2L7P1FxK1GFu5z7N/CWd1JWDnlSmqCaWfJx/pYbVoTt7/GCHkY4HcPwow2UNM4EeD4d0rUCkqXnTINwJ3PrA9aQy6Bd3Jogjxw60bQspRHcsb99nfkhHD6pFo0txxS5B9Vh2LNUHwX2gaxBmgqiNhbD4F1/AWdNGmo7DhGkughAlJYpqjNVEQggTb/fpr/6zSn+dyWbR1ZvGT/61zJag2GBJpFzetFdvi2sFhcXEFJAG0akRZlt3EnWxECIhBalMFsl0VmgSzIWQwubsNYqJ4uCh9ThkaD3W7urE9c+uxIda+LEoiinLcoZSzv2xSwlJEFCdaTXR4p2KeKhyCULkpFYI8AXOYewNLhpE1qjSunaXWosmFlF0W3SuKg988x4euRS8G59fZXrfk3LXIG6/0N5iJBJSXE0Pbtnke7vtq3d2rGQ6a66GqxEE74NIZqhjqDX76Uoeiw+RicePBsEjlaF4fOFWPLmkCX/R6k8B6oKjy5KhnKVGFVsrbOuLgJzUzCfTncxgV3svomEFWQpMuv4lzLpJXN7DrZyLoUHkvg6UUmSpurDhr8HX7nkPgLhYn14CXZqYKgedvWnUx/yZItwQr+CzqvsguIfgxInD/JvXXB6QDKV67D0r+1ATCeGei2fi+AmNqIvahSz/TMYjitAVm0uDeG7ZdmMOWpHAhrizQBdVpw0pxLW5vMhXc/6RasmwPV12xyJzRPakMpZIMeakNmsQufxg+RhBRdc2pCCvKKZUJqtrd7w/IpHKIkvtPjgnwlPDXIN3Ult7grD5uRWRdFsgGD4IL8l16l8rQbh9TzajzVdqEJWD9p40+tcUL+I3VskahGZWyHJCcHxjnf8D5TAxWc0a8UgIxxwyBA9cMitn0ll9LCK0dRuaR+6Hk/kD6mLOpkPRQxvmTEwiiMKB2XFaBBnjTAj0JC1mTIuTOpOlWL+707HkPNH/FufeyjdRLpUxSobw14hFBtVZCNQph8SuQQRPEMccPMRTMuwhP3/R8bMw8W5iYlq6QsTnQaxBMBOT1CAqBvt6Uq62ar+oZA1C0TUI9f1XjxiFn5zymTyOJH5Abnh2BZZu3Wt7EHP5eCh3vPp4WFgBdqfmE3CzwzPNiBVkqxFoKwwigmAmptZOcZhhWCDA2cPfLNiHEUQilTFrHxYn9brdHdp8xeeJVVrP5YPwihDJLw8i1durEzwflcMig6wE6qxBWLShAJzU1qiha0+fWHCUYciHBsEIVFHEQReihYhuYgrI5OYXFSzKSof2RMp3tIsbaot3qKKDRegktRXfBUcd6LlTnAkOD/SD723Wvsd888dz9JLgY+Xro2FcdNSBtjEbtAQnt5UvE1SJpFhgmb5TQBAhLVFu5m9eBaCG5/IQt89Ut4lIxWxi4qOYzE7qXq0siJMviJnsitVkJ6T4D6Y4V5mPU5+eiv7daqb2Ywu36kKYzd9KcE4hrrY8iAAEojWiq39NBLECQ9kjRip1zrG8iYlP+GQQmTIpOw8l7I/hBplJDU2DKCJB1IUpUBl5Ljbs3NsNhKE3Z8+37HOuFZSVIHJlao8ZbJi56mIh/PJLkxEOKcJIIzdHbbdmyulOqQ9abTSEb80edrmdtgAAIABJREFUg1nj7FFawnDbEEE6k9V/nvVnijSI6aP7AxCbmJgmZPNBsJ4TMCcUiuakblf/KkR83g8ZWo91uzuFn4nQLx7xTRCnhdTaYgO71gFQm3O98clunDRpmF6xt9aisYnOlwoaeBTTHkuUVSyiCHNw/EB/XDxoPEZHPeA7nxuH+975VP9sd0cCL6/cZdtHNzFVCEHs9xoEpRS/OOtQnDF1eNGOWUR3RtHBBAwTSPm3Rc1BENSqQbgTxPmzjN5Q4ZDam+EXZx0q7I3tThAqMfDlU248ewrOnCY4juCnhyw+COvvsBLqpBH9MKyfmv3KYu55MI2tJ5lBPGp3UrOjJ/W2t+JHcnCdWoLBSYNo607i4s+OAeAtIm32QYNx5IH+ssxTWqnxMDVW+0xDSDhoEG4+CLOJqTCBmMrYw1atDup4JFRw3/kQsQd5OMHwQRBc/8VDcd2Zk/TPZv3mNbyuNekCDN9NlhFDwMULvaKCRVlpQAjBBbPs5oxCUFvBZ1UPc9UEl/MKLwcEKyhrq1EeudqV1kbDWHzdydi+N3eCkNvKlxGDl/IpIrtwJKRgW4fhILTasa0r/FQmizGDnRsK7WzvxWeue1EQ5mrWIJhwizqscI0wV3W/kf3jpjyNls6kfi0HeNCGCSE4eeIQwL6I1ZFIZfDvxao5aVBdFOmkRhAwBC8jficfhKiSLFD8PIgv/vVtrNnZgU03n6lv67EkVcbD+RWk5O+BkEcn9Wurd+EnTywD4G0RFgkrQDLD+SAqgyD2ew0iCBRMEO3bA6vFwh7KpEutfk8QrKD4KplWweolEXFwfQxTR/XPOc7qqOXbabL6P/kWYAwpBErrBhxE1O621mxb6/nqTKQxvF/cFr3DsHpHu36uTeYySxQTM085mZgY2PUbNagWN1iyrVmIbIPHgAslxyr1T/PW4rpn1NLqX505StcgCOcvYNoWi2KymgSdNQiriSmHD2LXSlez5pqdHbZt1sz2SIjkVbGZ1yKNntTuJqYrHlmi58V4ecT05FCPGsT63Z34YKO9MGOxIQkiANSEC3AkZjPAHycBT11WvAlxYDc4e6CLaWLinYIpK0Hk4wh3gNXE9IVDDfMgMzF92qI6tP0KhAiheDX6YzwTvR6A3cRkDdHtSKRACPHUjnXCMM6kowmYeEQV5s1aS0znMEx1HkceqBIoAXDJ58aZRjCNyC33g0euRLlWzoZfFw0jTdXj0gx/nVnQg/r36IOHYNF1J+talRPh+Ypi2vgmcNfRwJIHXedrhVWDcLtO919ypONxnvlom/6aaRC5MrGTfJFKDwyhdksEPtmpObNzEObJf3wLX5/zfs7jFgpJEAEgphQQssdWEKufL85kLGDCld2yuVasjhA80HyhOKtgdorvzwdWE9OhWuMhwBAKv567GgAwoDbq69j9oTp6G4hqZsplamYaixcNqZ4X3Nr5O3yMuuLe260K41zn6bjPqDWzjhKURmGkzwjiB6GnsCl+oaM2qlB3IcTPpC4W1jWIEJfE9byWCMlMTLGwgiH1MV1zcDJhfnbcYMCriWm71lCpZZ3zGAFEtbHqHfJi3PrH8w2vFE4DZ/WZcsHbIoyYjm9rvl4mSIIIADnM7e4IqGgZg5FwpUVY5O2DsEvOO95cr79mjlsA+MGJBxe1/eqssebGP8ceMgT3fXsm+qELAzY+Z1rZDan3RxANMPcKyFW99JFLjwKQuwwIYDV3sQgXdRtzcDtqENr5HlIXwVvXHI+rTzrENqS71+x3uTSsJXwlxf0PQjnuNV6w1URCyBCVeGasvlXf/pS2uv7ew4sBQPezsIWHk4npz+fPwDnTuXpTbiaVhLaqrvHW9pVBRBDWKCsA+N/TJ3rWpPnFiddaTn7sCfrxK8RJXXKCIISMJoS8QQhZRQhZSQi5WjDmeELIPkLIUu3f9aWeZyGIKgVc3MAJwnz8Yvkg5i7fgblc1c/GhhgGaZE3hwzzEC3Tsxe4/Uhgp72dqBW3fX2a6X1dLIxDhjbglsgcHLHwv9G55WP9M7/ENKbOfO3cCOLkScNwjFYF15r09OyV9vamprlQRtCqQG3vUVfzbmGhbL8xg+uE+QW7O1Sn9dCGOG768lRjNepwDgjcNQheuayLhZBVxL4NvmQFyycSdQTkEY+EcNBQzrnvZlLp0Qgi7pEgWjcAny7QtUl+DiITU20snLM+GAM7pwTUM6mkHEp68DjswAH44vSRhvnUY16In/Lr+aAc8TZpAD+hlC4hhDQAWEwImUcpXWUZt4BSelYZ5lcwog6x6p4Q8MrBuMFV5E0Q3LqoqzeNK/+5RH9/woRG3PilyTjztgUAjBBNV2x8E2hZC7x1C/D1h12HRrjze8MXD8UBA2qwpyuJoUQVJF3tqvPuN1+ekvNr//HdozB6oCGoJg8yXzu3ktC8fGBJT1eecBC+NnM0xgyus0UamUBZFJkqmV5auROAh54kDguISIiAcdSI/nF84dBhyLwaAlJwtJMpORcjxg+sjYbRHo5C5LaYM3+D/nqAliXKBK5TFBMAc9KaW2mJrObzIB6l+F/VIozdxy4CAHz485P16yMKJpgysp9jXSYrWJh4CFnvBCEoqcHj+auOwUFD6/DKyl0IrfIXxdTT3or6py4CzvoTMHRS7h18ouQEQSndAWCH9rqDELIawAEArARRtYh4LKMsRMAahDUCaLBL+WpXcELnkvsXmj76/VenY3B9TH+ABnkhCEaMigenMneOmNmmNhpCL1WFU1d3F4AQxgzKXWPqcweb+2AMChv29WhYcQ2B5rmVreQOP3CgmvTX1YpG2oLtUDuQ8THw/G8Ihay+Gn8E8eQVszHw9Wsx5OAj0Dn1BDy3bDsmaz4ZXRlxWI3m8kHwv68uGoISqwEENeR+/8pax/m7NVwyWXs6djpPhNnjM+I+zd1J8e/44zx1Xvz9xxPWf518CL5//MGIhhUs2rTH+fs5MBOQgqwnsyIgLsHOg0XuNcTDSOuaojeCSG1dDGx5D3jmCuCyN71NyAfK6oMghIwFcBiADwQfzyaELCOEvEgImexyjMsIIYsIIYuam5sDmqk/xIl6w9J8am/qAiAY1dGwYBR4fE5QfWh5uFiY5fD+ce29h3UIIxwvq0Tu4RmmfUcsrKAXWkRQWzsAYNRAcS8CN/TXJGCKhrD216dj0oh+jmP5FSSruVTPTBi/OwjPJo1INFvymq5BWPMGHO4ZPbXbLGyOGDMI4zc/jn6v/RQjB9Tg8uMOMkxZ1H01agpzbVpk/5z7fbWxMKIRVdC2xw8Qz1EAN6e7SYPo2OE4Tv8dDgTx1bvf01+v3tHueW6bW7v1vAgnU6Q1XJuZgFQNwvnYfG8MswZBsTr2bVwUmmfbJxJSOBOTt4ViQlsUYftHnsb7RdkIghBSD+BJAP9FKbVe1SUAxlBKpwP4K4BnnI5DKZ1DKZ1JKZ3Z2NgY3IR9IAr1Rs6SPBS0gKMXeBvqVw7z/qDboR5n1Xb7A8keuru+cQRu/NJkjBronEhmHE773S4EoQtr7hydMGGouhshSBNVgO1u24d4RNHbd/pBvUYQ3citWfECdKxWKsTIQTALFttKWhP4VhNMrjBXf8TOImKcNAiOIP7zX7bP568zFlx10RBqwhbiseDLgvsp5GLcZ+eEkpA3gujcLfx4JXcPfvO+D52PYwGf+e5karX2uGDPT5hkXcNX+VLevRpBUKqWWK8hSdwYfsC2TzhEEGE2PI8+iESSi1Dz0SHPK8pCEISQCFRy+Ael9Cnr55TSdkppp/b6BQARQojPnpjlQwzqjZdxcOq5ooRO6lvOm+YyMge0UEdrOQMew/vH8a2jx3o7HlvlEmcT0zNXHo0VN57qeI7SikoQ6d4E+sUjnuLPrYil1YSrXngwi3GHv/dbM/F/50zBxOFih7wtnFgTzryJKRZW8vZBiMe6mysIf6xxx9k+39xq2JNCCtFNp10JcdjsBO63Mw6PumgQLKegt2YY0O5CEExYvne78xgNuez9TmOtYcoXzHkfv3xupd5wCACG94ubcnCcKutSSpFIZ3R/B29iYiaqkMBPGQ0pRpa6VxNTr3YtvvI3T+P9ohxRTATAvQBWU0r/6DBmuDYOhJBZUOcZfNpgkRAjGkGQfAgiWCc14ZzUXmrjO6JTXV22dYvVft/woEHEwiHVhONwjjIaQWze2Zx3WWfSq65GvfRdUAhRJeFbv8PgUDcu/uwYR1KymY40QiREwfeOGw8AmDF6gDOpOZiYXJHTxMSRO7f6TKQyuOKRxfr7s6aNwLghdYho7TadyoSfxdW7YoliorBS/fu1n9pbOxzobgHSDveSj6oCVpOQG/i8HWvG/XsbW/HAu5tMGsQXp48wkYJTomFSK/bICoDyRORWJiYSUhAhuYv1saRKAHh+6Vb1xaCDvMVa+0Q5NIjPAbgYwIlcGOsZhJDLCSGXa2POA7CCELIMwG0AzqdeqmNVCGpIETSIgH4uI4izCi1O2KkW8dlrIYgzp9qL4nmCB4LA7tXA+lcdHx6qqMIo0dOFprY8G64k9gEAwp4IAsCG14A3fg28cI1wzEWheZgXvcZuwmAkR4ieZX6Ak89k71ZjFe3rvmAmJsH5ymYQoup9SokCpI1oq3c3tODFFYbT+PYLDwchRF/1OhEEb0pkTnthw6YlDwOtG8A+ScQ1/0ynQ2GoNHctc/z+9oRZo13yiy84juULdMYdWg7zfcYzWXM13bCDBsHyUVjIL9MgvnbkaEwe5mz2DIe4Hh0uC0W+c+KabZr/z0twRx4oRxTT28jROZFSejuA3PpkheKYsfXAWqA2Hs892IqAi3SxlXFDgSVns10tQJZiT5e5gukd3+D6PH8wB5hwGjDAQzFEPYrJhSDu/Kz693sLxIfQCJmZ+PKCFm7pVobiq0eMwhOLm1QNggnWpLhF5K8j9wMAOq3aGhP4RNELGQqbVnXsAv7MhesWy8T0+0NwYLeqlGfCtQhzq3QnKw3rphbRzCDrf3M6jvzNq2gT9OJmhCgsbfHcVUCsP5RT7wUA9NRoBNGxAxgw2j6e1yDSvUDE+bk6hDTpr489eLAwgu7ObxyOHfsS+M7nxurbnGp28VpGllJTaDB1EOLdWgLduYePwn+Wb8f3jz8YgEoYd1w4HbhLPPdoSEEauTUIPmxcJ5RQME1oZCZ1AIhQdVWthP1l8QII3gdBzXkQ+eKe11fhW/d/iLbuJOIRBZ8ZVm8O5ezeA7x4DfDQOd4O6MEHYYwVEwDRNIg4CjB7aQLfTYNgZS4IwEVfuZ9RW5FW3dFO9FVqvUiY9rSZ3/vRINxMTN2GxTYdqgUyhhDe1maQ3Te4xk1hzQcRQQZPff9ohEOK3u/i7ou4hQEMB7TNxMR+d+8+3cTUE1cDDRwd1XyOhKBX8/ghRjjzS9H/0V8fNlrsDzpj6ghcesw4U+RSPBJCGGncGfkzNsUvxACovqgdXB5LllKTDyKd1sije49aGqdX3adbu54jBsTx7FXHYCw3v5wmphwEkc5ksX2fcT50gsjHWuEBkiCCAFvxhPIhiFL5IAozYbW1d2LBuhYs2dyGQbVRvPKj4/DdY8fbB3a3eDugFxMTQ0qcfMbcDnFSOEG4aRB68pzKEJ4Oa3NKsuvc247z992HunAW5xw20r6j1XSQawGx8S3gme+zwdqE3SNi0qEaIJ1AS2cvFm3ag18+b6QknX8kRxBgBJHWS6nccu40jG+sMxVMBAz/li16iyN35pbpiWp1pbocwtR7uUqtafu1H1QX1YMD+PM8OO6w2Ni1EthqjnaKhBRcEXoOZ4TU7RM0TWT7XkMYZ7LUZGJ6atEWbN3TDcw5Dnj8IuC3o4BtS/T6XKJCkW4EoZqY3J3Uv3x+JW596RNjH50ggjExSYIIAvpNnE8eRD7hjN5hhLnmp0NktBU+W+ks2tyGgW6JcF5XvH7yIARCAgCiWpFETxrEO7cBCwQxEhq5uyY7alNVCPGuQVjLWrAV4kcPY/Sqe7DiK3tx8FDBitd2Pizn07rSfOhLwNJ/qCv1HFFMDKlQLZBO4tv3f4jztJyCWFjB/d8+0lR+nZmYFEIxIKbO68xpI/D6T463EQET2LbMZY6s2C9LhtVkQvSaO+KNvXYubnx+papFRbUxgozrZCaL4f3jmDHaXIqjIeZwTe46GrhX801ks8AOtW9DI9lnzE27/izZDgDGDK41mZh27etC9x2fB/ZuMY791P/TNQiRgz7kcl+pUUzuYa58OZszpg5HmEgTU/WBaRD5mIsC9kGwFRDJU1OhWm5HlBg3sF6qoqsVYKWg2Q3e2+4tt8NPJrUTQWgPS8wLQcz7BfDajarTu9UoFcEEkIKs47wpmKBU36lwJwhifeAt558k7D2LxV9umZOTdpBNGXPLcU+llDiQTmDFNiOf4P5LjsQJE4eaxvHCrS7sfk1/dfYU3P/tI+11uHiCINz3A8CrN6gOeRiVae9/51O1WF+DFvwgMDEl01lEQgqG9TPnrjR4kZkLfg/c83lg+0euS7KHL52F7x4z3hLFRDEhY6kwu3cLuhPq/SfSIKwO/uMnGLlbJhMTzZrvS7Y/58v62szRnAYRjDtZEkQQYKucfAgi8DwITbjl3SRe3T/KOYInDG9QieF344G5P1Y38sdv81AWWfdBFKBBaAIsRlJ6TaCceORcvXaPemw+8UgsWLMUGIY9vjQImyC3Cu1ee8Mb4X7W+yPj4JDns45zEQSJ2EJJh/ezO4JDvAh1yGpmqImGbARjnQsLs03x4eBv3YLmjl5MuO4lAEA/dKvnoP8o9fNHzrNpEcl0FtGwgtOmmM1cdV5ugc3vqn+7WjB5pKEt8b6Grx4xCsce0ghFIabwZ2EodCaJ5Ga1MMQwwTnkj7vp5jPxwCWz9PfhEDE0AkC9Ly2LlAinqdXHwtIHURLsWgnsa8o9zisK0SBKRBAkm5+dniVXRTiTyTdnjwHataYqa15QH2BecDEh+t4dwC/7iyN+uKienEiJTXjMLDSqnuD1nxyf+zgi8OTjQKIHJ1big/hVODn1JmwaxMpngA2v23fKWAnC8r7boRZQLoJwInrT+XcniDQJA1vfx2RiEDnLDDcdkv8N6V41b+Ghc4BF99kPSinQJfA/cfMNZ9RznSLcyv+jh7F75Zv62y8oWj7GIaeofzt3Ak1G7a+/zd+IjS1d6E1lMKK/OUzYU6V3dr3XvYIoNZ4JXlMwCXru/Nv8CXUqIe5etxD94mEMbbBk4+/8GLE7ZzpOJcInyjE8+R3VFKrdH6wR17GHDEE8EjI0jpDUIILD304EPri7eMdjN10+Zpx8ndRbF6rCd89G12GKX4LY/B6w6ln9LRPJjCAW/vxkteCfZhoAzQC/GQ68aEST6KvNN36r/r3raPv3ZHxU7GTn1xIEwAhiYDTjrUCg27EBR+E7s0a1A59Yu96uQTzxLeDhL9t3chL07PdueE08H+vq3+rTcSSIZO4xAD7MTsC+qLryviT8sr5dlLCXSHKkk+pRhfXGN4D//MhujnvnL8DvDjLuC31enJNaI4ikYhak3dtW6q9HEo1kjrzUiHDb9DaQSqAjkcKtL68BAFw8eyxmjR1kOs6AuI976cM5mNb8nL45FiKYMbIGH8cuxSlZLqyaI4g/Rizxqt99FZlQDMmWTWhPpO31nTa+6TqVaEhBFGmkKGeaWvm0agq9dRyQ7NKT5IbUx9C/JmIEU0gTU4CINdgcZAWB3XT51FXKV4NY9k/173oHQaOBqbiRlf92r6DJcP9pwL++qb9lGsjhyjrMuegwNLJV0ivXqX/ZeVz7onEM3SSgCTeRyUlkslj/qriLGNPQwmbBwrJQY7TwKCZ1Tg7htNpzb354cpmYLMfKcrZmQM13EO6XpwbBt6x1MTG9TafjuzvVUORuGsPXZ47GP//fUcaA5U+oC4+eNrODNZ0A/jyVe29xHq/VyGbvZsf5KlpobYpEgTP/oG//x2I1munB78zCqFg3epQ69Vr/ogUYNlUtCf+bYfho4TtIZSju+sbhOO4zjTZSG1LjwZ/lEBF3xXHj8PujM2ggPZi6nls8cuf/6BBXgPqQU4GBY9AeHYFRxKloqPs9omjlTJZQezMoAOiYb6SG/fKLkzF6UC2mj1Q1vUw+dd88QBIEoEZIONmA8wETYA7x+q7I10nNVle5uoTxduRXb/T/Ndr+U5VNOKWDq6G4Q2sLKfrNzLHoNjdGEPyYR84Fbheo5CmtQ5olciOkaV/Rggii12hM49ajgMGrD4KRzZYPgPfvsmuKqS5xxJf1flhpKV3GEwS/Wv/0LW6OLpEzhGJ3IozV2dEYQfbglvOm4eiDuLJn7/5F/du2GVNG1BvbE5YijdZyGMzkYSVZbr5Mg+hFDJh5qb69lqjHGj+kDkNC3ehUtCKNigKMMu6Hz792NhRk9Yq+VtTm6g2fSQHNq4UfRRTg4O1q218ygqtZ5vR8xtRz01EzEtOUjeJACQ/acZRksCR7CD6b+CvmnzXf9FnD2zfhq6E3cZKyGP01H9uwOvW5T2SCEeWSIABVg0gGoEHkcOQJkW+JDRb9s/lddfXoqL1wx+/1XhrZ2J3bf7dmCnBylDJ4EbRMcHhxnrMWmhbHHHN6juhYDjz1vdzHsYJS9drVaQLS0z3hLYpJFyz3nQK8dK1Y0FiT4gA7kfAhlYD53O8WCzu3czo0ol6bHXQwxkf3qmW/u7iyZ1z4Md+oyTZXa+AAuzYuznnCTEwkZiLYmyL3YizZgfpYGIOVDuwlXMn10Zx2A2Ay2SR0qFu/S4h3/+r4UZRkgLZN6psYF4nlRLazrwIArO0diFGkBS8NvcM+xkOtpBBNI4UQdmIwHlmVAr48Bz9LXYrH08cDAH4XmYN7o4a2FVMyyFKCnnRQYfESwZmYcglOEfL1QTANYtUzwPLHgaRYIzJpEHkl1/D7a6vEnhwhmmzF70Z+jEy9aFD6tTIfL57i5rH8Mef9nebBVsG1PghCj77ya2ISCG1ROWvruHh/y+fc+XIifJdzOjKqanc76SCMCbcCfz9JzaVg0H0lxHwcG0FYNQiNIKz9sHkNIs0Iwu4vejP2EwyMpDEQHdiT5TSXGReg6eJ39bfXjVyEkQM057RX/wxDy1rHjya+dQWw+R31TVcL8OxV6l/Bufxh8iq8sldNctzUrZLVuPaFtnE5FxGZNAgoUlR9rlKZLD5oOBn/zJyEn6e/I9ylJtOJfahDwmNHPL+QBAFoBJHHatoJBWkQ3IX+ZX/VJOEFW941v3eq4MkLVa8tHJ2w+AE1I7XlE/dxetivZU4f/xvYpdlxdROTB4JIigmnX8YqtBzOv5PgYNeNaRBeFg36sTyamNzmcOdRhua35gXgobPtgtfqjOSvu9dQWX73EWp0UP2wMQj3agS7i+sLzu7HdBJo324EBuQiCDZP65w4ogxveAUpGkIaYdz60hocmbjTPLa7Ff+/vTMPk6K4Avjv7S4sLMcuu9w3CEpQ5BAFFFEwKmIUoxhv0WBIvKMxxiNRcxlNTDzRaILiFW+SGIIXeEQTFEERUEBQEUSuIIjIIntU/njV29U9PbMLu8vGoX7fN990V9f0dHV116t679WrQrOZ1WUFVVFaH5m9guF/Ws7t5Wo3GbLhb6EHYtwTrKKaKLCZ1qCA8L4tngZvPwgzf0Gz/NR35pnKA5j08gdsK6tgc1mG56C6982qYkvtWiQvLVnPSfe8DkA5eXxv+6XOtel70rR8E5+ZFpGw5HWJFxBQ9yqmsloIiHjD/mYN4rwveSZ1Rak0/11tiI1NK9U2cV1hqp45qec9+XCYcnTmcwY2CPea1iyApybAXcP0vIELZVD+TN47wYikMupK27w81mg9emqa60mj8goERIH1hon3fgOCBqmyAlYFYbFNZg+yeHnSPW/BvZr5C/V6WTErejzeEG+0RuBWPRInkel/JwiI/JYw5FzaDj4egKWlhal5IBQQFV+pCquTtQHEn7e4iikYQaQICMdI/elcGkkF0xes5s6XP2A9Rfyq7LQw7wcvUrR9DcZU8t7qzTwxZyVXTl0AwO/LvxPmC9RurmBL+u84mdagSOKt+2nWKNpkTpeDKSOPd1Zuos/PnuXvFY6H3ldfwKOnweerYNadqetZxFfxs/dwW2wtkm8P7MTIvdow5ojRYeIL10DpJjqveoY9clZz5C3/oj4CXu/yaK7/l9S1kToYjZhKbRjiqpxlM6FkD/Vcmf+YenDEl4kMqEmlx10JIWyMX7sFXvyleoCIREcQSXH23cihn8VmcgbX0m6f1JcxE0kN8h+Hh9tb1sG8h3Q7aEjj1+Y2sIGOvHSjnrtRU9iwDIn3GJfFlnWcO0VdMi9Js/x5lYCoRsX0qtUBB9cMsPR5WPBEcn5IVTGlEz5lpWrwbGL17vH4RPH7EjxrGz/SOExJxAXzA2P1dzm57GtDaWzJaxX9zdIZ2sgHDXpZKXz1ORT31FFLXIUX75AEKs8UAREVlJVGWLAqDHFxzuV/gE/HwGOnwT8uAmBBZU8uuf21MM/wHhyyVxvYPgWeOAvuOwqOu0uFpeTA+GkwZUxV6Pa0VDeCSCJ2/Yf1LoL54f4K2rN2n4m0WzQF3nlURx9NiqLPSsCfD4PrnGu0Aj4QEMN6lnD60G4c7ayzwcv2e/5j0D664Fe6ZVNrgx9BQN3aIIyJDr/jL87i6fDQ8XDPSLj3SJgzOdoQxVUsNXF7TXowAvXKjGv1JbcNS2QEsWR62jDVyf9tf/uNY1OyZiTTf4COJAICr5J4Y+/ufxw2FmHv8V3SEsQlmvFzLVNgfIwTNL6FdunMNEtcJlJdYxTvxceft7FWvVIWExwpOvyyqAOCO8pb+hyJuCOLf90URnLNyaNtyyb89oR9mXDKSdHfPHyC2iKCUVFQvta9oXlsfW1I7TAEAjGuuo3dBzf43WMTh+o65rGQ3y8XhhGBR/X5g1R2AAAS+0lEQVRpy2VH7sXBvdvoc1hiXUIXToVPZmvnJZh1nVQnrmt3TdTK18TUVrF3Ir+kG7eePID+XYqYcekIXr18JO36Hqzv/RorOTLZ+p50bAtW87DNNKZNi3wemTg0KhwACu292boBXr4egP23TaJ7yY4vr1sTvIAAFRDlpamzXXeGsq36cAQvkSsgVs6GR0/R7dLPqGpwXYGSrlHORJKAiAum7VugdBMtJdbgBDOgKxNiD7m9pcqKsKFplGZhm3SUbc28KpjbYK+aq9cRn9me7vfBb4PRTreDoscryuAP34Bftrb3HNiQMLcCwpFO8/aQ1wSevzoq3D58RecF7AxvTo7uxxv+xgXRawhGa26+PHvf1zkjoJqMfN0RnNtrtg3Xd/bvQucOHeDid+C0J5PPETyjjQpg+CVhemA0n3ZJNH/wn/FRWBp7yMQRPavCqNN+X/j23VXH2haFs7onjx8cLg+akwsTnlchUVmuHnxFXcNrCgREZSX88zINyrdmQXL5kijqltq4uyPB1nvBiMsYO6ATfz//IHq1baGLJhXbqMYbrHDNFEhv4VPhdpUNonHaNbIjLuP22V9PK645pm8NC7VjeAEBoRtbGs+fHSJ4kZrbODSucdKJwR+ZBexOOEpRMaUZQWz9DJ46B+bcF3VLDIj3wLdvgRu70VE+4/XKb8ChV2r642fCI6fCTb3USBr5jdM4VZSFvd7GzeDqNfDT9bDPCWGezgeQSFmp6tTTEe/JbVkTtWtUlKe352xcrjNrg/OfOAUuXQRjbtL9L1br+SKxoZannidwcQVdkCbY/vetYZ4HjoWp56QvRybi8xfchvPCt7ThhVQ7gisAgjI8MNY5nqEXPOB0e05HQLi2grjBu1V36H243sM4wXPduEDrP+CQK5L/O7juuABL49m3d0fHlVUE+p8Mx9wKE1+hW7H+30mDu6SqUQqKtcPy4Utats2fqn0lvxBmXKcC4cv1asubckzNR4WXfwTnWftPy85hunsv9xgJTWOqOYBW3fS7agRRjSZ/+5cqxBwbhLsORYTDU9+jj34zhlF9EkZ1dYAXEBAKiLqwQwSeFEkjCNeeUFAS/V1wLN6Lj9sgXrtZ1Smz7lCd97QfVg01I8RfREelMTRnEexpDV7r3oMl/1ThFXf7cwVEZVm4n99CX8q8xjB2Egw6E0p6QYs0D+nsu2HVW8nHIFUVsHF5NG3L2uQRRNNiNeSucdQbBSXQsmOoZlj6QurvkhqJ8m2w0nqMNQsjbFapfDZ/mvqbmnCOM7Pdjc7pCohGBaGA2L5VP0G8IbcOgs5C6WfaYD97FSxPXl0P0HrKbRwVOq5nV7rFmTrtl5pWNYJoGu3cDApn2fMbZ+XA4F1atyj6DKcZQRQVJIRG2e8s6DiAC0b14tpj+nL98f1S80DYEIN2CHJy4LCf6bv3x+EasRXUhrJsRvI54hQUh4JwwvNhulsf6Rr+qvZkc+Z8Add31HfYCp9Sk8+VR/VJzjvwdPi+M4HuzKfrxfYQ4AUEaEMD6Rcs2RGqRhA2sqQrINzeW2AIDQh6Jouejqa7KqbtW7VXdNeBup5BJub9JbPfesuExWniuI1YRVk4wnJ7kI2awrG3w4Vz4Vu3kpa4G24m4vVwc9/kEcR+Z6kdxV3LOFAJ9BihDeC8h9Off9RPw7T3nob172vAtQ79w/RZd8K6xaqmqo740qqHXqUzfw+8UPen/TA85rpkNmqiKi3QSXTXd6Cq3t3GPbBPmUp48dfwesJkLIA29lobNdXP9q0w9374zx3R+5hON17UFa5eCwPPCNPm3m/PWRA+xx0GqEE96Ax99bl67EDYOK57L/rsxT3jLCN6t05MB+hSXMDZB/VIXXwoicA+1nNkmDb7nnD73amQ64RocQVcOgKbFERtLR0HVv9bSL3PpyaoKefcWxWe5KfHD+H7h+yR/nzu89lh3/T56oAGERAiMlpElojIMhFJGaOKSL6IPGaPvyEi3ev1goIHIHi4a0MmFdN6Z76ASNQv+pUbtUF/+8Ho+Rb9Iwzy5Tac1YXxmHsfTL8s3H/m8nD7yOv1+r55XeZzvO4EI6ssd1RMzZPzNyuBcbHInm1iPSEn5k5a5j+e2rt1RxBH/149kfawjcBrCQv/NG4GLdo7bqgOwX3scQgMPV+3/zpRe+bBKOhsDTeNqYB//bb6awbod2J0f5hd2e3wX2ov8iM3dIIj+POaQHEP3Y4be92IqIWO8TaT+3M362rZtEgb9LKt6hH0/NVRI22mnm2jJrC3E3Qw6Bw0KgjduIOwF+fMDOt5gY3dtGlFqH5xn9vSmNEXePXykbXrBbtrlB/5a/1u3Qt6Hpqcv1lrVUEe9Tvt3Fy6uPr/aL1ndP+8N6DfuPT5+zpqwKCjVtgFLlsGex4B3Q+O5t+6QUfawKBBaVS1LsH6GEkqrjpklwsIEckFJgFHAX2BU0QkbmGZAGw0xvQCbgZurNeLCl68mqxbkIntX4YeOYEnxhNnaw9u/hPwyg1h3jULovaFf9+iC5ck8cBYPXdS4LpMuGGYXcNmP+tDPvwSaJ9m2A5hfCWAl28IXRvTCQhQm0Sg/4fQOyegpFf63wYN1uJpqd5cQRiJET+G/c9Rod6yUzSPY9gEosb07zpqgkBo5DWBTs5aECtmhaPJbsPCdNeQGMe9f4f8JHosUDWIhPc8idx8bbSSerPukq3HpVnt3qXvcWGvsm1f1ce7DbTrJVXdTPpeMTdM0I5F/5Ngj1Fw8I80ragLnDVdt2dcG+Yttr3gtx5QNZMx0Y4KQE4eXYpr6YHTYV9t6EdcHjUIH/mbcLu/MyemuCcc8D0YYgMatuwAnfcPjzdLWMdi5NXR/bZpVEAB4+6Dc2fp/Q/mP+w3Hppb9eUeI5N/1+OQmoXu/sFrcNHb1eerJQ0xD+IAYJkx5kMAEXkUGAu4zuljgevs9pPAHSIipj5mgoDqG9v00cknb07WhkoEkGQPIVOputytG3SInZuvPewvHD11L7uk4doFVmVg6TJEX7JFGgiM0TeoWgHC3uPpU9Vg+J/b1HcfVE9ZW7oO0wewmTOcn/iK2h4kByZl6LnMcbxwWmdo5EFfvsIuKvTa9oGL5sFtA/SYq9/e+9sazjhg6LnR+Dg9R6oqbN7DoXHYHdYXddWJW6vmaN7+J0ev4+S/aJkOvQq6DoHTnlL3zYD8FirQ5j+uLqKlG7XXHXD1GhXacdvM4O/CYddqb7i4Jzx2hn7n5avQfe1mOPXx6G/G/A7efzaxB02O7acNnqCNaTo6DtTG68uYDWXP0XpugGHna2PXvp/e67ceSFBbWpLcVZM4743QgaGktzZgZ/w1mqdZiV7/3PvCjs/Iq9SlO7CXJUVOdW00tSFJuLbrC6Nv1HkrIy6DvY+Dv3wnJZ4ToO/cDbZT9+OEjpj7XJz3evXXk5Or/1/UNXyv3d7+QZdAt+Eam6vqN43gWzdXf27Qd7hZerVcXSH11eam/UORccBoY8w5dv8MYIgx5gInz0Kb5xO7/4HNk7ICiYhMBCYCdO3adb+PP/44nqVmfPq2Nsbbt2rv1RgSXUyN0cqXXBUsZaWq/sjJtQvlbNdGZ5/jdR2FpS9oviaF+sL2OESH/K/fqQ/M4AmqF137rnrjHHyZepIEgmndIvXxXvmGCqGSXtqbmTNZr+Hj17Sntt9Z2pgu+oe6rn6xVpdq3O8sHR6/+WcNKJbOkAyqF5/9J+j9TfUCWb9IG4R3HtEh7cKn4MALosPnmrLhAx0FddhXjcT5LVWNsWahjqba99Me73+XqJDOy9cGv1UPuxTlCjVAnv1sVeTMKsq3q1DPqcGAePE/4cVfaYMy9FxNqyiHla/rjPR+J0LHAWF+Y9RgXL5N1SubV6meO51KZPuX6s7cY0T6Hvq/b1NV4rAL1G+/syM017+vAqD7cBWWq9/RRr9VD22kNn6sdqicXBg0HtrspUb1ygod8TnRTgG9t28/pI2JMYCoCmrjchhwao0CyAE663jTx9B1aPV51y0K1WabV6theMUs24laDX2O0RFIi3bRQHi7grXvamcwqW62fa6fuC0J9Bl57kptH8beUfP7VrpJ36ncPBhyrj7zLh++rO9Yszbq9NEAiMhcY0ziSkZfewHhMnjwYDNnzpxMWTwej8fjkElANISRehXgTpXsbNMS84hIHlAIJDj7ezwej6e+aAgB8SbQW0R6iEhj4GQgriR9Ghhvt8cBL9ab/cHj8Xg8iexyI7UxplxELgCeA3KBe40x74rIL4A5xpingcnAgyKyDPgMFSIej8fj2YU0SDRXY8x0YHos7RpnextwYvx3Ho/H49l1+JnUHo/H40nECwiPx+PxJOIFhMfj8XgS8QLC4/F4PIns8oly9YmIrAd2cio1rYGME/GyEF/m3QNf5uynNuXtZoxpk3QgqwREbRCROelmE2Yrvsy7B77M2U99ldermDwej8eTiBcQHo/H40nEC4iQe6rPknX4Mu8e+DJnP/VSXm+D8Hg8Hk8ifgTh8Xg8nkS8gPB4PB5PIru9gBCR0SKyRESWicgVDX09dYWIdBGRl0TkPRF5V0QutunFIvKCiCy1361suojIbfY+zBeRQZn/4f8XEckVkbdFZJrd7yEib9iyPWbDzCMi+XZ/mT3evSGve2cRkSIReVJEFovIIhEZlu31LCKX2Od6oYg8IiJNsq2eReReEVlnF1AL0na4XkVkvM2/VETGJ/1XOnZrASEiucAk4CigL3CKiPRt2KuqM8qBHxlj+gJDgfNt2a4AZhpjegMz7T7oPehtPxOBu3b9JdcZFwOLnP0bgZuNMb2AjcAEmz4B2GjTb7b5vo7cCjxrjOkD9EfLnrX1LCKdgIuAwcaYfdBlA04m++p5CjA6lrZD9SoixcC1wBDgAODaQKjUCGPMbvsBhgHPOftXAlc29HXVU1n/DhwOLAE62LQOwBK7fTdwipO/Kt/X6YOuUDgTGAVMAwSdYZoXr3N0TZJhdjvP5pOGLsMOlrcQ+Ch+3dlcz0AnYCVQbOttGnBkNtYz0B1YuLP1CpwC3O2kR/JV99mtRxCED1rAJzYtq7BD6oHAG0A7Y8xqe2gN0M5uZ8u9uAW4HKi0+yXAJmNMud13y1VVZnv8c5v/60QPYD1wn1Wr/VlEmpHF9WyMWQXcBKwAVqP1NpfsrueAHa3XWtX37i4gsh4RaQ48BfzQGLPZPWa0S5E1fs4i8i1gnTFmbkNfyy4kDxgE3GWMGQh8Sah2ALKynlsBY1Hh2BFoRqoqJuvZFfW6uwuIVUAXZ7+zTcsKRKQRKhweNsZMtclrRaSDPd4BWGfTs+FeHAQcKyLLgUdRNdOtQJGIBKsnuuWqKrM9Xghs2JUXXAd8AnxijHnD7j+JCoxsrudvAh8ZY9YbY8qAqWjdZ3M9B+xovdaqvnd3AfEm0Nt6PzRGDV1PN/A11QkiIuja3ouMMX9wDj0NBJ4M41HbRJB+pvWGGAp87gxlvxYYY640xnQ2xnRH6/JFY8xpwEvAOJstXubgXoyz+b9WPW1jzBpgpYjsZZMOA94ji+sZVS0NFZEC+5wHZc7aenbY0Xp9DjhCRFrZkdcRNq1mNLQRpqE/wBjgfeAD4OqGvp46LNdwdPg5H5hnP2NQ3etMYCkwAyi2+QX16PoAWIB6iDR4OWpR/kOBaXa7JzAbWAY8AeTb9CZ2f5k93rOhr3snyzoAmGPr+m9Aq2yvZ+DnwGJgIfAgkJ9t9Qw8gtpYytCR4oSdqVfgu7bsy4Czd+QafKgNj8fj8SSyu6uYPB6Px5MGLyA8Ho/Hk4gXEB6Px+NJxAsIj8fj8STiBYTH4/F4EvECwuOpBhGpEJF5zqfOov6KSHc3WqfH8/9EXvVZPJ7dnlJjzICGvgiPZ1fjRxAez04iIstF5LciskBEZotIL5veXURetHH5Z4pIV5veTkT+KiLv2M+B9lS5IvInu77B8yLS1Oa/SHQ9j/ki8mgDFdOzG+MFhMdTPU1jKqaTnGOfG2P6AXegkWQBbgfuN8bsCzwM3GbTbwNeMcb0R+MlvWvTewOTjDF7A5uAE2z6FcBAe54f1FfhPJ50+JnUHk81iMgWY0zzhPTlwChjzIc2MOIaY0yJiPwXjdlfZtNXG2Nai8h6oLMx5ivnHN2BF4wuAIOI/ARoZIz5lYg8C2xBw2f8zRizpZ6L6vFE8CMIj6d2mDTbO8JXznYFoW3waDS+ziDgTSdSqcezS/ACwuOpHSc537Ps9n/QaLIApwGv2u2ZwLlQtW52YbqTikgO0MUY8xLwEzREdcooxuOpT3yPxOOpnqYiMs/Zf9YYE7i6thKR+ego4BSbdiG6wtuP0dXezrbpFwP3iMgEdKRwLhqtM4lc4CErRAS4zRizqc5K5PHUAG+D8Hh2EmuDGGyM+W9DX4vHUx94FZPH4/F4EvEjCI/H4/Ek4kcQHo/H40nECwiPx+PxJOIFhMfj8XgS8QLC4/F4PIl4AeHxeDyeRP4H+oB9AcwmeGwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}