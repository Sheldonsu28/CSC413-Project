{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECtlZMbXHxLd",
        "outputId": "9278bbe1-3696-4c74-bc86-117f30ca15fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FOMahyixf7L6"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/hr\n",
        "!rm -rf /content/1.png\n",
        "!rm -rf /content/hr_temp\n",
        "!rm -rf /content/temp1\n",
        "!rm -rf /content/temp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thHq5jZCNfVh",
        "outputId": "81c0af49-c2e0-4812-80c3-25e59ab969ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/1.png': No such file or directory\n",
            "cp: cannot stat '/content/drive/MyDrive/hr_temp': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/drive/MyDrive/hr /content/hr\n",
        "!cp /content/drive/MyDrive/1.png /content/1.png\n",
        "!cp -r /content/drive/MyDrive/hr_temp /content/hr_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4TqWmtSSKFKv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import cv2 as cv\n",
        "import torch\n",
        "import imageio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FJOJzMsc7nt"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMk3C0qOHjv6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def calc_output_size(H, kernel_size, padding=0, dilation=1, stride=1):\n",
        "    return ((H + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
        "\n",
        "\n",
        "# Create annotation file\n",
        "def build_index_file(dir=\"hr\"):\n",
        "    file_names = [[filename] for filename in os.listdir(dir)]\n",
        "\n",
        "    np.savetxt(dir + \"/\" + dir + \".csv\",\n",
        "               file_names,\n",
        "               delimiter=\", \",\n",
        "               fmt='% s',\n",
        "               encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def crop_images(w, h, dir=\"hr\"):\n",
        "    file_names = [filename for filename in os.listdir(dir) if '.csv' not in filename]\n",
        "    folder_hr = 'hr'\n",
        "    for file in file_names:\n",
        "        image = cv.imread(os.path.join(dir, file))\n",
        "        img_size = image.shape\n",
        "        x = img_size[1] / 2 - w / 2\n",
        "        y = img_size[0] / 2 - h / 2\n",
        "        crop_img = image[int(y):int(y + h), int(x):int(x + w)]\n",
        "        cv.imwrite(os.path.join(folder_hr, file), crop_img)\n",
        "    build_index_file(dir=\"hr\")\n",
        "\n",
        "\n",
        "def to_var(tensor, device):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "\n",
        "    return Variable(tensor.float()).cuda(device)\n",
        "\n",
        "\n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "def create_image_grid(array, ncols=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_images, channels, cell_h, cell_w = array.shape\n",
        "    if not ncols:\n",
        "        ncols = int(np.sqrt(num_images))\n",
        "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
        "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
        "    for i in range(0, nrows):\n",
        "        for j in range(0, ncols):\n",
        "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
        "                                                                                                                 0)\n",
        "\n",
        "    if channels == 1:\n",
        "        result = result.squeeze()\n",
        "    return result\n",
        "\n",
        "\n",
        "def gan_save_samples(data, iteration, opts):\n",
        "    generated_images = to_data(data)\n",
        "\n",
        "    grid = create_image_grid(generated_images)\n",
        "\n",
        "    # merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
        "    imageio.imwrite(path, grid)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "\n",
        "\n",
        "# a = calc_output_size(256, 9, stride=1, padding=4)\n",
        "# b = calc_output_size(a, 5, stride=1, padding=2)\n",
        "# print(calc_output_size(b, 5, stride=1, padding=2))\n",
        "#crop_images(128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQHcRoG1c9Z8"
      },
      "source": [
        "## ESRGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Dne005dwKjPW"
      },
      "outputs": [],
      "source": [
        "from torch import cat\n",
        "from torch import nn\n",
        "from torch import flatten\n",
        "from torch import add\n",
        "\n",
        "\"\"\"\n",
        "Reference:\n",
        "https://arxiv.org/pdf/1809.00219v2.pdf\n",
        "\"\"\"\n",
        "global_beta = 0.2\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, (3, 3), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.RRDB_layers = nn.Sequential(*[RRDB(64, 32, global_beta) for i in range(16)])\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.upSample0 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64 * 4, (1, 1)),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, channels, (kernel_size, kernel_size), stride=(stride, stride), padding=padding),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.RRDB_layers(x1)\n",
        "        x3 = add(self.conv1(x2), x1)\n",
        "        x4 = self.upSample0(x3)\n",
        "        x5 = self.conv2(x4)\n",
        "        return self.conv3(x5)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 64, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=padding,\n",
        "                      bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.basicBlocks = nn.Sequential(\n",
        "            DiscriminatorBlock(64, 64, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(64, 128, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(128, 128, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(128, 256, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(256, 256, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(256, 512, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=4, stride=2, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=3, stride=stride, padding=padding),\n",
        "            DiscriminatorBlock(512, 512, kernel_size=4, stride=2, padding=padding),\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4, 100),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer1 = self.block1(x)\n",
        "        block_out = self.basicBlocks(layer1)\n",
        "        flattened = flatten(block_out, 1)\n",
        "        return self.block2(flattened)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size),\n",
        "                      stride=(stride, stride), padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block1(x)\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, channels, growth_rate, beta):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = DenseBlock(channels, growth_rate)\n",
        "        self.block1 = DenseBlock(channels, growth_rate)\n",
        "        self.block2 = DenseBlock(channels, growth_rate)\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block0(x)\n",
        "        skip0 = self.beta * x1 + x\n",
        "\n",
        "        x2 = self.block1(skip0)\n",
        "        skip1 = self.beta * x2 + skip0\n",
        "\n",
        "        x3 = self.block2(skip1)\n",
        "        return (skip1 + self.beta * x3) * self.beta + x\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Follows the design used in: https://arxiv.org/pdf/1809.00219v2.pdf\n",
        "    Original design: https://arxiv.org/pdf/1608.06993v5.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, growth_rate, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(nn.Conv2d(channels, growth_rate, (kernel_size, kernel_size), padding=padding),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(channels + growth_rate, growth_rate, (kernel_size, kernel_size), padding=padding),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(channels + 2 * growth_rate, growth_rate, (kernel_size, kernel_size), padding=padding),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(channels + 3 * growth_rate, growth_rate, (kernel_size, kernel_size), padding=padding),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(channels + 4 * growth_rate, channels, (kernel_size, kernel_size), padding=padding),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.conv1(cat((x, x1), 1))\n",
        "        x3 = self.conv2(cat((x, x1, x2), 1))\n",
        "        x4 = self.conv3(cat((x, x1, x2, x3), 1))\n",
        "        return self.conv4(cat((x, x1, x2, x3, x4), 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jBRIUFdDLn"
      },
      "source": [
        "# SRCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lKGPmsnMK3Pm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Reference:\n",
        "https://arxiv.org/pdf/1501.00092.pdf\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self, channel, f1=9, f2=5, f3=5, n1=64, n2=32):\n",
        "\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.channel = channel\n",
        "        self.block1 = nn.Conv2d(channel, n1, kernel_size=(f1, f1), bias=True, padding=f1 // 2)\n",
        "        self.block2 = nn.Conv2d(n1, n2, kernel_size=(f2, f2), bias=True, padding=f2 // 2)\n",
        "        self.block3 = nn.Conv2d(n2, channel, kernel_size=(f3, f3), bias=True, padding=f3 // 2)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        # self.upSample0 = nn.Sequential(\n",
        "        #     nn.Conv2d(3, 3 * 4, (1, 1)),\n",
        "        #     nn.PixelShuffle(2),\n",
        "        #     nn.ReLU(inplace=True)\n",
        "        # )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.activation(self.block1(x))\n",
        "        x2 = self.activation(self.block2(x1))\n",
        "        return self.block3(x2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT2mWll0dGrm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XaTiViEwK5g4"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.cuda import amp\n",
        "import torch.nn as nn\n",
        "from torchvision.models.vgg import vgg19\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    # https://github.com/Lornatang/ESRGAN-PyTorch/blob/a9031d71f6f27449fe63ec703344577e35fa87cb/model.py#L175\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "\n",
        "        vgg = vgg19(pretrained=True)\n",
        "        vgg_model = nn.Sequential(*list(vgg.features)[:19]).eval()\n",
        "        vgg_model.eval()\n",
        "\n",
        "        for param in vgg_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.vgg = vgg_model\n",
        "        self.loss_func = nn.L1Loss()\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        hr = self.vgg(hr)\n",
        "        sr = self.vgg(sr)\n",
        "        loss = self.loss_func(sr, hr)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CustomData(Dataset):\n",
        "    def __init__(self, model='SRCNN'):\n",
        "        self.names = np.loadtxt('hr/hr.csv', dtype='str', delimiter=\", \",\n",
        "                                encoding=\"utf-8\")\n",
        "\n",
        "        self.names = [name for name in self.names if \"csv\" not in name]\n",
        "        \n",
        "        self.hrs = [read_image(os.path.join('hr', name)).float() / 255.0 for name in self.names]\n",
        "        if model == 'SRCNN':\n",
        "          self.lr_transform = transforms.Compose([transforms.Resize((64, 64)), \n",
        "                                                  transforms.Resize((128, 128))])\n",
        "        else:\n",
        "          self.lr_transform = transforms.Compose([transforms.Resize((64, 64))])\n",
        "        self.lrs = [self.lr_transform(img) for img in self.hrs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.lrs[idx], self.hrs[idx]\n",
        "\n",
        "\n",
        "def train_SRCNN(args):\n",
        "    \"\"\"\n",
        "    Train SRCNN model\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Using device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "    model = SRCNN(args['out_channels'])\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda(device)\n",
        "\n",
        "    model.train()\n",
        "    params = model.parameters()\n",
        "    optimizer = Adam(params, args['learning_rate'], args['betas'])\n",
        "    # train with mse loss\n",
        "    loss = nn.MSELoss()\n",
        "    dataloader = DataLoader(CustomData(), batch_size=args['batch_size'], shuffle=True, num_workers=0,\n",
        "                            pin_memory=True)\n",
        "    train_iter = iter(dataloader)\n",
        "    train_loss = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    for i in range(args['epochs']):\n",
        "        try:\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        except StopIteration:\n",
        "            train_iter = iter(dataloader)\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with amp.autocast():\n",
        "          g = model(lr)\n",
        "          l = loss(g, hr)\n",
        "        \n",
        "        scaler.scale(l).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # l.backward()\n",
        "        # optimizer.step()\n",
        "        train_loss.append(l.item())\n",
        "\n",
        "        if l.item() < best_loss:\n",
        "          best_loss = l.item()\n",
        "          best_model = model.state_dict()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration {}/{}, training loss: {}'.format(i, args['epochs'], l))\n",
        "\n",
        "    plt.plot([i for i in range(args['epochs'])], train_loss)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('SRCNN.png')\n",
        "    pt_name = \"SRCNN_lr_{}.pt\".format(args['learning_rate'])\n",
        "    torch.save(best_model, pt_name)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    model.load_state_dict(torch.load(pt_name))\n",
        "    with torch.no_grad():\n",
        "        img = read_image('hr/0001.png').float()/255.0\n",
        "        # plt.figure(figsize=(10, 10))\n",
        "        # plt.imshow(img.T.numpy().astype(\"uint8\"))\n",
        "        # plt.axis(\"off\")\n",
        "        lr_transform = transforms.Compose([transforms.Resize((64, 64)), \n",
        "                                                  transforms.Resize((128, 128))])\n",
        "        tensor = lr_transform(torch.tensor(img))\n",
        "        out = model(to_var(tensor[None, :, :, :], device))\n",
        "        out = out.squeeze(0).cpu().detach()\n",
        "        save_image(out, \"SRCNN_out.png\")\n",
        "\n",
        "\n",
        "def train_ESRGAN(args):\n",
        "    \"\"\"\n",
        "    Train ESRGAN model\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Using device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "    G = Generator(args['out_channels'])\n",
        "    D = Discriminator(args['out_channels'])\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda(device)\n",
        "        D.cuda(device)\n",
        "\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "    params_G = G.parameters()\n",
        "    params_D = D.parameters()\n",
        "    num_params_G = sum(p.numel() for p in G.parameters() if p.requires_grad)\n",
        "    num_params_D = sum(p.numel() for p in D.parameters() if p.requires_grad)\n",
        "    print(\"Number of parameters in G: {}\".format(num_params_G))\n",
        "    print(\"Number of parameters in D: {}\".format(num_params_D))\n",
        "\n",
        "    optimizer_G = Adam(params_G, args['learning_rate'], args['betas'])\n",
        "    optimizer_D = Adam(params_D, args['learning_rate'], args['betas'])\n",
        "\n",
        "    D_loss_func = nn.BCEWithLogitsLoss()\n",
        "    G_pixel_loss_func = nn.L1Loss()\n",
        "    G_adversarial_loss_func = nn.BCEWithLogitsLoss()\n",
        "    G_content_loss_func = ContentLoss().cuda(device)\n",
        "\n",
        "    # Set up weights\n",
        "    adv_weight = args['adversarial_weight']\n",
        "    pixel_weight = args['pixel_weight']\n",
        "\n",
        "    dataloader = DataLoader(CustomData(model='ESRGAN'), batch_size=args['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    G_pixel_losses = []\n",
        "    G_losses_total = []\n",
        "    G_adversarial_losses = []\n",
        "    D_losses = []\n",
        "    D_fake_losses = []\n",
        "    D_real_losses = []\n",
        "\n",
        "    # Store the best Model\n",
        "    best_G = None\n",
        "    best_D = None\n",
        "    best_G_loss = float('inf')\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    for i in range(args['epochs']):\n",
        "        try:\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(dataloader)\n",
        "            lr, hr = train_iter.next()\n",
        "            lr, hr = to_var(lr, device), to_var(hr, device)\n",
        "\n",
        "        # D loss (Relativistic Loss)\n",
        "\n",
        "        for param in G.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        with amp.autocast():\n",
        "          fake_img = G(lr)\n",
        "\n",
        "          predict_fake = D(fake_img.detach())\n",
        "          predict_real = D(hr)\n",
        "\n",
        "          fake_loss = D_loss_func(predict_real - torch.mean(predict_fake), torch.ones_like(predict_real))\n",
        "          real_loss = D_loss_func(predict_fake - torch.mean(predict_real), torch.zeros_like(predict_fake))\n",
        "\n",
        "        # Code segment from PA4 DCGAN\n",
        "        # ---- Gradient Penalty ----\n",
        "        # if args['gradient_penalty']:\n",
        "          \n",
        "        #   alpha = torch.rand(hr.shape[0], 1, 1, 1)\n",
        "        #   alpha = alpha.expand_as(hr).cuda()\n",
        "        #   interp_images = Variable(alpha * hr.data + (1 - alpha) * fake_img.data,\n",
        "        #                           requires_grad=True).cuda()\n",
        "          \n",
        "        #   D_interp_output = D(interp_images)\n",
        "\n",
        "        #   gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
        "        #                                   grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
        "        #                                   create_graph=True, retain_graph=True)[0]\n",
        "          \n",
        "        #   gradients = gradients.view(hr.shape[0], -1)\n",
        "        #   gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "        #   gp = gradients_norm.mean()\n",
        "        # else:\n",
        "        #   gp = 0.0\n",
        "\n",
        "          d_loss = fake_loss + real_loss\n",
        "\n",
        "        scaler.scale(d_loss).backward()\n",
        "        scaler.step(optimizer_D)\n",
        "        # scaler.update()\n",
        "\n",
        "        # d_loss.backward()\n",
        "        # optimizer_D.step()\n",
        "\n",
        "        # G loss\n",
        "\n",
        "        for param in G.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "        for param in D.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        with amp.autocast():\n",
        "          fake_img = G(lr)\n",
        "          score_r = D(hr.detach())\n",
        "          score_f = D(fake_img)\n",
        "\n",
        "          adv_loss_real = G_adversarial_loss_func(score_f - torch.mean(score_r), torch.ones_like(score_r))\n",
        "          adv_loss_fake = G_adversarial_loss_func(score_r - torch.mean(score_f), torch.zeros_like(score_r))\n",
        "          adv_loss = adv_loss_real + adv_loss_fake\n",
        "          g_pixel_loss = G_pixel_loss_func(fake_img, hr)\n",
        "\n",
        "          content_loss = G_content_loss_func(fake_img, hr)\n",
        "\n",
        "          g_loss = adv_weight * adv_loss + pixel_weight * g_pixel_loss + content_loss\n",
        "\n",
        "        scaler.scale(g_loss).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "        scaler.update()\n",
        "\n",
        "        for param in D.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "        # g_loss.backward()\n",
        "        # optimizer_G.step()\n",
        "\n",
        "        G_losses_total.append(g_loss.item())\n",
        "        D_fake_losses.append(fake_loss.item())\n",
        "        D_real_losses.append(real_loss.item())\n",
        "\n",
        "        G_pixel_losses.append(g_pixel_loss.item())\n",
        "        G_adversarial_losses.append(adv_loss.item())\n",
        "        D_losses.append(d_loss.item())\n",
        "\n",
        "        if g_loss.item() < best_G_loss:\n",
        "          best_G_loss = g_loss.item()\n",
        "          best_D = D.state_dict()\n",
        "          best_G = G.state_dict()\n",
        "\n",
        "        # if i % 100 == 0:\n",
        "        print('Iteration {}/{}, G loss: {}, D loss: {}'.format(i, args['epochs'], g_loss, d_loss))\n",
        "\n",
        "        if i % 200 == 0:\n",
        "          G_weights_pt = \"ESRGAN_G_lr_{}_{}.pt\".format(args['learning_rate'], i)\n",
        "          D_weights_pt = \"ESRGAN_D_lr_{}_{}.pt\".format(args['learning_rate'], i)\n",
        "          torch.save(best_G, G_weights_pt)\n",
        "          torch.save(best_D, D_weights_pt)\n",
        "\n",
        "    x_axis = [i for i in range(args['epochs'])]\n",
        "    # plt.plot(x_axis, G_pixel_losses, label='G pixel loss')\n",
        "    plt.plot(x_axis, G_adversarial_losses, label='G loss' )\n",
        "    plt.plot(x_axis, D_losses, label='D loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('ESRGAN.png')\n",
        "\n",
        "    G_weights_pt = \"ESRGAN_G_lr_{}.pt\".format(args['learning_rate'])\n",
        "    D_weights_pt = \"ESRGAN_D_lr_{}.pt\".format(args['learning_rate'])\n",
        "    torch.save(best_G, G_weights_pt)\n",
        "    torch.save(best_D, D_weights_pt)\n",
        "\n",
        "    # G.load_state_dict(torch.load(G_weights_pt))\n",
        "    # D.load_state_dict(torch.load(D_weights_pt))\n",
        "\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    \n",
        "\n",
        "    np.savetxt(\"G_losses_total.csv\", np.asarray(G_losses_total), delimiter=\",\")\n",
        "    np.savetxt(\"D_fake_losses.csv\", np.asarray(D_fake_losses), delimiter=\",\")\n",
        "    np.savetxt(\"D_real_losses.csv\", np.asarray(D_real_losses), delimiter=\",\")\n",
        "\n",
        "    np.savetxt(\"G_pixel_losses.csv\", np.asarray(G_pixel_losses), delimiter=\",\")\n",
        "    np.savetxt(\"G_adversarial_losses.csv\", np.asarray(G_adversarial_losses), delimiter=\",\")\n",
        "    np.savetxt(\"D_losses.csv\", np.asarray(D_losses), delimiter=\",\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img = read_image('hr/0001.png').float() /255.0\n",
        "        # plt.figure(figsize=(10, 10))\n",
        "        # plt.imshow(img.T.numpy().astype(\"uint8\"))\n",
        "        # plt.axis(\"off\")\n",
        "        lr_transform = transforms.Compose([transforms.Resize((64, 64))])\n",
        "        tensor = lr_transform(torch.tensor(img))\n",
        "        out = G(to_var(tensor[None, :, :, :], device))\n",
        "        \n",
        "        out = out.squeeze(0).cpu().detach()\n",
        "        save_image(out, \"ESRGAN_out.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BFIXKKMzSZvk"
      },
      "outputs": [],
      "source": [
        "args = {'out_channels': 3,\n",
        "        'batch_size': 20,\n",
        "        'learning_rate': 1e-4,\n",
        "        'epochs': 6000,\n",
        "        'betas': [0.9, 0.999],\n",
        "        'gradient_penalty': False,  # This field is not used in SRCNN training\n",
        "        'pixel_weight': 1.0,       \n",
        "        'adversarial_weight': .001 # This field is not used in SRCNN training\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "sD6HCrCXSdbn",
        "outputId": "3f70ba65-461e-40fd-fd6e-46a6ecd087e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: Tesla T4\n",
            "Iteration 0/1000, training loss: 0.3227791488170624\n",
            "Iteration 100/1000, training loss: 0.007947875186800957\n",
            "Iteration 200/1000, training loss: 0.0035071843303740025\n",
            "Iteration 300/1000, training loss: 0.0030675113666802645\n",
            "Iteration 400/1000, training loss: 0.0014530635671690106\n",
            "Iteration 500/1000, training loss: 0.003244096180424094\n",
            "Iteration 600/1000, training loss: 0.002804366871714592\n",
            "Iteration 700/1000, training loss: 0.002872740849852562\n",
            "Iteration 800/1000, training loss: 0.003488966729491949\n",
            "Iteration 900/1000, training loss: 0.0011524069122970104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_SRCNN(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJaNSof0SeZi",
        "outputId": "f9db7b87-f2e5-432b-ff46-de15655f56ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 1000/6000, G loss: 0.6545177102088928, D loss: 2.9968538001412526e-05\n",
            "Iteration 1001/6000, G loss: 0.5806796550750732, D loss: 0.001293917652219534\n",
            "Iteration 1002/6000, G loss: 0.651491641998291, D loss: 0.12338763475418091\n",
            "Iteration 1003/6000, G loss: 0.7338296175003052, D loss: 0.005930301733314991\n",
            "Iteration 1004/6000, G loss: 0.6864088773727417, D loss: 0.03583860397338867\n",
            "Iteration 1005/6000, G loss: 0.6259036064147949, D loss: 0.4324507713317871\n",
            "Iteration 1006/6000, G loss: 0.6592927575111389, D loss: 0.19167150557041168\n",
            "Iteration 1007/6000, G loss: 0.641502857208252, D loss: 0.05371622368693352\n",
            "Iteration 1008/6000, G loss: 0.7329839468002319, D loss: 0.2508074939250946\n",
            "Iteration 1009/6000, G loss: 0.5713266134262085, D loss: 0.07897740602493286\n",
            "Iteration 1010/6000, G loss: 0.5729882121086121, D loss: 0.005635078065097332\n",
            "Iteration 1011/6000, G loss: 0.4974014461040497, D loss: 8.597002306487411e-05\n",
            "Iteration 1012/6000, G loss: 0.5880742073059082, D loss: 0.0020322015043348074\n",
            "Iteration 1013/6000, G loss: 0.7093729972839355, D loss: 1.4745923181180842e-05\n",
            "Iteration 1014/6000, G loss: 0.7557026743888855, D loss: 6.794924729547347e-07\n",
            "Iteration 1015/6000, G loss: 0.5417693257331848, D loss: 0.00265737297013402\n",
            "Iteration 1016/6000, G loss: 0.5272265672683716, D loss: 1.3866138458251953\n",
            "Iteration 1017/6000, G loss: 0.60923171043396, D loss: 0.010921983979642391\n",
            "Iteration 1018/6000, G loss: 0.7613214254379272, D loss: 0.01214446872472763\n",
            "Iteration 1019/6000, G loss: 0.5583667159080505, D loss: 0.3234438896179199\n",
            "Iteration 1020/6000, G loss: 0.6589282751083374, D loss: 0.00021016606478951871\n",
            "Iteration 1021/6000, G loss: 0.44834011793136597, D loss: 2.7725577354431152\n",
            "Iteration 1022/6000, G loss: 0.5353495478630066, D loss: 0.37042170763015747\n",
            "Iteration 1023/6000, G loss: 0.6013845801353455, D loss: 0.12707488238811493\n",
            "Iteration 1024/6000, G loss: 0.6681100726127625, D loss: 0.3042653203010559\n",
            "Iteration 1025/6000, G loss: 0.5959188938140869, D loss: 0.4946083426475525\n",
            "Iteration 1026/6000, G loss: 0.6965735554695129, D loss: 0.003019029274582863\n",
            "Iteration 1027/6000, G loss: 0.6384503245353699, D loss: 0.0003821976133622229\n",
            "Iteration 1028/6000, G loss: 0.5363086462020874, D loss: 0.0013721378054469824\n",
            "Iteration 1029/6000, G loss: 0.733517587184906, D loss: 8.213498404074926e-06\n",
            "Iteration 1030/6000, G loss: 0.7046223282814026, D loss: 7.018846372375265e-05\n",
            "Iteration 1031/6000, G loss: 0.5720038414001465, D loss: 6.831714563304558e-05\n",
            "Iteration 1032/6000, G loss: 0.5983254909515381, D loss: 0.00022087858815211803\n",
            "Iteration 1033/6000, G loss: 0.7498239278793335, D loss: 3.113707134616561e-05\n",
            "Iteration 1034/6000, G loss: 0.5049442052841187, D loss: 2.877625956898555e-05\n",
            "Iteration 1035/6000, G loss: 0.6535420417785645, D loss: 1.2278532039999845e-06\n",
            "Iteration 1036/6000, G loss: 0.6675584316253662, D loss: 1.4209654182195663e-05\n",
            "Iteration 1037/6000, G loss: 0.5739161372184753, D loss: 6.923368346178904e-05\n",
            "Iteration 1038/6000, G loss: 0.6709153652191162, D loss: 1.5717547285021283e-05\n",
            "Iteration 1039/6000, G loss: 0.6556075215339661, D loss: 9.832390060182661e-05\n",
            "Iteration 1040/6000, G loss: 0.5929681658744812, D loss: 4.6789386942691635e-06\n",
            "Iteration 1041/6000, G loss: 0.6440585851669312, D loss: 2.3632954253116623e-05\n",
            "Iteration 1042/6000, G loss: 0.5879330635070801, D loss: 0.002460347255691886\n",
            "Iteration 1043/6000, G loss: 0.5373885631561279, D loss: 0.0008168546482920647\n",
            "Iteration 1044/6000, G loss: 0.5992653965950012, D loss: 0.007349248509854078\n",
            "Iteration 1045/6000, G loss: 0.714148223400116, D loss: 0.000629155954811722\n",
            "Iteration 1046/6000, G loss: 0.5998225212097168, D loss: 0.002732914173975587\n",
            "Iteration 1047/6000, G loss: 0.5523227453231812, D loss: 0.08336903899908066\n",
            "Iteration 1048/6000, G loss: 0.6334860920906067, D loss: 0.010784356854856014\n",
            "Iteration 1049/6000, G loss: 0.6029995679855347, D loss: 0.010426506400108337\n",
            "Iteration 1050/6000, G loss: 0.5431536436080933, D loss: 0.17656420171260834\n",
            "Iteration 1051/6000, G loss: 0.6097561717033386, D loss: 0.0023352140560746193\n",
            "Iteration 1052/6000, G loss: 0.6067046523094177, D loss: 0.005900316871702671\n",
            "Iteration 1053/6000, G loss: 0.5409985780715942, D loss: 0.05157438665628433\n",
            "Iteration 1054/6000, G loss: 0.5178776383399963, D loss: 0.05583817511796951\n",
            "Iteration 1055/6000, G loss: 0.6459478139877319, D loss: 0.0006047955830581486\n",
            "Iteration 1056/6000, G loss: 0.6525803804397583, D loss: 2.5093511339946417e-06\n",
            "Iteration 1057/6000, G loss: 0.5680108666419983, D loss: 0.04383358359336853\n",
            "Iteration 1058/6000, G loss: 0.521623969078064, D loss: 0.29452553391456604\n",
            "Iteration 1059/6000, G loss: 0.6444481611251831, D loss: 0.753778338432312\n",
            "Iteration 1060/6000, G loss: 0.6666585803031921, D loss: 8.62096349010244e-05\n",
            "Iteration 1061/6000, G loss: 0.7098094820976257, D loss: 0.0012873574160039425\n",
            "Iteration 1062/6000, G loss: 0.6941226124763489, D loss: 0.06697866320610046\n",
            "Iteration 1063/6000, G loss: 0.5270012617111206, D loss: 0.5074678659439087\n",
            "Iteration 1064/6000, G loss: 0.6248056888580322, D loss: 0.4328582286834717\n",
            "Iteration 1065/6000, G loss: 0.686037003993988, D loss: 0.01650872454047203\n",
            "Iteration 1066/6000, G loss: 0.5999752879142761, D loss: 0.003711890894919634\n",
            "Iteration 1067/6000, G loss: 0.6114153861999512, D loss: 0.007574259769171476\n",
            "Iteration 1068/6000, G loss: 0.5857545733451843, D loss: 0.0009330706670880318\n",
            "Iteration 1069/6000, G loss: 0.6278859376907349, D loss: 0.0028680339455604553\n",
            "Iteration 1070/6000, G loss: 0.5375760793685913, D loss: 0.0024170270189642906\n",
            "Iteration 1071/6000, G loss: 0.5664759874343872, D loss: 0.0014449869049713016\n",
            "Iteration 1072/6000, G loss: 0.6350305676460266, D loss: 0.056824080646038055\n",
            "Iteration 1073/6000, G loss: 0.6147775650024414, D loss: 0.0036902204155921936\n",
            "Iteration 1074/6000, G loss: 0.6734620332717896, D loss: 0.00013589607260655612\n",
            "Iteration 1075/6000, G loss: 0.5959840416908264, D loss: 0.018690459430217743\n",
            "Iteration 1076/6000, G loss: 0.5820923447608948, D loss: 0.006672712974250317\n",
            "Iteration 1077/6000, G loss: 0.7018760442733765, D loss: 0.02012111246585846\n",
            "Iteration 1078/6000, G loss: 0.6674809455871582, D loss: 1.4662679177490645e-06\n",
            "Iteration 1079/6000, G loss: 0.5720255970954895, D loss: 0.009053658694028854\n",
            "Iteration 1080/6000, G loss: 0.5610952377319336, D loss: 0.8670582175254822\n",
            "Iteration 1081/6000, G loss: 0.5200584530830383, D loss: 0.000792239501606673\n",
            "Iteration 1082/6000, G loss: 0.7366414070129395, D loss: 0.04144371300935745\n",
            "Iteration 1083/6000, G loss: 0.6233333349227905, D loss: 0.1047048270702362\n",
            "Iteration 1084/6000, G loss: 0.48285818099975586, D loss: 0.006728245411068201\n",
            "Iteration 1085/6000, G loss: 0.6715218424797058, D loss: 0.006235470995306969\n",
            "Iteration 1086/6000, G loss: 0.6414755582809448, D loss: 3.1308940378949046e-05\n",
            "Iteration 1087/6000, G loss: 0.5412883758544922, D loss: 0.00017974946240428835\n",
            "Iteration 1088/6000, G loss: 0.6671876907348633, D loss: 5.9107449487783015e-05\n",
            "Iteration 1089/6000, G loss: 0.6765040159225464, D loss: 4.1066750782192685e-06\n",
            "Iteration 1090/6000, G loss: 0.5316470861434937, D loss: 0.0005612113163806498\n",
            "Iteration 1091/6000, G loss: 0.7292967438697815, D loss: 0.0001012724096653983\n",
            "Iteration 1092/6000, G loss: 0.6216697096824646, D loss: 0.00014024505799170583\n",
            "Iteration 1093/6000, G loss: 0.6419103145599365, D loss: 0.0009405699092894793\n",
            "Iteration 1094/6000, G loss: 0.5772051215171814, D loss: 0.0017689256928861141\n",
            "Iteration 1095/6000, G loss: 0.5887657403945923, D loss: 0.002454133704304695\n",
            "Iteration 1096/6000, G loss: 0.6492234468460083, D loss: 0.0001762711617629975\n",
            "Iteration 1097/6000, G loss: 0.6421414613723755, D loss: 0.3057025372982025\n",
            "Iteration 1098/6000, G loss: 0.5645296573638916, D loss: 0.0015163291245698929\n",
            "Iteration 1099/6000, G loss: 0.586127758026123, D loss: 0.00300548505038023\n",
            "Iteration 1100/6000, G loss: 0.6430785059928894, D loss: 0.0001569712330820039\n",
            "Iteration 1101/6000, G loss: 0.7056679725646973, D loss: 6.746116559952497e-05\n",
            "Iteration 1102/6000, G loss: 0.5681967735290527, D loss: 0.008236023597419262\n",
            "Iteration 1103/6000, G loss: 0.5337719917297363, D loss: 0.07351034879684448\n",
            "Iteration 1104/6000, G loss: 0.5462914109230042, D loss: 0.017943311482667923\n",
            "Iteration 1105/6000, G loss: 0.6330815553665161, D loss: 7.33483539079316e-05\n",
            "Iteration 1106/6000, G loss: 0.603549599647522, D loss: 0.0035029882565140724\n",
            "Iteration 1107/6000, G loss: 0.5673097968101501, D loss: 2.4735834358580178e-06\n",
            "Iteration 1108/6000, G loss: 0.4783034026622772, D loss: 0.2174181342124939\n",
            "Iteration 1109/6000, G loss: 0.5315889120101929, D loss: 0.0002809360157698393\n",
            "Iteration 1110/6000, G loss: 0.7320868372917175, D loss: 2.347800000279676e-05\n",
            "Iteration 1111/6000, G loss: 0.5308482646942139, D loss: 0.0006520365714095533\n",
            "Iteration 1112/6000, G loss: 0.6459139585494995, D loss: 0.0002205781202064827\n",
            "Iteration 1113/6000, G loss: 0.7372786402702332, D loss: 5.8142442867392674e-05\n",
            "Iteration 1114/6000, G loss: 0.508801281452179, D loss: 0.00123316771350801\n",
            "Iteration 1115/6000, G loss: 0.6508101224899292, D loss: 0.0007124283001758158\n",
            "Iteration 1116/6000, G loss: 0.551098108291626, D loss: 0.017981380224227905\n",
            "Iteration 1117/6000, G loss: 0.5649933815002441, D loss: 0.029503770172595978\n",
            "Iteration 1118/6000, G loss: 0.6506322026252747, D loss: 0.008696997538208961\n",
            "Iteration 1119/6000, G loss: 0.6392077803611755, D loss: 0.0001680854184087366\n",
            "Iteration 1120/6000, G loss: 0.7554106712341309, D loss: 8.441582758678123e-05\n",
            "Iteration 1121/6000, G loss: 0.6311061382293701, D loss: 9.167093594442122e-06\n",
            "Iteration 1122/6000, G loss: 0.5721390247344971, D loss: 8.070424883044325e-06\n",
            "Iteration 1123/6000, G loss: 0.6632806658744812, D loss: 0.0018539740703999996\n",
            "Iteration 1124/6000, G loss: 0.6579476594924927, D loss: 0.00038490165024995804\n",
            "Iteration 1125/6000, G loss: 0.6828949451446533, D loss: 3.133173595415428e-05\n",
            "Iteration 1126/6000, G loss: 0.5172364711761475, D loss: 1.3659073114395142\n",
            "Iteration 1127/6000, G loss: 0.6121355295181274, D loss: 0.002266798634082079\n",
            "Iteration 1128/6000, G loss: 0.48302313685417175, D loss: 0.950769305229187\n",
            "Iteration 1129/6000, G loss: 0.6380234956741333, D loss: 0.0035907987039536238\n",
            "Iteration 1130/6000, G loss: 0.5858949422836304, D loss: 0.001554260030388832\n",
            "Iteration 1131/6000, G loss: 0.6701045632362366, D loss: 1.4245501915866043e-06\n",
            "Iteration 1132/6000, G loss: 0.570391058921814, D loss: 0.007115054875612259\n",
            "Iteration 1133/6000, G loss: 0.5715281963348389, D loss: 9.788875468075275e-05\n",
            "Iteration 1134/6000, G loss: 0.6205084323883057, D loss: 0.00010190674220211804\n",
            "Iteration 1135/6000, G loss: 0.5933640003204346, D loss: 3.3974643542933336e-07\n",
            "Iteration 1136/6000, G loss: 0.6758674383163452, D loss: 5.066393669039826e-07\n",
            "Iteration 1137/6000, G loss: 0.717464804649353, D loss: 5.781648724223487e-07\n",
            "Iteration 1138/6000, G loss: 0.6070616841316223, D loss: 2.391856469330378e-05\n",
            "Iteration 1139/6000, G loss: 0.494406521320343, D loss: 0.0007673458894714713\n",
            "Iteration 1140/6000, G loss: 0.5745930075645447, D loss: 0.009376134723424911\n",
            "Iteration 1141/6000, G loss: 0.6392003893852234, D loss: 0.0004955257754772902\n",
            "Iteration 1142/6000, G loss: 0.6355887651443481, D loss: 0.034630246460437775\n",
            "Iteration 1143/6000, G loss: 0.5535051822662354, D loss: 0.03650333732366562\n",
            "Iteration 1144/6000, G loss: 0.5732352137565613, D loss: 1.1287767887115479\n",
            "Iteration 1145/6000, G loss: 0.5431724786758423, D loss: 0.04256138205528259\n",
            "Iteration 1146/6000, G loss: 0.6390177607536316, D loss: 0.09953941404819489\n",
            "Iteration 1147/6000, G loss: 0.6091338396072388, D loss: 0.00025938384351320565\n",
            "Iteration 1148/6000, G loss: 0.5477065443992615, D loss: 0.11746862530708313\n",
            "Iteration 1149/6000, G loss: 0.6615685224533081, D loss: 3.1548428523819894e-05\n",
            "Iteration 1150/6000, G loss: 0.6133604049682617, D loss: 0.0005183022003620863\n",
            "Iteration 1151/6000, G loss: 0.49700307846069336, D loss: 0.005642390809953213\n",
            "Iteration 1152/6000, G loss: 0.5842940807342529, D loss: 0.0015620056074112654\n",
            "Iteration 1153/6000, G loss: 0.7119253873825073, D loss: 4.887579052592628e-07\n",
            "Iteration 1154/6000, G loss: 0.5751596689224243, D loss: 7.697474939050153e-05\n",
            "Iteration 1155/6000, G loss: 0.6542149186134338, D loss: 1.267149218620034e-05\n",
            "Iteration 1156/6000, G loss: 0.5982106328010559, D loss: 0.002350870054215193\n",
            "Iteration 1157/6000, G loss: 0.580488383769989, D loss: 2.437825514789438e-06\n",
            "Iteration 1158/6000, G loss: 0.5995294451713562, D loss: 5.897590745007619e-05\n",
            "Iteration 1159/6000, G loss: 0.6024666428565979, D loss: 0.0022644882556051016\n",
            "Iteration 1160/6000, G loss: 0.5632661581039429, D loss: 0.056267060339450836\n",
            "Iteration 1161/6000, G loss: 0.5216485857963562, D loss: 0.01897001639008522\n",
            "Iteration 1162/6000, G loss: 0.6483407020568848, D loss: 0.0329643189907074\n",
            "Iteration 1163/6000, G loss: 0.5236546397209167, D loss: 0.000845119240693748\n",
            "Iteration 1164/6000, G loss: 0.5387784242630005, D loss: 0.020937995985150337\n",
            "Iteration 1165/6000, G loss: 0.6133325099945068, D loss: 0.001166942878626287\n",
            "Iteration 1166/6000, G loss: 0.6195920705795288, D loss: 5.7360091886948794e-05\n",
            "Iteration 1167/6000, G loss: 0.6855230927467346, D loss: 8.193528628908098e-05\n",
            "Iteration 1168/6000, G loss: 0.5664585828781128, D loss: 5.53997015231289e-05\n",
            "Iteration 1169/6000, G loss: 0.5730901956558228, D loss: 0.11886096000671387\n",
            "Iteration 1170/6000, G loss: 0.5930873155593872, D loss: 2.0821118354797363\n",
            "Iteration 1171/6000, G loss: 0.5738584399223328, D loss: 9.155125553661492e-06\n",
            "Iteration 1172/6000, G loss: 0.7765756249427795, D loss: 5.960464122267695e-09\n",
            "Iteration 1173/6000, G loss: 0.6906408667564392, D loss: 0.0003572588902898133\n",
            "Iteration 1174/6000, G loss: 0.658420205116272, D loss: 0.0002491309423930943\n",
            "Iteration 1175/6000, G loss: 0.6946738958358765, D loss: 6.998138269409537e-05\n",
            "Iteration 1176/6000, G loss: 0.6869761943817139, D loss: 1.0728830091011332e-07\n",
            "Iteration 1177/6000, G loss: 0.5317960977554321, D loss: 0.016485000029206276\n",
            "Iteration 1178/6000, G loss: 0.4881128668785095, D loss: 0.02317049354314804\n",
            "Iteration 1179/6000, G loss: 0.6276777982711792, D loss: 0.0004351352108642459\n",
            "Iteration 1180/6000, G loss: 0.5394867658615112, D loss: 3.228427158319391e-05\n",
            "Iteration 1181/6000, G loss: 0.6743752956390381, D loss: 0.0007551590097136796\n",
            "Iteration 1182/6000, G loss: 0.4893357753753662, D loss: 0.049776773899793625\n",
            "Iteration 1183/6000, G loss: 0.5396260023117065, D loss: 0.004644810687750578\n",
            "Iteration 1184/6000, G loss: 0.6721935868263245, D loss: 0.0019231205806136131\n",
            "Iteration 1185/6000, G loss: 0.5477395057678223, D loss: 0.0006566855008713901\n",
            "Iteration 1186/6000, G loss: 0.5933111310005188, D loss: 0.0013484670780599117\n",
            "Iteration 1187/6000, G loss: 0.5930591821670532, D loss: 0.0006116005824878812\n",
            "Iteration 1188/6000, G loss: 0.4957719147205353, D loss: 0.00047485355753451586\n",
            "Iteration 1189/6000, G loss: 0.5334264636039734, D loss: 0.014275197871029377\n",
            "Iteration 1190/6000, G loss: 0.5932642221450806, D loss: 0.00040403573075309396\n",
            "Iteration 1191/6000, G loss: 0.5398709177970886, D loss: 0.0004554990155156702\n",
            "Iteration 1192/6000, G loss: 0.6236718893051147, D loss: 0.035418011248111725\n",
            "Iteration 1193/6000, G loss: 0.6351395845413208, D loss: 0.0026187379844486713\n",
            "Iteration 1194/6000, G loss: 0.4995812773704529, D loss: 0.0064437538385391235\n",
            "Iteration 1195/6000, G loss: 0.7321723699569702, D loss: 0.00022498852922581136\n",
            "Iteration 1196/6000, G loss: 0.6891612410545349, D loss: 0.04044771194458008\n",
            "Iteration 1197/6000, G loss: 0.5617790818214417, D loss: 0.05060327798128128\n",
            "Iteration 1198/6000, G loss: 0.6573021411895752, D loss: 0.1384221762418747\n",
            "Iteration 1199/6000, G loss: 0.562241792678833, D loss: 0.018438078463077545\n",
            "Iteration 1200/6000, G loss: 0.5967764258384705, D loss: 0.07795602083206177\n",
            "Iteration 1201/6000, G loss: 0.45369723439216614, D loss: 0.0563507154583931\n",
            "Iteration 1202/6000, G loss: 0.5195667147636414, D loss: 0.10208546370267868\n",
            "Iteration 1203/6000, G loss: 0.6254326701164246, D loss: 0.00047284195898100734\n",
            "Iteration 1204/6000, G loss: 0.6329948902130127, D loss: 1.424550305273442e-06\n",
            "Iteration 1205/6000, G loss: 0.5735064744949341, D loss: 0.00010492399451322854\n",
            "Iteration 1206/6000, G loss: 0.5719005465507507, D loss: 0.0004905951209366322\n",
            "Iteration 1207/6000, G loss: 0.6442815065383911, D loss: 7.569787499051017e-07\n",
            "Iteration 1208/6000, G loss: 0.7419794797897339, D loss: 1.5276316844392568e-05\n",
            "Iteration 1209/6000, G loss: 0.6801455020904541, D loss: 3.9044625737005845e-05\n",
            "Iteration 1210/6000, G loss: 0.6687999963760376, D loss: 5.872197652934119e-05\n",
            "Iteration 1211/6000, G loss: 0.6955373883247375, D loss: 6.25848599611345e-07\n",
            "Iteration 1212/6000, G loss: 0.5849027037620544, D loss: 0.0001745613553794101\n",
            "Iteration 1213/6000, G loss: 0.4129480719566345, D loss: 0.13915959000587463\n",
            "Iteration 1214/6000, G loss: 0.5290993452072144, D loss: 0.012156466953456402\n",
            "Iteration 1215/6000, G loss: 0.6653348207473755, D loss: 4.86533172079362e-05\n",
            "Iteration 1216/6000, G loss: 0.5638494491577148, D loss: 0.2277354598045349\n",
            "Iteration 1217/6000, G loss: 0.5509588718414307, D loss: 0.0730755478143692\n",
            "Iteration 1218/6000, G loss: 0.5896571278572083, D loss: 0.002326149959117174\n",
            "Iteration 1219/6000, G loss: 0.5271956920623779, D loss: 0.0005604900070466101\n",
            "Iteration 1220/6000, G loss: 0.5596867203712463, D loss: 0.005737911909818649\n",
            "Iteration 1221/6000, G loss: 0.6703310608863831, D loss: 0.0006985711515881121\n",
            "Iteration 1222/6000, G loss: 0.5536326169967651, D loss: 0.1760803461074829\n",
            "Iteration 1223/6000, G loss: 0.5967501997947693, D loss: 0.45651987195014954\n",
            "Iteration 1224/6000, G loss: 0.615706205368042, D loss: 0.002338527236133814\n",
            "Iteration 1225/6000, G loss: 0.49885880947113037, D loss: 0.27600276470184326\n",
            "Iteration 1226/6000, G loss: 0.5587105751037598, D loss: 0.6887112855911255\n",
            "Iteration 1227/6000, G loss: 0.7066960334777832, D loss: 5.986550968373194e-05\n",
            "Iteration 1228/6000, G loss: 0.536013126373291, D loss: 0.0004295386897865683\n",
            "Iteration 1229/6000, G loss: 0.6781672835350037, D loss: 0.5094382762908936\n",
            "Iteration 1230/6000, G loss: 0.6794097423553467, D loss: 0.08837388455867767\n",
            "Iteration 1231/6000, G loss: 0.7536413073539734, D loss: 0.10470546782016754\n",
            "Iteration 1232/6000, G loss: 0.6646369099617004, D loss: 0.00017159587878268212\n",
            "Iteration 1233/6000, G loss: 0.6813420057296753, D loss: 0.03456863760948181\n",
            "Iteration 1234/6000, G loss: 0.6555538177490234, D loss: 0.1349734216928482\n",
            "Iteration 1235/6000, G loss: 0.6160135269165039, D loss: 1.3891364336013794\n",
            "Iteration 1236/6000, G loss: 0.4633710980415344, D loss: 2.3341541290283203\n",
            "Iteration 1237/6000, G loss: 0.5733156800270081, D loss: 2.0725791454315186\n",
            "Iteration 1238/6000, G loss: 0.571382462978363, D loss: 1.8618695735931396\n",
            "Iteration 1239/6000, G loss: 0.6306058168411255, D loss: 1.3528475761413574\n",
            "Iteration 1240/6000, G loss: 0.6188139915466309, D loss: 0.7655525207519531\n",
            "Iteration 1241/6000, G loss: 0.7428531646728516, D loss: 0.12966465950012207\n",
            "Iteration 1242/6000, G loss: 0.5134835839271545, D loss: 0.5086791515350342\n",
            "Iteration 1243/6000, G loss: 0.7048754692077637, D loss: 0.08117861300706863\n",
            "Iteration 1244/6000, G loss: 0.7521732449531555, D loss: 0.16962137818336487\n",
            "Iteration 1245/6000, G loss: 0.602419912815094, D loss: 0.005306527949869633\n",
            "Iteration 1246/6000, G loss: 0.6544805765151978, D loss: 0.04955710470676422\n",
            "Iteration 1247/6000, G loss: 0.4940372705459595, D loss: 0.049045875668525696\n",
            "Iteration 1248/6000, G loss: 0.5998701453208923, D loss: 0.0761163979768753\n",
            "Iteration 1249/6000, G loss: 0.7342482805252075, D loss: 0.023590944707393646\n",
            "Iteration 1250/6000, G loss: 0.6329748630523682, D loss: 0.05477602779865265\n",
            "Iteration 1251/6000, G loss: 0.48623254895210266, D loss: 0.39826446771621704\n",
            "Iteration 1252/6000, G loss: 0.727474570274353, D loss: 0.0024382215924561024\n",
            "Iteration 1253/6000, G loss: 0.5543996095657349, D loss: 0.029535707086324692\n",
            "Iteration 1254/6000, G loss: 0.5644927620887756, D loss: 0.010159238241612911\n",
            "Iteration 1255/6000, G loss: 0.6414241790771484, D loss: 0.0028586366679519415\n",
            "Iteration 1256/6000, G loss: 0.5685032606124878, D loss: 0.005917401053011417\n",
            "Iteration 1257/6000, G loss: 0.582470178604126, D loss: 0.008085322566330433\n",
            "Iteration 1258/6000, G loss: 0.6034870743751526, D loss: 0.04864944890141487\n",
            "Iteration 1259/6000, G loss: 0.56076979637146, D loss: 0.024099105969071388\n",
            "Iteration 1260/6000, G loss: 0.6177358031272888, D loss: 0.007295606192201376\n",
            "Iteration 1261/6000, G loss: 0.5486868023872375, D loss: 0.016428962349891663\n",
            "Iteration 1262/6000, G loss: 0.7499098181724548, D loss: 0.003077611792832613\n",
            "Iteration 1263/6000, G loss: 0.6941500902175903, D loss: 0.019099842756986618\n",
            "Iteration 1264/6000, G loss: 0.5071120858192444, D loss: 0.3166365921497345\n",
            "Iteration 1265/6000, G loss: 0.5682061314582825, D loss: 0.012727353721857071\n",
            "Iteration 1266/6000, G loss: 0.6434119939804077, D loss: 0.029241152107715607\n",
            "Iteration 1267/6000, G loss: 0.549266517162323, D loss: 0.03389516472816467\n",
            "Iteration 1268/6000, G loss: 0.5346062183380127, D loss: 0.040584955364465714\n",
            "Iteration 1269/6000, G loss: 0.5042759776115417, D loss: 7.239614933496341e-05\n",
            "Iteration 1270/6000, G loss: 0.6184413433074951, D loss: 0.027938062325119972\n",
            "Iteration 1271/6000, G loss: 0.5861014723777771, D loss: 0.004336295183748007\n",
            "Iteration 1272/6000, G loss: 0.6273399591445923, D loss: 0.0002570252399891615\n",
            "Iteration 1273/6000, G loss: 0.5527687072753906, D loss: 0.006040163338184357\n",
            "Iteration 1274/6000, G loss: 0.5815661549568176, D loss: 0.0016652573831379414\n",
            "Iteration 1275/6000, G loss: 0.5313965082168579, D loss: 0.023610824719071388\n",
            "Iteration 1276/6000, G loss: 0.5225365161895752, D loss: 0.006431951653212309\n",
            "Iteration 1277/6000, G loss: 0.6736090183258057, D loss: 0.06315099447965622\n",
            "Iteration 1278/6000, G loss: 0.517243504524231, D loss: 0.1838010549545288\n",
            "Iteration 1279/6000, G loss: 0.5029390454292297, D loss: 0.3017280697822571\n",
            "Iteration 1280/6000, G loss: 0.6068291664123535, D loss: 0.10669007897377014\n",
            "Iteration 1281/6000, G loss: 0.5296353697776794, D loss: 1.3452677194436546e-05\n",
            "Iteration 1282/6000, G loss: 0.5908101201057434, D loss: 7.68775207689032e-05\n",
            "Iteration 1283/6000, G loss: 0.6950598955154419, D loss: 0.0003866241895593703\n",
            "Iteration 1284/6000, G loss: 0.6689992547035217, D loss: 4.6178563934518024e-05\n",
            "Iteration 1285/6000, G loss: 0.6356745958328247, D loss: 0.0033282628282904625\n",
            "Iteration 1286/6000, G loss: 0.5978191494941711, D loss: 0.02875611186027527\n",
            "Iteration 1287/6000, G loss: 0.5076824426651001, D loss: 0.0060975379310548306\n",
            "Iteration 1288/6000, G loss: 0.6168715953826904, D loss: 0.33513009548187256\n",
            "Iteration 1289/6000, G loss: 0.6912657022476196, D loss: 0.08957074582576752\n",
            "Iteration 1290/6000, G loss: 0.6629964709281921, D loss: 0.0003518576268106699\n",
            "Iteration 1291/6000, G loss: 0.43101054430007935, D loss: 3.334799289703369\n",
            "Iteration 1292/6000, G loss: 0.5828472375869751, D loss: 0.018588252365589142\n",
            "Iteration 1293/6000, G loss: 0.6712919473648071, D loss: 0.3386382460594177\n",
            "Iteration 1294/6000, G loss: 0.5739988088607788, D loss: 0.01777002587914467\n",
            "Iteration 1295/6000, G loss: 0.5312721133232117, D loss: 0.13080820441246033\n",
            "Iteration 1296/6000, G loss: 0.5051578283309937, D loss: 0.11646884679794312\n",
            "Iteration 1297/6000, G loss: 0.5352397561073303, D loss: 0.018049020320177078\n",
            "Iteration 1298/6000, G loss: 0.5653876066207886, D loss: 0.018495870754122734\n",
            "Iteration 1299/6000, G loss: 0.6610433459281921, D loss: 0.0032930278684943914\n",
            "Iteration 1300/6000, G loss: 0.5496941804885864, D loss: 0.06389960646629333\n",
            "Iteration 1301/6000, G loss: 0.5332655906677246, D loss: 0.022571776062250137\n",
            "Iteration 1302/6000, G loss: 0.5252634286880493, D loss: 0.01213070098310709\n",
            "Iteration 1303/6000, G loss: 0.5924413800239563, D loss: 0.0004226876772008836\n",
            "Iteration 1304/6000, G loss: 0.5079361200332642, D loss: 0.012157060205936432\n",
            "Iteration 1305/6000, G loss: 0.5858263373374939, D loss: 0.0028100914787501097\n",
            "Iteration 1306/6000, G loss: 0.6104022264480591, D loss: 0.005099743604660034\n",
            "Iteration 1307/6000, G loss: 0.6339411735534668, D loss: 0.03688492998480797\n",
            "Iteration 1308/6000, G loss: 0.5770093202590942, D loss: 0.013042102567851543\n",
            "Iteration 1309/6000, G loss: 0.6077591776847839, D loss: 0.0315588004887104\n",
            "Iteration 1310/6000, G loss: 0.673987627029419, D loss: 0.00032269928487949073\n",
            "Iteration 1311/6000, G loss: 0.7073301672935486, D loss: 0.00679416861385107\n",
            "Iteration 1312/6000, G loss: 0.5763683319091797, D loss: 0.010417522862553596\n",
            "Iteration 1313/6000, G loss: 0.5598101019859314, D loss: 0.01906152442097664\n",
            "Iteration 1314/6000, G loss: 0.5714507102966309, D loss: 0.0006193930166773498\n",
            "Iteration 1315/6000, G loss: 0.5790458917617798, D loss: 0.014485711231827736\n",
            "Iteration 1316/6000, G loss: 0.7433056235313416, D loss: 0.00011575118696782738\n",
            "Iteration 1317/6000, G loss: 0.47386428713798523, D loss: 0.12229903042316437\n",
            "Iteration 1318/6000, G loss: 0.6492149829864502, D loss: 0.009976984933018684\n",
            "Iteration 1319/6000, G loss: 0.6295837759971619, D loss: 0.01680518500506878\n",
            "Iteration 1320/6000, G loss: 0.49437594413757324, D loss: 1.0100743770599365\n",
            "Iteration 1321/6000, G loss: 0.6347657442092896, D loss: 0.0007401014445349574\n",
            "Iteration 1322/6000, G loss: 0.6096612215042114, D loss: 0.003373237792402506\n",
            "Iteration 1323/6000, G loss: 0.6775222420692444, D loss: 9.709536243462935e-06\n",
            "Iteration 1324/6000, G loss: 0.4687454402446747, D loss: 0.020981600508093834\n",
            "Iteration 1325/6000, G loss: 0.628602921962738, D loss: 0.01546245627105236\n",
            "Iteration 1326/6000, G loss: 0.6206655502319336, D loss: 0.002727359067648649\n",
            "Iteration 1327/6000, G loss: 0.5964030623435974, D loss: 0.0017487815348431468\n",
            "Iteration 1328/6000, G loss: 0.6443448066711426, D loss: 0.37822961807250977\n",
            "Iteration 1329/6000, G loss: 0.6209871768951416, D loss: 0.313443660736084\n",
            "Iteration 1330/6000, G loss: 0.5669092535972595, D loss: 0.09860999882221222\n",
            "Iteration 1331/6000, G loss: 0.4603111147880554, D loss: 0.009831670671701431\n",
            "Iteration 1332/6000, G loss: 0.6541022658348083, D loss: 1.348766454611905e-05\n",
            "Iteration 1333/6000, G loss: 0.5528175830841064, D loss: 0.0012085878988727927\n",
            "Iteration 1334/6000, G loss: 0.6759141087532043, D loss: 0.0005198850994929671\n",
            "Iteration 1335/6000, G loss: 0.7113196849822998, D loss: 0.0028489527758210897\n",
            "Iteration 1336/6000, G loss: 0.5694926381111145, D loss: 0.0036966309417039156\n",
            "Iteration 1337/6000, G loss: 0.5389131903648376, D loss: 0.0019208135781809688\n",
            "Iteration 1338/6000, G loss: 0.5652292966842651, D loss: 0.11489101499319077\n",
            "Iteration 1339/6000, G loss: 0.5562934875488281, D loss: 0.324556440114975\n",
            "Iteration 1340/6000, G loss: 0.4805946946144104, D loss: 0.03659149631857872\n",
            "Iteration 1341/6000, G loss: 0.47720688581466675, D loss: 0.001989417476579547\n",
            "Iteration 1342/6000, G loss: 0.587044894695282, D loss: 0.00028769130585715175\n",
            "Iteration 1343/6000, G loss: 0.6423198580741882, D loss: 0.003255293006077409\n",
            "Iteration 1344/6000, G loss: 0.5088022947311401, D loss: 0.009005000814795494\n",
            "Iteration 1345/6000, G loss: 0.7635167837142944, D loss: 7.47301965020597e-05\n",
            "Iteration 1346/6000, G loss: 0.5830676555633545, D loss: 0.0034476383589208126\n",
            "Iteration 1347/6000, G loss: 0.6114122867584229, D loss: 0.21026529371738434\n",
            "Iteration 1348/6000, G loss: 0.7086611390113831, D loss: 1.8862746953964233\n",
            "Iteration 1349/6000, G loss: 0.6575636267662048, D loss: 2.1457668708535493e-07\n",
            "Iteration 1350/6000, G loss: 0.6532103419303894, D loss: 0.00039310764987021685\n",
            "Iteration 1351/6000, G loss: 0.6169767379760742, D loss: 0.0007306984625756741\n",
            "Iteration 1352/6000, G loss: 0.6474699974060059, D loss: 1.645085717427719e-06\n",
            "Iteration 1353/6000, G loss: 0.5603793263435364, D loss: 3.303047560621053e-05\n",
            "Iteration 1354/6000, G loss: 0.6406243443489075, D loss: 2.4795467652438674e-06\n",
            "Iteration 1355/6000, G loss: 0.6076482534408569, D loss: 9.000283398563624e-07\n",
            "Iteration 1356/6000, G loss: 0.6322391033172607, D loss: 2.777557483568671e-06\n",
            "Iteration 1357/6000, G loss: 0.516639769077301, D loss: 6.598204436158994e-06\n",
            "Iteration 1358/6000, G loss: 0.696482241153717, D loss: 1.25169719922269e-07\n",
            "Iteration 1359/6000, G loss: 0.6076115965843201, D loss: 2.965670319099445e-05\n",
            "Iteration 1360/6000, G loss: 0.6110100746154785, D loss: 2.410280649201013e-05\n",
            "Iteration 1361/6000, G loss: 0.6006646156311035, D loss: 3.0278856684162747e-06\n",
            "Iteration 1362/6000, G loss: 0.62677401304245, D loss: 1.7881387748275301e-07\n",
            "Iteration 1363/6000, G loss: 0.4812493622303009, D loss: 1.2939630323671736e-05\n",
            "Iteration 1364/6000, G loss: 0.5959722399711609, D loss: 1.6271890217467444e-06\n",
            "Iteration 1365/6000, G loss: 0.5573367476463318, D loss: 3.069623289775336e-06\n",
            "Iteration 1366/6000, G loss: 0.5934666395187378, D loss: 5.042351403972134e-06\n",
            "Iteration 1367/6000, G loss: 0.7330891489982605, D loss: 1.2040073897878756e-06\n",
            "Iteration 1368/6000, G loss: 0.6585211753845215, D loss: 0.0008849308942444623\n",
            "Iteration 1369/6000, G loss: 0.5804013609886169, D loss: 0.005922614596784115\n",
            "Iteration 1370/6000, G loss: 0.6311119794845581, D loss: 0.005218716338276863\n",
            "Iteration 1371/6000, G loss: 0.635353684425354, D loss: 0.0014413191238418221\n",
            "Iteration 1372/6000, G loss: 0.6653711199760437, D loss: 4.278120104572736e-05\n",
            "Iteration 1373/6000, G loss: 0.624038577079773, D loss: 0.00842821691185236\n",
            "Iteration 1374/6000, G loss: 0.5924293994903564, D loss: 2.7748817956307903e-05\n",
            "Iteration 1375/6000, G loss: 0.5002358555793762, D loss: 0.0006195456953719258\n",
            "Iteration 1376/6000, G loss: 0.629952073097229, D loss: 4.7472109145019203e-05\n",
            "Iteration 1377/6000, G loss: 0.6321213841438293, D loss: 4.977307980880141e-05\n",
            "Iteration 1378/6000, G loss: 0.6264866590499878, D loss: 0.002686411142349243\n",
            "Iteration 1379/6000, G loss: 0.5176966190338135, D loss: 0.0003180369676556438\n",
            "Iteration 1380/6000, G loss: 0.5506138205528259, D loss: 0.16954275965690613\n",
            "Iteration 1381/6000, G loss: 0.8304288387298584, D loss: 0.03927156329154968\n",
            "Iteration 1382/6000, G loss: 0.58026522397995, D loss: 0.04628262296319008\n",
            "Iteration 1383/6000, G loss: 0.5070124864578247, D loss: 4.3980755435768515e-05\n",
            "Iteration 1384/6000, G loss: 0.5108556747436523, D loss: 0.6521105170249939\n",
            "Iteration 1385/6000, G loss: 0.5535249710083008, D loss: 0.07596101611852646\n",
            "Iteration 1386/6000, G loss: 0.703578770160675, D loss: 0.32546767592430115\n",
            "Iteration 1387/6000, G loss: 0.6074740886688232, D loss: 0.12756702303886414\n",
            "Iteration 1388/6000, G loss: 0.5474885106086731, D loss: 0.16058430075645447\n",
            "Iteration 1389/6000, G loss: 0.5835707783699036, D loss: 2.360329744988121e-06\n",
            "Iteration 1390/6000, G loss: 0.6658007502555847, D loss: 0.1904684603214264\n",
            "Iteration 1391/6000, G loss: 0.5386319756507874, D loss: 0.5546981692314148\n",
            "Iteration 1392/6000, G loss: 0.4871470034122467, D loss: 1.7785813808441162\n",
            "Iteration 1393/6000, G loss: 0.5207595229148865, D loss: 0.7069185376167297\n",
            "Iteration 1394/6000, G loss: 0.6498166918754578, D loss: 0.0006022877641953528\n",
            "Iteration 1395/6000, G loss: 0.563032329082489, D loss: 0.00855567492544651\n",
            "Iteration 1396/6000, G loss: 0.6520041823387146, D loss: 0.08513227105140686\n",
            "Iteration 1397/6000, G loss: 0.5943923592567444, D loss: 9.918147952703293e-06\n",
            "Iteration 1398/6000, G loss: 0.6876153349876404, D loss: 1.856064409366809e-05\n",
            "Iteration 1399/6000, G loss: 0.5917686223983765, D loss: 0.002099445089697838\n",
            "Iteration 1400/6000, G loss: 0.7307274341583252, D loss: 0.0008162255398929119\n",
            "Iteration 1401/6000, G loss: 0.5711928606033325, D loss: 0.026823490858078003\n",
            "Iteration 1402/6000, G loss: 0.5271555185317993, D loss: 0.0008294056169688702\n",
            "Iteration 1403/6000, G loss: 0.6063105463981628, D loss: 0.00038018421037122607\n",
            "Iteration 1404/6000, G loss: 0.5551865696907043, D loss: 0.0001293211244046688\n",
            "Iteration 1405/6000, G loss: 0.57361900806427, D loss: 0.0053037540055811405\n",
            "Iteration 1406/6000, G loss: 0.6388917565345764, D loss: 1.5854789126024116e-06\n",
            "Iteration 1407/6000, G loss: 0.526922881603241, D loss: 0.299541175365448\n",
            "Iteration 1408/6000, G loss: 0.572674572467804, D loss: 0.00012099785089958459\n",
            "Iteration 1409/6000, G loss: 0.6700121164321899, D loss: 0.042080461978912354\n",
            "Iteration 1410/6000, G loss: 0.6048435568809509, D loss: 0.05504336953163147\n",
            "Iteration 1411/6000, G loss: 0.603945791721344, D loss: 0.00010915751045104116\n",
            "Iteration 1412/6000, G loss: 0.6020708680152893, D loss: 1.7942439317703247\n",
            "Iteration 1413/6000, G loss: 0.6510024666786194, D loss: 0.003942030016332865\n",
            "Iteration 1414/6000, G loss: 0.6509159803390503, D loss: 1.9439767599105835\n",
            "Iteration 1415/6000, G loss: 0.5663822293281555, D loss: 0.0008695465512573719\n",
            "Iteration 1416/6000, G loss: 0.6281743049621582, D loss: 0.8734105825424194\n",
            "Iteration 1417/6000, G loss: 0.6581668853759766, D loss: 0.005595109425485134\n",
            "Iteration 1418/6000, G loss: 0.5163955688476562, D loss: 0.05038803815841675\n",
            "Iteration 1419/6000, G loss: 0.6233052015304565, D loss: 2.683950515347533e-05\n",
            "Iteration 1420/6000, G loss: 0.6433603167533875, D loss: 0.001102382899262011\n",
            "Iteration 1421/6000, G loss: 0.5444345474243164, D loss: 2.6565150619717315e-05\n",
            "Iteration 1422/6000, G loss: 0.610134482383728, D loss: 3.637645932030864e-05\n",
            "Iteration 1423/6000, G loss: 0.5676412582397461, D loss: 0.00404747948050499\n",
            "Iteration 1424/6000, G loss: 0.6365155577659607, D loss: 0.011872909031808376\n",
            "Iteration 1425/6000, G loss: 0.6858741641044617, D loss: 0.0030792290344834328\n",
            "Iteration 1426/6000, G loss: 0.5707777142524719, D loss: 0.059666797518730164\n",
            "Iteration 1427/6000, G loss: 0.6128095388412476, D loss: 0.018469803035259247\n",
            "Iteration 1428/6000, G loss: 0.5622878670692444, D loss: 0.003975150175392628\n",
            "Iteration 1429/6000, G loss: 0.5068758130073547, D loss: 0.5460245013237\n",
            "Iteration 1430/6000, G loss: 0.6925230026245117, D loss: 0.3455803394317627\n",
            "Iteration 1431/6000, G loss: 0.687627375125885, D loss: 0.008349106647074223\n",
            "Iteration 1432/6000, G loss: 0.5364548563957214, D loss: 1.9213486909866333\n",
            "Iteration 1433/6000, G loss: 0.6755818128585815, D loss: 0.00011045776773244143\n",
            "Iteration 1434/6000, G loss: 0.5331608653068542, D loss: 0.012269582599401474\n",
            "Iteration 1435/6000, G loss: 0.752056360244751, D loss: 1.907348234908568e-07\n",
            "Iteration 1436/6000, G loss: 0.5545544028282166, D loss: 1.7618725905776955e-05\n",
            "Iteration 1437/6000, G loss: 0.5130670070648193, D loss: 0.006982112303376198\n",
            "Iteration 1438/6000, G loss: 0.6802769303321838, D loss: 9.536741885085576e-08\n",
            "Iteration 1439/6000, G loss: 0.6823354959487915, D loss: 3.576278473360617e-08\n",
            "Iteration 1440/6000, G loss: 0.5677315592765808, D loss: 1.42093140311772e-05\n",
            "Iteration 1441/6000, G loss: 0.5854317545890808, D loss: 4.237868779455312e-06\n",
            "Iteration 1442/6000, G loss: 0.6161890625953674, D loss: 0.0\n",
            "Iteration 1443/6000, G loss: 0.5477690696716309, D loss: 1.5437569800269557e-06\n",
            "Iteration 1444/6000, G loss: 0.5804316401481628, D loss: 4.1365387914993335e-06\n",
            "Iteration 1445/6000, G loss: 0.6700671315193176, D loss: 1.192092824453539e-08\n",
            "Iteration 1446/6000, G loss: 0.6225459575653076, D loss: 6.496902642538771e-07\n",
            "Iteration 1447/6000, G loss: 0.5519508719444275, D loss: 0.00064485875191167\n",
            "Iteration 1448/6000, G loss: 0.7703140377998352, D loss: 1.7881373537420586e-07\n",
            "Iteration 1449/6000, G loss: 0.6811548471450806, D loss: 6.556509646316044e-08\n",
            "Iteration 1450/6000, G loss: 0.4871869683265686, D loss: 0.0028512082062661648\n",
            "Iteration 1451/6000, G loss: 0.6211970448493958, D loss: 3.7712103221565485e-05\n",
            "Iteration 1452/6000, G loss: 0.5999278426170349, D loss: 1.3303451851243153e-05\n",
            "Iteration 1453/6000, G loss: 0.7574887275695801, D loss: 1.4305103945844166e-07\n",
            "Iteration 1454/6000, G loss: 0.5591422319412231, D loss: 0.001094588777050376\n",
            "Iteration 1455/6000, G loss: 0.5163509249687195, D loss: 0.00649437727406621\n",
            "Iteration 1456/6000, G loss: 0.599449634552002, D loss: 2.056354333035415e-06\n",
            "Iteration 1457/6000, G loss: 0.6469483971595764, D loss: 2.5038076273631305e-05\n",
            "Iteration 1458/6000, G loss: 0.6227089762687683, D loss: 8.106175641842128e-07\n",
            "Iteration 1459/6000, G loss: 0.6454048752784729, D loss: 7.724642273387872e-06\n",
            "Iteration 1460/6000, G loss: 0.5871951580047607, D loss: 5.173199315322563e-05\n",
            "Iteration 1461/6000, G loss: 0.4979526996612549, D loss: 0.006477739661931992\n",
            "Iteration 1462/6000, G loss: 0.6876941919326782, D loss: 2.5808458303799853e-06\n",
            "Iteration 1463/6000, G loss: 0.5227853059768677, D loss: 0.0010704071028158069\n",
            "Iteration 1464/6000, G loss: 0.6152679324150085, D loss: 0.00031384333851747215\n",
            "Iteration 1465/6000, G loss: 0.5101208686828613, D loss: 0.002258002059534192\n",
            "Iteration 1466/6000, G loss: 0.6046067476272583, D loss: 1.4715608813276049e-05\n",
            "Iteration 1467/6000, G loss: 0.6760959029197693, D loss: 6.985070649534464e-05\n",
            "Iteration 1468/6000, G loss: 0.6198379993438721, D loss: 0.000270720396656543\n",
            "Iteration 1469/6000, G loss: 0.613537609577179, D loss: 0.07194842398166656\n",
            "Iteration 1470/6000, G loss: 0.5252414345741272, D loss: 0.0008188779465854168\n",
            "Iteration 1471/6000, G loss: 0.6638092398643494, D loss: 0.5267760753631592\n",
            "Iteration 1472/6000, G loss: 0.5546746850013733, D loss: 0.011860407888889313\n",
            "Iteration 1473/6000, G loss: 0.49128782749176025, D loss: 0.0007081747753545642\n",
            "Iteration 1474/6000, G loss: 0.6364535689353943, D loss: 9.53674117454284e-08\n",
            "Iteration 1475/6000, G loss: 0.6170058846473694, D loss: 0.0001853052235674113\n",
            "Iteration 1476/6000, G loss: 0.5732139348983765, D loss: 0.00018180458573624492\n",
            "Iteration 1477/6000, G loss: 0.7518622875213623, D loss: 4.613354576576967e-06\n",
            "Iteration 1478/6000, G loss: 0.5623366832733154, D loss: 0.010578718036413193\n",
            "Iteration 1479/6000, G loss: 0.6021487712860107, D loss: 0.22272691130638123\n",
            "Iteration 1480/6000, G loss: 0.5506594181060791, D loss: 0.005303803365677595\n",
            "Iteration 1481/6000, G loss: 0.6133274435997009, D loss: 0.01175905205309391\n",
            "Iteration 1482/6000, G loss: 0.6550238728523254, D loss: 0.28542616963386536\n",
            "Iteration 1483/6000, G loss: 0.7305726408958435, D loss: 1.615831206436269e-05\n",
            "Iteration 1484/6000, G loss: 0.5195198059082031, D loss: 1.7123886346817017\n",
            "Iteration 1485/6000, G loss: 0.5592240691184998, D loss: 0.5429713129997253\n",
            "Iteration 1486/6000, G loss: 0.6654973030090332, D loss: 0.001101701520383358\n",
            "Iteration 1487/6000, G loss: 0.49874910712242126, D loss: 0.273120254278183\n",
            "Iteration 1488/6000, G loss: 0.5058721899986267, D loss: 3.314013156341389e-06\n",
            "Iteration 1489/6000, G loss: 0.5635617971420288, D loss: 3.4647535358089954e-05\n",
            "Iteration 1490/6000, G loss: 0.5722241401672363, D loss: 0.005038628354668617\n",
            "Iteration 1491/6000, G loss: 0.6088925004005432, D loss: 8.684355634613894e-06\n",
            "Iteration 1492/6000, G loss: 0.5324510335922241, D loss: 0.02274341695010662\n",
            "Iteration 1493/6000, G loss: 0.6226595044136047, D loss: 0.04470444843173027\n",
            "Iteration 1494/6000, G loss: 0.5347801446914673, D loss: 1.0311600817658473e-06\n",
            "Iteration 1495/6000, G loss: 0.6710100769996643, D loss: 0.005905061960220337\n",
            "Iteration 1496/6000, G loss: 0.6537168622016907, D loss: 0.008858446031808853\n",
            "Iteration 1497/6000, G loss: 0.6250092387199402, D loss: 0.0036607880610972643\n",
            "Iteration 1498/6000, G loss: 0.5437598824501038, D loss: 0.00033923095907084644\n",
            "Iteration 1499/6000, G loss: 0.5299844145774841, D loss: 2.384185648907078e-08\n",
            "Iteration 1500/6000, G loss: 0.6255488991737366, D loss: 0.011607564985752106\n",
            "Iteration 1501/6000, G loss: 0.6021371483802795, D loss: 1.3685133126273286e-05\n",
            "Iteration 1502/6000, G loss: 0.5965918302536011, D loss: 0.00021603280038107187\n",
            "Iteration 1503/6000, G loss: 0.6160025596618652, D loss: 0.00011123783770017326\n",
            "Iteration 1504/6000, G loss: 0.6378995180130005, D loss: 9.911246888805181e-05\n",
            "Iteration 1505/6000, G loss: 0.6811130046844482, D loss: 0.0167086124420166\n",
            "Iteration 1506/6000, G loss: 0.521599531173706, D loss: 4.345169145381078e-06\n",
            "Iteration 1507/6000, G loss: 0.5429874062538147, D loss: 2.3221482479129918e-05\n",
            "Iteration 1508/6000, G loss: 0.5938575267791748, D loss: 1.3768658391200006e-06\n",
            "Iteration 1509/6000, G loss: 0.684589147567749, D loss: 0.000241977657424286\n",
            "Iteration 1510/6000, G loss: 0.5530309677124023, D loss: 9.29295492824167e-05\n",
            "Iteration 1511/6000, G loss: 0.7467478513717651, D loss: 0.0025173998437821865\n",
            "Iteration 1512/6000, G loss: 0.5458914637565613, D loss: 6.854531875433167e-07\n",
            "Iteration 1513/6000, G loss: 0.6252521872520447, D loss: 2.71200701718044e-06\n",
            "Iteration 1514/6000, G loss: 0.5310332775115967, D loss: 6.139275683381129e-07\n",
            "Iteration 1515/6000, G loss: 0.5523165464401245, D loss: 1.1742106380552286e-06\n",
            "Iteration 1516/6000, G loss: 0.4707595705986023, D loss: 1.188507849292364e-05\n",
            "Iteration 1517/6000, G loss: 0.599165678024292, D loss: 2.9539740353357047e-05\n",
            "Iteration 1518/6000, G loss: 0.5705795884132385, D loss: 5.722337664337829e-05\n",
            "Iteration 1519/6000, G loss: 0.7136455178260803, D loss: 1.3291679351823404e-05\n",
            "Iteration 1520/6000, G loss: 0.6118493676185608, D loss: 5.429809243651107e-05\n",
            "Iteration 1521/6000, G loss: 0.6713324189186096, D loss: 0.00028617476345971227\n",
            "Iteration 1522/6000, G loss: 0.5458930134773254, D loss: 7.535483746323735e-05\n",
            "Iteration 1523/6000, G loss: 0.6558608412742615, D loss: 8.73796125233639e-06\n",
            "Iteration 1524/6000, G loss: 0.48235026001930237, D loss: 0.015145570039749146\n",
            "Iteration 1525/6000, G loss: 0.6536698937416077, D loss: 0.0016144253313541412\n",
            "Iteration 1526/6000, G loss: 0.5877493023872375, D loss: 4.5299520934349857e-07\n",
            "Iteration 1527/6000, G loss: 0.5572039484977722, D loss: 0.0011055965442210436\n",
            "Iteration 1528/6000, G loss: 0.48092302680015564, D loss: 6.446031329687685e-05\n",
            "Iteration 1529/6000, G loss: 0.5420394539833069, D loss: 0.16314895451068878\n",
            "Iteration 1530/6000, G loss: 0.5362226963043213, D loss: 2.1713822206947953e-05\n",
            "Iteration 1531/6000, G loss: 0.6136458516120911, D loss: 0.0014959118561819196\n",
            "Iteration 1532/6000, G loss: 0.6161643862724304, D loss: 0.00017654143448453397\n",
            "Iteration 1533/6000, G loss: 0.6977506875991821, D loss: 1.4895034837536514e-05\n",
            "Iteration 1534/6000, G loss: 0.5351479053497314, D loss: 9.133412095252424e-05\n",
            "Iteration 1535/6000, G loss: 0.6001323461532593, D loss: 0.0020518621895462275\n",
            "Iteration 1536/6000, G loss: 0.8188003897666931, D loss: 0.08144369721412659\n",
            "Iteration 1537/6000, G loss: 0.5874526500701904, D loss: 3.7097864151000977\n",
            "Iteration 1538/6000, G loss: 0.6289561986923218, D loss: 0.00593159906566143\n",
            "Iteration 1539/6000, G loss: 0.5143318176269531, D loss: 0.00270563131198287\n",
            "Iteration 1540/6000, G loss: 0.5876399874687195, D loss: 4.8368452553404495e-05\n",
            "Iteration 1541/6000, G loss: 0.4863220751285553, D loss: 0.015464553609490395\n",
            "Iteration 1542/6000, G loss: 0.6935526132583618, D loss: 2.4437883894279366e-06\n",
            "Iteration 1543/6000, G loss: 0.5336516499519348, D loss: 0.00046560674672946334\n",
            "Iteration 1544/6000, G loss: 0.7240955829620361, D loss: 2.384185648907078e-08\n",
            "Iteration 1545/6000, G loss: 0.5758006572723389, D loss: 0.00011034215276595205\n",
            "Iteration 1546/6000, G loss: 0.7017802596092224, D loss: 1.2099737887183437e-06\n",
            "Iteration 1547/6000, G loss: 0.8111845254898071, D loss: 1.8477439311936905e-07\n",
            "Iteration 1548/6000, G loss: 0.551060676574707, D loss: 0.0011284968350082636\n",
            "Iteration 1549/6000, G loss: 0.6060373187065125, D loss: 0.00017803354421630502\n",
            "Iteration 1550/6000, G loss: 0.522456169128418, D loss: 0.0002546824689488858\n",
            "Iteration 1551/6000, G loss: 0.6179980039596558, D loss: 5.8650821301853284e-06\n",
            "Iteration 1552/6000, G loss: 0.5928993821144104, D loss: 6.651853254879825e-06\n",
            "Iteration 1553/6000, G loss: 0.6274235844612122, D loss: 2.792445957311429e-05\n",
            "Iteration 1554/6000, G loss: 0.5365661382675171, D loss: 0.0009655992034822702\n",
            "Iteration 1555/6000, G loss: 0.6067970395088196, D loss: 5.286079249344766e-05\n",
            "Iteration 1556/6000, G loss: 0.515285074710846, D loss: 0.0002959952689707279\n",
            "Iteration 1557/6000, G loss: 0.5115145444869995, D loss: 0.0012225292157381773\n",
            "Iteration 1558/6000, G loss: 0.6472572684288025, D loss: 2.555796891101636e-05\n",
            "Iteration 1559/6000, G loss: 0.5082899332046509, D loss: 0.0008183561731129885\n",
            "Iteration 1560/6000, G loss: 0.5268634557723999, D loss: 0.00019494770094752312\n",
            "Iteration 1561/6000, G loss: 0.5729673504829407, D loss: 0.00012283070827834308\n",
            "Iteration 1562/6000, G loss: 0.6383649110794067, D loss: 6.592250883841189e-06\n",
            "Iteration 1563/6000, G loss: 0.6887161135673523, D loss: 6.719992961734533e-05\n",
            "Iteration 1564/6000, G loss: 0.5528362989425659, D loss: 0.0007109061116352677\n",
            "Iteration 1565/6000, G loss: 0.5538527369499207, D loss: 0.0029430014546960592\n",
            "Iteration 1566/6000, G loss: 0.6150739789009094, D loss: 0.0002916138619184494\n",
            "Iteration 1567/6000, G loss: 0.44577041268348694, D loss: 0.023398730903863907\n",
            "Iteration 1568/6000, G loss: 0.6513844728469849, D loss: 5.698177574231522e-06\n",
            "Iteration 1569/6000, G loss: 0.5456217527389526, D loss: 0.0003573015856090933\n",
            "Iteration 1570/6000, G loss: 0.4851871728897095, D loss: 0.0013688099570572376\n",
            "Iteration 1571/6000, G loss: 0.5327458381652832, D loss: 0.00013970080181024969\n",
            "Iteration 1572/6000, G loss: 0.5681203603744507, D loss: 3.0278444683062844e-05\n",
            "Iteration 1573/6000, G loss: 0.6156697273254395, D loss: 8.198009163606912e-05\n",
            "Iteration 1574/6000, G loss: 0.6155605316162109, D loss: 2.324580918866559e-07\n",
            "Iteration 1575/6000, G loss: 0.4985572099685669, D loss: 0.0005585764884017408\n",
            "Iteration 1576/6000, G loss: 0.7278878688812256, D loss: 9.536741885085576e-08\n",
            "Iteration 1577/6000, G loss: 0.716326117515564, D loss: 5.236620199866593e-05\n",
            "Iteration 1578/6000, G loss: 0.5227228403091431, D loss: 3.2781790650915354e-05\n",
            "Iteration 1579/6000, G loss: 0.6194447875022888, D loss: 1.0251992534904275e-06\n",
            "Iteration 1580/6000, G loss: 0.5789620876312256, D loss: 0.00040605035610497\n",
            "Iteration 1581/6000, G loss: 0.5709895491600037, D loss: 1.721357330097817e-05\n",
            "Iteration 1582/6000, G loss: 0.5105781555175781, D loss: 0.0019363656174391508\n",
            "Iteration 1583/6000, G loss: 0.5972884297370911, D loss: 0.0016424073837697506\n",
            "Iteration 1584/6000, G loss: 0.5782999992370605, D loss: 0.0004881956847384572\n",
            "Iteration 1585/6000, G loss: 0.6512365937232971, D loss: 7.811334216967225e-05\n",
            "Iteration 1586/6000, G loss: 0.5339613556861877, D loss: 9.00741943041794e-05\n",
            "Iteration 1587/6000, G loss: 0.5482558608055115, D loss: 0.0022023587953299284\n",
            "Iteration 1588/6000, G loss: 0.5516488552093506, D loss: 2.8907765226904303e-05\n",
            "Iteration 1589/6000, G loss: 0.5594974160194397, D loss: 4.4058811909053475e-05\n",
            "Iteration 1590/6000, G loss: 0.5193109512329102, D loss: 0.011687533929944038\n",
            "Iteration 1591/6000, G loss: 0.5580538511276245, D loss: 0.00011631121742539108\n",
            "Iteration 1592/6000, G loss: 0.6223780512809753, D loss: 0.0002858525840565562\n",
            "Iteration 1593/6000, G loss: 0.5560718178749084, D loss: 3.641162402345799e-05\n",
            "Iteration 1594/6000, G loss: 0.4696488380432129, D loss: 0.020197659730911255\n",
            "Iteration 1595/6000, G loss: 0.6147463917732239, D loss: 3.799139813054353e-05\n",
            "Iteration 1596/6000, G loss: 0.6476439833641052, D loss: 2.3317010345635936e-05\n",
            "Iteration 1597/6000, G loss: 0.5937353372573853, D loss: 0.0018782453844323754\n",
            "Iteration 1598/6000, G loss: 0.6109412908554077, D loss: 1.042479107127292e-05\n",
            "Iteration 1599/6000, G loss: 0.5544149279594421, D loss: 7.71614140830934e-05\n",
            "Iteration 1600/6000, G loss: 0.4341154396533966, D loss: 0.003433471079915762\n",
            "Iteration 1601/6000, G loss: 0.7011936902999878, D loss: 6.198879418661818e-07\n",
            "Iteration 1602/6000, G loss: 0.565384030342102, D loss: 5.6326252888538875e-06\n",
            "Iteration 1603/6000, G loss: 0.5576317310333252, D loss: 0.0006953406846150756\n",
            "Iteration 1604/6000, G loss: 0.5539725422859192, D loss: 0.00027121033053845167\n",
            "Iteration 1605/6000, G loss: 0.5650674104690552, D loss: 0.0004025461967103183\n",
            "Iteration 1606/6000, G loss: 0.6704169511795044, D loss: 1.3530096111935563e-05\n",
            "Iteration 1607/6000, G loss: 0.4963796138763428, D loss: 7.629111496498808e-05\n",
            "Iteration 1608/6000, G loss: 0.39861252903938293, D loss: 0.1093512773513794\n",
            "Iteration 1609/6000, G loss: 0.7075324058532715, D loss: 3.3140113373519853e-06\n",
            "Iteration 1610/6000, G loss: 0.5657012462615967, D loss: 0.00026273608091287315\n",
            "Iteration 1611/6000, G loss: 0.5213771462440491, D loss: 0.00015334843192249537\n",
            "Iteration 1612/6000, G loss: 0.5694577097892761, D loss: 8.422095561400056e-06\n",
            "Iteration 1613/6000, G loss: 0.5852819681167603, D loss: 8.727081876713783e-05\n",
            "Iteration 1614/6000, G loss: 0.6146738529205322, D loss: 0.00011572978110052645\n",
            "Iteration 1615/6000, G loss: 0.6976987719535828, D loss: 3.331895641167648e-06\n",
            "Iteration 1616/6000, G loss: 0.5871598720550537, D loss: 2.6934838388115168e-05\n",
            "Iteration 1617/6000, G loss: 0.4901202321052551, D loss: 0.0004769029619637877\n",
            "Iteration 1618/6000, G loss: 0.5725107789039612, D loss: 0.00010718321573222056\n",
            "Iteration 1619/6000, G loss: 0.5909929871559143, D loss: 2.038477532551042e-06\n",
            "Iteration 1620/6000, G loss: 0.5802776217460632, D loss: 4.114418334211223e-05\n",
            "Iteration 1621/6000, G loss: 0.5306157469749451, D loss: 0.0015441435389220715\n",
            "Iteration 1622/6000, G loss: 0.6195555329322815, D loss: 6.961807230254635e-06\n",
            "Iteration 1623/6000, G loss: 0.6524811387062073, D loss: 4.544788316707127e-05\n",
            "Iteration 1624/6000, G loss: 0.661878228187561, D loss: 3.391498921700986e-06\n",
            "Iteration 1625/6000, G loss: 0.4763527810573578, D loss: 0.0052541582845151424\n",
            "Iteration 1626/6000, G loss: 0.7021072506904602, D loss: 0.0001400345063302666\n",
            "Iteration 1627/6000, G loss: 0.5021669268608093, D loss: 0.012927882373332977\n",
            "Iteration 1628/6000, G loss: 0.5993205904960632, D loss: 0.0024990737438201904\n",
            "Iteration 1629/6000, G loss: 0.6344088315963745, D loss: 7.301369623746723e-05\n",
            "Iteration 1630/6000, G loss: 0.5177439451217651, D loss: 0.010192052461206913\n",
            "Iteration 1631/6000, G loss: 0.5579115152359009, D loss: 0.005521345417946577\n",
            "Iteration 1632/6000, G loss: 0.5373688340187073, D loss: 0.007820327766239643\n",
            "Iteration 1633/6000, G loss: 0.5583930015563965, D loss: 0.00047434272710233927\n",
            "Iteration 1634/6000, G loss: 0.6119764447212219, D loss: 9.564425999997184e-05\n",
            "Iteration 1635/6000, G loss: 0.54878169298172, D loss: 0.0007103293319232762\n",
            "Iteration 1636/6000, G loss: 0.6718617677688599, D loss: 0.0003298648807685822\n",
            "Iteration 1637/6000, G loss: 0.544532299041748, D loss: 0.0007214456563815475\n",
            "Iteration 1638/6000, G loss: 0.5416774153709412, D loss: 0.02249540202319622\n",
            "Iteration 1639/6000, G loss: 0.4741123914718628, D loss: 0.014715414494276047\n",
            "Iteration 1640/6000, G loss: 0.5257312059402466, D loss: 0.010929198004305363\n",
            "Iteration 1641/6000, G loss: 0.5609524250030518, D loss: 4.201470437692478e-05\n",
            "Iteration 1642/6000, G loss: 0.5040754079818726, D loss: 0.0025715590454638004\n",
            "Iteration 1643/6000, G loss: 0.5875604152679443, D loss: 0.00018666965479496866\n",
            "Iteration 1644/6000, G loss: 0.6600501537322998, D loss: 0.00017499635578133166\n",
            "Iteration 1645/6000, G loss: 0.6016894578933716, D loss: 0.014917993918061256\n",
            "Iteration 1646/6000, G loss: 0.5001162886619568, D loss: 0.38420307636260986\n",
            "Iteration 1647/6000, G loss: 0.685341477394104, D loss: 0.0018593700369819999\n",
            "Iteration 1648/6000, G loss: 0.5607221722602844, D loss: 0.0012746278662234545\n",
            "Iteration 1649/6000, G loss: 0.5898805260658264, D loss: 0.00015809452452231199\n",
            "Iteration 1650/6000, G loss: 0.5943080186843872, D loss: 0.00036097370320931077\n",
            "Iteration 1651/6000, G loss: 0.6189670562744141, D loss: 4.12576919188723e-05\n",
            "Iteration 1652/6000, G loss: 0.6116872429847717, D loss: 0.0008693654090166092\n",
            "Iteration 1653/6000, G loss: 0.5107991695404053, D loss: 0.6607584953308105\n",
            "Iteration 1654/6000, G loss: 0.4574708342552185, D loss: 0.013877645134925842\n",
            "Iteration 1655/6000, G loss: 0.6676635146141052, D loss: 0.0007771108066663146\n",
            "Iteration 1656/6000, G loss: 0.6307738423347473, D loss: 0.0001409469114150852\n",
            "Iteration 1657/6000, G loss: 0.47498437762260437, D loss: 0.028261255472898483\n",
            "Iteration 1658/6000, G loss: 0.5541284084320068, D loss: 3.1124960514716804e-05\n",
            "Iteration 1659/6000, G loss: 0.6004892587661743, D loss: 5.750550189986825e-05\n",
            "Iteration 1660/6000, G loss: 0.5542468428611755, D loss: 0.007487108930945396\n",
            "Iteration 1661/6000, G loss: 0.5250043272972107, D loss: 0.0018287913408130407\n",
            "Iteration 1662/6000, G loss: 0.6012279391288757, D loss: 0.00038417120231315494\n",
            "Iteration 1663/6000, G loss: 0.6249130964279175, D loss: 0.00010679577826522291\n",
            "Iteration 1664/6000, G loss: 0.5452101826667786, D loss: 0.0015031929360702634\n",
            "Iteration 1665/6000, G loss: 0.6739769577980042, D loss: 0.00016222347039729357\n",
            "Iteration 1666/6000, G loss: 0.5326026082038879, D loss: 0.0001705311588011682\n",
            "Iteration 1667/6000, G loss: 0.5623456239700317, D loss: 0.008711221627891064\n",
            "Iteration 1668/6000, G loss: 0.725398063659668, D loss: 9.244655302609317e-06\n",
            "Iteration 1669/6000, G loss: 0.4556192457675934, D loss: 0.0045009939931333065\n",
            "Iteration 1670/6000, G loss: 0.6319208741188049, D loss: 1.930582402565051e-05\n",
            "Iteration 1671/6000, G loss: 0.6364620327949524, D loss: 4.7385619836859405e-06\n",
            "Iteration 1672/6000, G loss: 0.4686402678489685, D loss: 0.0004587214789353311\n",
            "Iteration 1673/6000, G loss: 0.4982493221759796, D loss: 0.03961984068155289\n",
            "Iteration 1674/6000, G loss: 0.5439156889915466, D loss: 0.00041182106360793114\n",
            "Iteration 1675/6000, G loss: 0.5932568907737732, D loss: 0.0029996498487889767\n",
            "Iteration 1676/6000, G loss: 0.6964986324310303, D loss: 0.0014245092170313\n",
            "Iteration 1677/6000, G loss: 0.6852119565010071, D loss: 0.003376601729542017\n",
            "Iteration 1678/6000, G loss: 0.5138834118843079, D loss: 0.03980276733636856\n",
            "Iteration 1679/6000, G loss: 0.6069517731666565, D loss: 0.018706411123275757\n",
            "Iteration 1680/6000, G loss: 0.5589377880096436, D loss: 0.010728372260928154\n",
            "Iteration 1681/6000, G loss: 0.5413401126861572, D loss: 0.057850487530231476\n",
            "Iteration 1682/6000, G loss: 0.6160721778869629, D loss: 0.04662007838487625\n",
            "Iteration 1683/6000, G loss: 0.572725236415863, D loss: 0.2942039668560028\n",
            "Iteration 1684/6000, G loss: 0.5348029732704163, D loss: 0.0007257972611114383\n",
            "Iteration 1685/6000, G loss: 0.5688155293464661, D loss: 0.9952179193496704\n",
            "Iteration 1686/6000, G loss: 0.4255330264568329, D loss: 2.5933523178100586\n",
            "Iteration 1687/6000, G loss: 0.6044201254844666, D loss: 0.024450313299894333\n",
            "Iteration 1688/6000, G loss: 0.6193345189094543, D loss: 0.00040938996244221926\n",
            "Iteration 1689/6000, G loss: 0.5056700110435486, D loss: 0.16726426780223846\n",
            "Iteration 1690/6000, G loss: 0.6085376143455505, D loss: 0.04408101737499237\n",
            "Iteration 1691/6000, G loss: 0.6678403615951538, D loss: 0.004120228812098503\n",
            "Iteration 1692/6000, G loss: 0.5929459929466248, D loss: 0.0008174182148650289\n",
            "Iteration 1693/6000, G loss: 0.6561337113380432, D loss: 5.960464122267695e-09\n",
            "Iteration 1694/6000, G loss: 0.6193016171455383, D loss: 3.7383466406026855e-05\n",
            "Iteration 1695/6000, G loss: 0.5829723477363586, D loss: 0.007210887968540192\n",
            "Iteration 1696/6000, G loss: 0.6285693049430847, D loss: 6.163106718304334e-06\n",
            "Iteration 1697/6000, G loss: 0.5677297115325928, D loss: 0.0003052569227293134\n",
            "Iteration 1698/6000, G loss: 0.6758461594581604, D loss: 2.927749665104784e-05\n",
            "Iteration 1699/6000, G loss: 0.5536695122718811, D loss: 6.816224777139723e-05\n",
            "Iteration 1700/6000, G loss: 0.6033996343612671, D loss: 0.00037625490222126245\n",
            "Iteration 1701/6000, G loss: 0.4843290150165558, D loss: 0.005869661457836628\n",
            "Iteration 1702/6000, G loss: 0.6106464266777039, D loss: 0.00012712980969808996\n",
            "Iteration 1703/6000, G loss: 0.6050488948822021, D loss: 0.0008777685579843819\n",
            "Iteration 1704/6000, G loss: 0.621222734451294, D loss: 5.953173968009651e-05\n",
            "Iteration 1705/6000, G loss: 0.6674484610557556, D loss: 1.311302071371756e-07\n",
            "Iteration 1706/6000, G loss: 0.6592661738395691, D loss: 9.477080311626196e-06\n",
            "Iteration 1707/6000, G loss: 0.588497519493103, D loss: 3.135155202471651e-05\n",
            "Iteration 1708/6000, G loss: 0.45299839973449707, D loss: 0.12130182981491089\n",
            "Iteration 1709/6000, G loss: 0.49778950214385986, D loss: 0.0015420143026858568\n",
            "Iteration 1710/6000, G loss: 0.5619795918464661, D loss: 0.0013195450883358717\n",
            "Iteration 1711/6000, G loss: 0.5757520794868469, D loss: 0.00198958907276392\n",
            "Iteration 1712/6000, G loss: 0.5779582858085632, D loss: 0.00010379113518865779\n",
            "Iteration 1713/6000, G loss: 0.5607122778892517, D loss: 6.092664261814207e-05\n",
            "Iteration 1714/6000, G loss: 0.7606462836265564, D loss: 4.4703477897201083e-07\n",
            "Iteration 1715/6000, G loss: 0.5880599021911621, D loss: 1.645671363803558e-05\n",
            "Iteration 1716/6000, G loss: 0.5901407599449158, D loss: 0.0006321856053546071\n",
            "Iteration 1717/6000, G loss: 0.512441873550415, D loss: 0.0337148942053318\n",
            "Iteration 1718/6000, G loss: 0.5451664328575134, D loss: 0.007990784011781216\n",
            "Iteration 1719/6000, G loss: 0.5885493159294128, D loss: 0.001465743756853044\n",
            "Iteration 1720/6000, G loss: 0.5315693616867065, D loss: 0.019252821803092957\n",
            "Iteration 1721/6000, G loss: 0.5050289630889893, D loss: 0.00117287237662822\n",
            "Iteration 1722/6000, G loss: 0.6346695423126221, D loss: 0.0015454652020707726\n",
            "Iteration 1723/6000, G loss: 0.512042224407196, D loss: 0.028046132996678352\n",
            "Iteration 1724/6000, G loss: 0.4720568358898163, D loss: 0.02528861165046692\n",
            "Iteration 1725/6000, G loss: 0.5284162759780884, D loss: 1.0728835775353218e-07\n",
            "Iteration 1726/6000, G loss: 0.7039862871170044, D loss: 0.0\n",
            "Iteration 1727/6000, G loss: 0.5990660190582275, D loss: 2.5808790269366e-06\n",
            "Iteration 1728/6000, G loss: 0.5310409665107727, D loss: 0.004542171023786068\n",
            "Iteration 1729/6000, G loss: 0.6055730581283569, D loss: 3.516673530157277e-07\n",
            "Iteration 1730/6000, G loss: 0.5019359588623047, D loss: 0.001153160585090518\n",
            "Iteration 1731/6000, G loss: 0.5033760070800781, D loss: 0.00014638212451245636\n",
            "Iteration 1732/6000, G loss: 0.5391637086868286, D loss: 0.00473046163097024\n",
            "Iteration 1733/6000, G loss: 0.42014870047569275, D loss: 0.047052256762981415\n",
            "Iteration 1734/6000, G loss: 0.5617668628692627, D loss: 0.00017898165970109403\n",
            "Iteration 1735/6000, G loss: 0.5284861922264099, D loss: 0.0015679614152759314\n",
            "Iteration 1736/6000, G loss: 0.6052291393280029, D loss: 0.0\n",
            "Iteration 1737/6000, G loss: 0.5831699371337891, D loss: 0.0\n",
            "Iteration 1738/6000, G loss: 0.786914050579071, D loss: 0.0\n",
            "Iteration 1739/6000, G loss: 0.6564710140228271, D loss: 0.0\n",
            "Iteration 1740/6000, G loss: 0.6479312777519226, D loss: 0.0\n",
            "Iteration 1741/6000, G loss: 0.6595874428749084, D loss: 0.0\n",
            "Iteration 1742/6000, G loss: 0.6867950558662415, D loss: 0.0\n",
            "Iteration 1743/6000, G loss: 0.5646380186080933, D loss: 6.437300044126459e-07\n",
            "Iteration 1744/6000, G loss: 0.5533267855644226, D loss: 0.0\n",
            "Iteration 1745/6000, G loss: 0.6146570444107056, D loss: 0.0\n",
            "Iteration 1746/6000, G loss: 0.6894557476043701, D loss: 0.0\n",
            "Iteration 1747/6000, G loss: 0.6936382055282593, D loss: 0.0\n",
            "Iteration 1748/6000, G loss: 0.551870584487915, D loss: 0.0\n",
            "Iteration 1749/6000, G loss: 0.7754930257797241, D loss: 0.0\n",
            "Iteration 1750/6000, G loss: 0.6382293701171875, D loss: 0.0\n",
            "Iteration 1751/6000, G loss: 0.4945705533027649, D loss: 1.2218948768349946e-06\n",
            "Iteration 1752/6000, G loss: 0.5898584127426147, D loss: 0.0\n",
            "Iteration 1753/6000, G loss: 0.6078171730041504, D loss: 4.172324565843155e-07\n",
            "Iteration 1754/6000, G loss: 0.6378442645072937, D loss: 0.0\n",
            "Iteration 1755/6000, G loss: 0.6683298945426941, D loss: 0.0\n",
            "Iteration 1756/6000, G loss: 0.5232313871383667, D loss: 5.245208285487024e-07\n",
            "Iteration 1757/6000, G loss: 0.6446884274482727, D loss: 0.0\n",
            "Iteration 1758/6000, G loss: 0.6677241921424866, D loss: 1.192092824453539e-08\n",
            "Iteration 1759/6000, G loss: 0.6182427406311035, D loss: 0.0\n",
            "Iteration 1760/6000, G loss: 0.5606637001037598, D loss: 3.278254894212296e-07\n",
            "Iteration 1761/6000, G loss: 0.6450773477554321, D loss: 0.0\n",
            "Iteration 1762/6000, G loss: 0.6046240925788879, D loss: 1.7458089132560417e-05\n",
            "Iteration 1763/6000, G loss: 0.543333113193512, D loss: 0.00048587535275146365\n",
            "Iteration 1764/6000, G loss: 0.5612956285476685, D loss: 0.0004325582995079458\n",
            "Iteration 1765/6000, G loss: 0.5936883687973022, D loss: 1.0132787338079652e-06\n",
            "Iteration 1766/6000, G loss: 0.5643030405044556, D loss: 0.019726324826478958\n",
            "Iteration 1767/6000, G loss: 0.7089467644691467, D loss: 0.0\n",
            "Iteration 1768/6000, G loss: 0.6184601187705994, D loss: 1.2093742043361999e-05\n",
            "Iteration 1769/6000, G loss: 0.5586448311805725, D loss: 0.00029845061362721026\n",
            "Iteration 1770/6000, G loss: 0.6416140198707581, D loss: 4.1127199779111834e-07\n",
            "Iteration 1771/6000, G loss: 0.7304410338401794, D loss: 0.00012355338549241424\n",
            "Iteration 1772/6000, G loss: 0.5665070414543152, D loss: 0.0\n",
            "Iteration 1773/6000, G loss: 0.5551923513412476, D loss: 2.2375402450561523\n",
            "Iteration 1774/6000, G loss: 0.5555314421653748, D loss: 1.6289865016005933e-05\n",
            "Iteration 1775/6000, G loss: 0.5742327570915222, D loss: 6.4611290326865856e-06\n",
            "Iteration 1776/6000, G loss: 0.6626173257827759, D loss: 0.0012802125420421362\n",
            "Iteration 1777/6000, G loss: 0.48661816120147705, D loss: 0.017758436501026154\n",
            "Iteration 1778/6000, G loss: 0.6256846785545349, D loss: 0.06597834825515747\n",
            "Iteration 1779/6000, G loss: 0.5912425518035889, D loss: 0.05390041321516037\n",
            "Iteration 1780/6000, G loss: 0.663357138633728, D loss: 0.10707149654626846\n",
            "Iteration 1781/6000, G loss: 0.6568357944488525, D loss: 0.26969265937805176\n",
            "Iteration 1782/6000, G loss: 0.6223207712173462, D loss: 0.03895086050033569\n",
            "Iteration 1783/6000, G loss: 0.6698428392410278, D loss: 0.0018217384349554777\n",
            "Iteration 1784/6000, G loss: 0.4680442214012146, D loss: 0.00879689957946539\n",
            "Iteration 1785/6000, G loss: 0.5707387328147888, D loss: 0.001729950774461031\n",
            "Iteration 1786/6000, G loss: 0.694085955619812, D loss: 0.002674260176718235\n",
            "Iteration 1787/6000, G loss: 0.4787989854812622, D loss: 0.014211386442184448\n",
            "Iteration 1788/6000, G loss: 0.4675256609916687, D loss: 0.009352424181997776\n",
            "Iteration 1789/6000, G loss: 0.47287455201148987, D loss: 0.0022949143312871456\n",
            "Iteration 1790/6000, G loss: 0.5396289229393005, D loss: 0.0034187668934464455\n",
            "Iteration 1791/6000, G loss: 0.5815261602401733, D loss: 0.008614937774837017\n",
            "Iteration 1792/6000, G loss: 0.5460101962089539, D loss: 0.01398986391723156\n",
            "Iteration 1793/6000, G loss: 0.5888288021087646, D loss: 0.004807005170732737\n",
            "Iteration 1794/6000, G loss: 0.5640472769737244, D loss: 0.0005553271621465683\n",
            "Iteration 1795/6000, G loss: 0.5748807787895203, D loss: 0.0018756341887637973\n",
            "Iteration 1796/6000, G loss: 0.5028020739555359, D loss: 0.12834800779819489\n",
            "Iteration 1797/6000, G loss: 0.5341280102729797, D loss: 0.058627963066101074\n",
            "Iteration 1798/6000, G loss: 0.40934881567955017, D loss: 0.09824278950691223\n",
            "Iteration 1799/6000, G loss: 0.6590089201927185, D loss: 0.0026259422302246094\n",
            "Iteration 1800/6000, G loss: 0.4335767924785614, D loss: 0.6144794821739197\n",
            "Iteration 1801/6000, G loss: 0.4663229286670685, D loss: 0.001298680086620152\n",
            "Iteration 1802/6000, G loss: 0.4086691439151764, D loss: 0.16155363619327545\n",
            "Iteration 1803/6000, G loss: 0.6837922930717468, D loss: 4.734318645205349e-05\n",
            "Iteration 1804/6000, G loss: 0.6500812768936157, D loss: 1.5628183973603882e-05\n",
            "Iteration 1805/6000, G loss: 0.5195071697235107, D loss: 0.0004209075996186584\n",
            "Iteration 1806/6000, G loss: 0.6043726205825806, D loss: 3.194805685780011e-06\n",
            "Iteration 1807/6000, G loss: 0.5658308267593384, D loss: 0.0006948939990252256\n",
            "Iteration 1808/6000, G loss: 0.44232919812202454, D loss: 0.018017088994383812\n",
            "Iteration 1809/6000, G loss: 0.6250669956207275, D loss: 0.0010678383987396955\n",
            "Iteration 1810/6000, G loss: 0.5775665640830994, D loss: 4.816046384803485e-06\n",
            "Iteration 1811/6000, G loss: 0.6624979972839355, D loss: 8.112159775919281e-06\n",
            "Iteration 1812/6000, G loss: 0.6012446284294128, D loss: 0.0003912061220034957\n",
            "Iteration 1813/6000, G loss: 0.6187640428543091, D loss: 9.411531209480017e-06\n",
            "Iteration 1814/6000, G loss: 0.4657250642776489, D loss: 0.0069607924669981\n",
            "Iteration 1815/6000, G loss: 0.5275937914848328, D loss: 8.872966282069683e-05\n",
            "Iteration 1816/6000, G loss: 0.6441956162452698, D loss: 2.697043601074256e-05\n",
            "Iteration 1817/6000, G loss: 0.43966102600097656, D loss: 0.07071255147457123\n",
            "Iteration 1818/6000, G loss: 0.5664161443710327, D loss: 5.066393669039826e-07\n",
            "Iteration 1819/6000, G loss: 0.744415283203125, D loss: 0.0\n",
            "Iteration 1820/6000, G loss: 0.5230359435081482, D loss: 0.005525273270905018\n",
            "Iteration 1821/6000, G loss: 0.5712636709213257, D loss: 0.0018507984932512045\n",
            "Iteration 1822/6000, G loss: 0.49031901359558105, D loss: 0.02559880167245865\n",
            "Iteration 1823/6000, G loss: 0.6115753650665283, D loss: 1.1503689165692776e-06\n",
            "Iteration 1824/6000, G loss: 0.5059581398963928, D loss: 1.6686182022094727\n",
            "Iteration 1825/6000, G loss: 0.6618838906288147, D loss: 1.3649455468112137e-06\n",
            "Iteration 1826/6000, G loss: 0.6693438291549683, D loss: 1.988994517887477e-05\n",
            "Iteration 1827/6000, G loss: 0.6782902479171753, D loss: 0.0012776728253811598\n",
            "Iteration 1828/6000, G loss: 0.6075537204742432, D loss: 0.09454366564750671\n",
            "Iteration 1829/6000, G loss: 0.5529743432998657, D loss: 0.09930586814880371\n",
            "Iteration 1830/6000, G loss: 0.5647404789924622, D loss: 0.03475377708673477\n",
            "Iteration 1831/6000, G loss: 0.6912924647331238, D loss: 0.002851237077265978\n",
            "Iteration 1832/6000, G loss: 0.6005242466926575, D loss: 0.015443868935108185\n",
            "Iteration 1833/6000, G loss: 0.5647664070129395, D loss: 0.0407533198595047\n",
            "Iteration 1834/6000, G loss: 0.48968666791915894, D loss: 0.13430872559547424\n",
            "Iteration 1835/6000, G loss: 0.5716067552566528, D loss: 0.05607030168175697\n",
            "Iteration 1836/6000, G loss: 0.6217501759529114, D loss: 0.03806028515100479\n",
            "Iteration 1837/6000, G loss: 0.544514000415802, D loss: 0.0057482533156871796\n",
            "Iteration 1838/6000, G loss: 0.532569169998169, D loss: 0.006099292542785406\n",
            "Iteration 1839/6000, G loss: 0.7252620458602905, D loss: 0.000647615292109549\n",
            "Iteration 1840/6000, G loss: 0.543766975402832, D loss: 0.05315761640667915\n",
            "Iteration 1841/6000, G loss: 0.4663165509700775, D loss: 0.0635320246219635\n",
            "Iteration 1842/6000, G loss: 0.637158215045929, D loss: 0.0004246161552146077\n",
            "Iteration 1843/6000, G loss: 0.6109145879745483, D loss: 0.0003888990613631904\n",
            "Iteration 1844/6000, G loss: 0.5691338181495667, D loss: 0.0003167240065522492\n",
            "Iteration 1845/6000, G loss: 0.5917506217956543, D loss: 0.0006238613277673721\n",
            "Iteration 1846/6000, G loss: 0.5286210775375366, D loss: 0.0011233638506382704\n",
            "Iteration 1847/6000, G loss: 0.5191645622253418, D loss: 0.002591407857835293\n",
            "Iteration 1848/6000, G loss: 0.690983235836029, D loss: 6.997560376476031e-06\n",
            "Iteration 1849/6000, G loss: 0.5019506216049194, D loss: 0.00035953219048678875\n",
            "Iteration 1850/6000, G loss: 0.6786165237426758, D loss: 7.569786930616829e-07\n",
            "Iteration 1851/6000, G loss: 0.5386313796043396, D loss: 3.5464681786834262e-06\n",
            "Iteration 1852/6000, G loss: 0.5328753590583801, D loss: 0.005217928439378738\n",
            "Iteration 1853/6000, G loss: 0.6838685870170593, D loss: 2.980230249249871e-07\n",
            "Iteration 1854/6000, G loss: 0.5140440464019775, D loss: 0.007075449917465448\n",
            "Iteration 1855/6000, G loss: 0.6054708361625671, D loss: 6.976472650421783e-05\n",
            "Iteration 1856/6000, G loss: 0.603776216506958, D loss: 0.0\n",
            "Iteration 1857/6000, G loss: 0.6740930080413818, D loss: 3.576278118089249e-08\n",
            "Iteration 1858/6000, G loss: 0.7161301374435425, D loss: 3.6954575080017094e-06\n",
            "Iteration 1859/6000, G loss: 0.5789656043052673, D loss: 4.408786480780691e-05\n",
            "Iteration 1860/6000, G loss: 0.6536873579025269, D loss: 2.312654487468535e-06\n",
            "Iteration 1861/6000, G loss: 0.46878695487976074, D loss: 0.0008893270278349519\n",
            "Iteration 1862/6000, G loss: 0.4646593928337097, D loss: 0.022735193371772766\n",
            "Iteration 1863/6000, G loss: 0.5993356704711914, D loss: 3.277403811807744e-05\n",
            "Iteration 1864/6000, G loss: 0.6629123687744141, D loss: 1.513956476628664e-06\n",
            "Iteration 1865/6000, G loss: 0.5906537771224976, D loss: 3.278248641436221e-06\n",
            "Iteration 1866/6000, G loss: 0.5230097770690918, D loss: 3.802757419180125e-06\n",
            "Iteration 1867/6000, G loss: 0.5873773694038391, D loss: 6.699509413010674e-06\n",
            "Iteration 1868/6000, G loss: 0.5997767448425293, D loss: 4.099885336472653e-05\n",
            "Iteration 1869/6000, G loss: 0.5694307088851929, D loss: 8.314443221024703e-06\n",
            "Iteration 1870/6000, G loss: 0.4211106300354004, D loss: 0.08762824535369873\n",
            "Iteration 1871/6000, G loss: 0.5317310094833374, D loss: 0.00012189131666673347\n",
            "Iteration 1872/6000, G loss: 0.7138117551803589, D loss: 2.3662955754844006e-06\n",
            "Iteration 1873/6000, G loss: 0.4922347068786621, D loss: 0.004962832201272249\n",
            "Iteration 1874/6000, G loss: 0.4910985231399536, D loss: 0.00020460260566323996\n",
            "Iteration 1875/6000, G loss: 0.6424148082733154, D loss: 5.5432303724956e-07\n",
            "Iteration 1876/6000, G loss: 0.6771248579025269, D loss: 6.437298907258082e-07\n",
            "Iteration 1877/6000, G loss: 0.5055561065673828, D loss: 0.00014454268966801465\n",
            "Iteration 1878/6000, G loss: 0.47234752774238586, D loss: 0.003262563841417432\n",
            "Iteration 1879/6000, G loss: 0.5577555298805237, D loss: 7.915470632724464e-06\n",
            "Iteration 1880/6000, G loss: 0.5191641449928284, D loss: 0.0003429631469771266\n",
            "Iteration 1881/6000, G loss: 0.6486514806747437, D loss: 2.384185648907078e-08\n",
            "Iteration 1882/6000, G loss: 0.4696776866912842, D loss: 0.020602043718099594\n",
            "Iteration 1883/6000, G loss: 0.570853590965271, D loss: 4.510768849286251e-05\n",
            "Iteration 1884/6000, G loss: 0.5288548469543457, D loss: 1.0734680472523905e-05\n",
            "Iteration 1885/6000, G loss: 0.5744885802268982, D loss: 4.714679562312085e-06\n",
            "Iteration 1886/6000, G loss: 0.6273396015167236, D loss: 5.960463766996327e-08\n",
            "Iteration 1887/6000, G loss: 0.7168676853179932, D loss: 0.0\n",
            "Iteration 1888/6000, G loss: 0.5197104811668396, D loss: 2.5188648578478023e-05\n",
            "Iteration 1889/6000, G loss: 0.5170058608055115, D loss: 0.0016513577429577708\n",
            "Iteration 1890/6000, G loss: 0.4560263156890869, D loss: 0.0014108287869021297\n",
            "Iteration 1891/6000, G loss: 0.5611406564712524, D loss: 0.0003385540039744228\n",
            "Iteration 1892/6000, G loss: 0.6758384108543396, D loss: 4.768371297814156e-08\n",
            "Iteration 1893/6000, G loss: 0.6723709106445312, D loss: 0.0\n",
            "Iteration 1894/6000, G loss: 0.6186056137084961, D loss: 7.629390665897517e-07\n",
            "Iteration 1895/6000, G loss: 0.4720204174518585, D loss: 9.256566045223735e-06\n",
            "Iteration 1896/6000, G loss: 0.5269833207130432, D loss: 0.00013877602759748697\n",
            "Iteration 1897/6000, G loss: 0.6126294732093811, D loss: 0.0005155279068276286\n",
            "Iteration 1898/6000, G loss: 0.6147633194923401, D loss: 1.382826440021745e-06\n",
            "Iteration 1899/6000, G loss: 0.5591317415237427, D loss: 0.00012233878078404814\n",
            "Iteration 1900/6000, G loss: 0.7367169857025146, D loss: 3.7550913134509756e-07\n",
            "Iteration 1901/6000, G loss: 0.5664947628974915, D loss: 1.0055257007479668e-05\n",
            "Iteration 1902/6000, G loss: 0.5756441950798035, D loss: 1.711147888272535e-05\n",
            "Iteration 1903/6000, G loss: 0.5407116413116455, D loss: 0.0013241891283541918\n",
            "Iteration 1904/6000, G loss: 0.4249371588230133, D loss: 0.0005616836715489626\n",
            "Iteration 1905/6000, G loss: 0.559598982334137, D loss: 0.0\n",
            "Iteration 1906/6000, G loss: 0.6369027495384216, D loss: 0.0\n",
            "Iteration 1907/6000, G loss: 0.6502277851104736, D loss: 1.2814989531761967e-06\n",
            "Iteration 1908/6000, G loss: 0.5269936323165894, D loss: 0.0\n",
            "Iteration 1909/6000, G loss: 0.5311936140060425, D loss: 2.1159630705369636e-06\n",
            "Iteration 1910/6000, G loss: 0.4973667860031128, D loss: 0.0060932813212275505\n",
            "Iteration 1911/6000, G loss: 0.47748684883117676, D loss: 3.5405109883868136e-06\n",
            "Iteration 1912/6000, G loss: 0.44708186388015747, D loss: 0.0008432553149759769\n",
            "Iteration 1913/6000, G loss: 0.5392398238182068, D loss: 1.6122954548336565e-05\n",
            "Iteration 1914/6000, G loss: 0.6936958432197571, D loss: 0.0\n",
            "Iteration 1915/6000, G loss: 0.6015016436576843, D loss: 8.761878689256264e-07\n",
            "Iteration 1916/6000, G loss: 0.5886109471321106, D loss: 4.511382576311007e-05\n",
            "Iteration 1917/6000, G loss: 0.677375078201294, D loss: 1.1146064480271889e-06\n",
            "Iteration 1918/6000, G loss: 0.6748943328857422, D loss: 1.2516970855358522e-06\n",
            "Iteration 1919/6000, G loss: 0.5885416269302368, D loss: 9.298321401729481e-07\n",
            "Iteration 1920/6000, G loss: 0.6271721124649048, D loss: 5.9782282733067404e-06\n",
            "Iteration 1921/6000, G loss: 0.6564277410507202, D loss: 0.0\n",
            "Iteration 1922/6000, G loss: 0.5675647854804993, D loss: 0.000759490008931607\n",
            "Iteration 1923/6000, G loss: 0.49243324995040894, D loss: 0.0013939032796770334\n",
            "Iteration 1924/6000, G loss: 0.5400567054748535, D loss: 1.1420204828027636e-05\n",
            "Iteration 1925/6000, G loss: 0.5818272829055786, D loss: 0.00010743210441432893\n",
            "Iteration 1926/6000, G loss: 0.5306087732315063, D loss: 0.0011456521460786462\n",
            "Iteration 1927/6000, G loss: 0.6507824063301086, D loss: 3.9219457903527655e-06\n",
            "Iteration 1928/6000, G loss: 0.5918900370597839, D loss: 1.565800812386442e-05\n",
            "Iteration 1929/6000, G loss: 0.5412184596061707, D loss: 5.0723438107525e-06\n",
            "Iteration 1930/6000, G loss: 0.5060305595397949, D loss: 0.0018052025698125362\n",
            "Iteration 1931/6000, G loss: 0.694077730178833, D loss: 1.152144068328198e-05\n",
            "Iteration 1932/6000, G loss: 0.6540734767913818, D loss: 5.006787091588194e-07\n",
            "Iteration 1933/6000, G loss: 0.595434844493866, D loss: 0.0004854051803704351\n",
            "Iteration 1934/6000, G loss: 0.4844408631324768, D loss: 0.8117266893386841\n",
            "Iteration 1935/6000, G loss: 0.5831955671310425, D loss: 2.088527980959043e-05\n",
            "Iteration 1936/6000, G loss: 0.6208047866821289, D loss: 1.5139569313760148e-06\n",
            "Iteration 1937/6000, G loss: 0.6625043749809265, D loss: 5.188830982660875e-05\n",
            "Iteration 1938/6000, G loss: 0.5074871778488159, D loss: 0.0003423269372433424\n",
            "Iteration 1939/6000, G loss: 0.582039475440979, D loss: 4.172324707951702e-08\n",
            "Iteration 1940/6000, G loss: 0.6154035329818726, D loss: 0.0\n",
            "Iteration 1941/6000, G loss: 0.6407627463340759, D loss: 2.384185471271394e-08\n",
            "Iteration 1942/6000, G loss: 0.5357774496078491, D loss: 1.2516915148808039e-06\n",
            "Iteration 1943/6000, G loss: 0.6570373177528381, D loss: 0.0\n",
            "Iteration 1944/6000, G loss: 0.6159478425979614, D loss: 0.0\n",
            "Iteration 1945/6000, G loss: 0.6444243788719177, D loss: 1.1324881654672936e-07\n",
            "Iteration 1946/6000, G loss: 0.46791255474090576, D loss: 0.003955159801989794\n",
            "Iteration 1947/6000, G loss: 0.6062048077583313, D loss: 4.010360498796217e-05\n",
            "Iteration 1948/6000, G loss: 0.4831424653530121, D loss: 0.00034441612660884857\n",
            "Iteration 1949/6000, G loss: 0.5480886101722717, D loss: 3.88939370168373e-05\n",
            "Iteration 1950/6000, G loss: 0.6547839045524597, D loss: 5.173646059120074e-06\n",
            "Iteration 1951/6000, G loss: 0.49617356061935425, D loss: 0.00045574374962598085\n",
            "Iteration 1952/6000, G loss: 0.5848119258880615, D loss: 2.163642648156383e-06\n",
            "Iteration 1953/6000, G loss: 0.6126925945281982, D loss: 5.364417532405241e-08\n",
            "Iteration 1954/6000, G loss: 0.6044930219650269, D loss: 0.0006684123072773218\n",
            "Iteration 1955/6000, G loss: 0.4725121259689331, D loss: 0.0009203384397551417\n",
            "Iteration 1956/6000, G loss: 0.6054369211196899, D loss: 2.974247991005541e-06\n",
            "Iteration 1957/6000, G loss: 0.47529566287994385, D loss: 0.012208260595798492\n",
            "Iteration 1958/6000, G loss: 0.5779761075973511, D loss: 0.003015076043084264\n",
            "Iteration 1959/6000, G loss: 0.5228323936462402, D loss: 6.710022717015818e-05\n",
            "Iteration 1960/6000, G loss: 0.6174256205558777, D loss: 1.084801056094875e-06\n",
            "Iteration 1961/6000, G loss: 0.6023414134979248, D loss: 0.00011256205470999703\n",
            "Iteration 1962/6000, G loss: 0.5903446674346924, D loss: 1.4841525626252405e-06\n",
            "Iteration 1963/6000, G loss: 0.5684733390808105, D loss: 9.477120670453587e-07\n",
            "Iteration 1964/6000, G loss: 0.580776572227478, D loss: 9.000252248370089e-06\n",
            "Iteration 1965/6000, G loss: 0.4931372106075287, D loss: 0.006478584371507168\n",
            "Iteration 1966/6000, G loss: 0.5795769095420837, D loss: 2.790043072309345e-05\n",
            "Iteration 1967/6000, G loss: 0.5596346855163574, D loss: 0.07087468355894089\n",
            "Iteration 1968/6000, G loss: 0.6722460985183716, D loss: 0.010290836915373802\n",
            "Iteration 1969/6000, G loss: 0.5112675428390503, D loss: 0.37488651275634766\n",
            "Iteration 1970/6000, G loss: 0.754270613193512, D loss: 0.16261306405067444\n",
            "Iteration 1971/6000, G loss: 0.6140629053115845, D loss: 2.768049716949463\n",
            "Iteration 1972/6000, G loss: 0.484477162361145, D loss: 1.688191533088684\n",
            "Iteration 1973/6000, G loss: 0.5895472764968872, D loss: 2.0950908947270364e-05\n",
            "Iteration 1974/6000, G loss: 0.6379390358924866, D loss: 6.30783470114693e-05\n",
            "Iteration 1975/6000, G loss: 0.5728809237480164, D loss: 0.01255507580935955\n",
            "Iteration 1976/6000, G loss: 0.4809848964214325, D loss: 0.02819427102804184\n",
            "Iteration 1977/6000, G loss: 0.624505341053009, D loss: 1.0377122634963598e-05\n",
            "Iteration 1978/6000, G loss: 0.5204101800918579, D loss: 8.183682439266704e-06\n",
            "Iteration 1979/6000, G loss: 0.5799981355667114, D loss: 1.1855266166094225e-05\n",
            "Iteration 1980/6000, G loss: 0.6075385808944702, D loss: 0.007780466228723526\n",
            "Iteration 1981/6000, G loss: 0.5953347682952881, D loss: 3.6597134567273315e-06\n",
            "Iteration 1982/6000, G loss: 0.5502607822418213, D loss: 6.854530170130602e-07\n",
            "Iteration 1983/6000, G loss: 0.591848611831665, D loss: 6.425342689908575e-06\n",
            "Iteration 1984/6000, G loss: 0.6446030139923096, D loss: 1.8653725419426337e-05\n",
            "Iteration 1985/6000, G loss: 0.5528626441955566, D loss: 5.43551541341003e-05\n",
            "Iteration 1986/6000, G loss: 0.6481821537017822, D loss: 0.00749603658914566\n",
            "Iteration 1987/6000, G loss: 0.6311424374580383, D loss: 1.3887847671867348e-06\n",
            "Iteration 1988/6000, G loss: 0.6673647165298462, D loss: 1.7881390590446244e-08\n",
            "Iteration 1989/6000, G loss: 0.5918654203414917, D loss: 3.5762766970037774e-07\n",
            "Iteration 1990/6000, G loss: 0.5868950486183167, D loss: 3.8181678974069655e-05\n",
            "Iteration 1991/6000, G loss: 0.533783495426178, D loss: 1.6689265294189681e-06\n",
            "Iteration 1992/6000, G loss: 0.5439885854721069, D loss: 0.03399595990777016\n",
            "Iteration 1993/6000, G loss: 0.5408793091773987, D loss: 7.852626731619239e-05\n",
            "Iteration 1994/6000, G loss: 0.5361253023147583, D loss: 0.04116193205118179\n",
            "Iteration 1995/6000, G loss: 0.5462497472763062, D loss: 0.0004896831815131009\n",
            "Iteration 1996/6000, G loss: 0.5785645246505737, D loss: 3.218649453629041e-07\n",
            "Iteration 1997/6000, G loss: 0.6451767683029175, D loss: 6.854525054222904e-07\n",
            "Iteration 1998/6000, G loss: 0.6170589923858643, D loss: 1.9090648493147455e-05\n",
            "Iteration 1999/6000, G loss: 0.5607063174247742, D loss: 6.908039722475223e-06\n",
            "Iteration 2000/6000, G loss: 0.5318638682365417, D loss: 1.609305036254227e-05\n",
            "Iteration 2001/6000, G loss: 0.6120967864990234, D loss: 2.3603377030667616e-06\n",
            "Iteration 2002/6000, G loss: 0.7251805067062378, D loss: 0.0\n",
            "Iteration 2003/6000, G loss: 0.6268709897994995, D loss: 2.562998417943163e-07\n",
            "Iteration 2004/6000, G loss: 0.7162178754806519, D loss: 8.9941531769e-06\n",
            "Iteration 2005/6000, G loss: 0.6400198936462402, D loss: 1.6033579868235392e-06\n",
            "Iteration 2006/6000, G loss: 0.4744619131088257, D loss: 5.4744894441682845e-05\n",
            "Iteration 2007/6000, G loss: 0.6481890678405762, D loss: 6.174891041155206e-06\n",
            "Iteration 2008/6000, G loss: 0.6910582184791565, D loss: 8.344642310476047e-07\n",
            "Iteration 2009/6000, G loss: 0.5856274962425232, D loss: 3.2303269108524546e-05\n",
            "Iteration 2010/6000, G loss: 0.5020328760147095, D loss: 1.211750441143522e-05\n",
            "Iteration 2011/6000, G loss: 0.5431805849075317, D loss: 0.0011564533924683928\n",
            "Iteration 2012/6000, G loss: 0.6736313104629517, D loss: 4.231924606301618e-07\n",
            "Iteration 2013/6000, G loss: 0.4808950126171112, D loss: 0.00012004970631096512\n",
            "Iteration 2014/6000, G loss: 0.4985244572162628, D loss: 0.00021214090520516038\n",
            "Iteration 2015/6000, G loss: 0.44533008337020874, D loss: 0.12837621569633484\n",
            "Iteration 2016/6000, G loss: 0.5386410355567932, D loss: 0.00046622782247141004\n",
            "Iteration 2017/6000, G loss: 0.5857429504394531, D loss: 1.8912051018560305e-05\n",
            "Iteration 2018/6000, G loss: 0.6341297030448914, D loss: 9.95385016722139e-06\n",
            "Iteration 2019/6000, G loss: 0.5733551383018494, D loss: 0.0016568673308938742\n",
            "Iteration 2020/6000, G loss: 0.6105199456214905, D loss: 5.21885885973461e-05\n",
            "Iteration 2021/6000, G loss: 0.5656024217605591, D loss: 8.39752028696239e-05\n",
            "Iteration 2022/6000, G loss: 0.500761866569519, D loss: 0.6631007194519043\n",
            "Iteration 2023/6000, G loss: 0.5404759645462036, D loss: 9.089662853511982e-06\n",
            "Iteration 2024/6000, G loss: 0.617754340171814, D loss: 0.00024079656577669084\n",
            "Iteration 2025/6000, G loss: 0.4624802768230438, D loss: 0.006722625810652971\n",
            "Iteration 2026/6000, G loss: 0.6402927041053772, D loss: 5.4178763093659654e-05\n",
            "Iteration 2027/6000, G loss: 0.5999736785888672, D loss: 1.829849679779727e-05\n",
            "Iteration 2028/6000, G loss: 0.5658798813819885, D loss: 0.07298053801059723\n",
            "Iteration 2029/6000, G loss: 0.5455055236816406, D loss: 0.00010902735812123865\n",
            "Iteration 2030/6000, G loss: 0.5774202346801758, D loss: 0.004341452848166227\n",
            "Iteration 2031/6000, G loss: 0.4844666123390198, D loss: 0.032764047384262085\n",
            "Iteration 2032/6000, G loss: 0.5917633771896362, D loss: 0.00036448269383981824\n",
            "Iteration 2033/6000, G loss: 0.5588712096214294, D loss: 0.6542162895202637\n",
            "Iteration 2034/6000, G loss: 0.4561733603477478, D loss: 0.015514523722231388\n",
            "Iteration 2035/6000, G loss: 0.5601453185081482, D loss: 0.00020794359443243593\n",
            "Iteration 2036/6000, G loss: 0.5469722747802734, D loss: 0.0051829335279762745\n",
            "Iteration 2037/6000, G loss: 0.6464821696281433, D loss: 2.287010647705756e-05\n",
            "Iteration 2038/6000, G loss: 0.5334579944610596, D loss: 1.2012302875518799\n",
            "Iteration 2039/6000, G loss: 0.6560186147689819, D loss: 2.9196572303771973\n",
            "Iteration 2040/6000, G loss: 0.6234458684921265, D loss: 0.0005274645518511534\n",
            "Iteration 2041/6000, G loss: 0.5813305974006653, D loss: 0.06472766399383545\n",
            "Iteration 2042/6000, G loss: 0.591444730758667, D loss: 0.0016875623259693384\n",
            "Iteration 2043/6000, G loss: 0.5479053854942322, D loss: 6.419405508495402e-06\n",
            "Iteration 2044/6000, G loss: 0.6829248666763306, D loss: 0.0\n",
            "Iteration 2045/6000, G loss: 0.6269077658653259, D loss: 0.0\n",
            "Iteration 2046/6000, G loss: 0.6335631608963013, D loss: 1.9973336748080328e-05\n",
            "Iteration 2047/6000, G loss: 0.6700478792190552, D loss: 0.9766322374343872\n",
            "Iteration 2048/6000, G loss: 0.5750323534011841, D loss: 1.4305112472356996e-07\n",
            "Iteration 2049/6000, G loss: 0.6723640561103821, D loss: 0.0\n",
            "Iteration 2050/6000, G loss: 0.619797945022583, D loss: 0.0\n",
            "Iteration 2051/6000, G loss: 0.5518358945846558, D loss: 1.156329517471022e-06\n",
            "Iteration 2052/6000, G loss: 0.5260980725288391, D loss: 1.1920928955078125e-07\n",
            "Iteration 2053/6000, G loss: 0.6293733716011047, D loss: 2.0742400010931306e-06\n",
            "Iteration 2054/6000, G loss: 0.5227599740028381, D loss: 9.179111657431349e-07\n",
            "Iteration 2055/6000, G loss: 0.6308555006980896, D loss: 1.4305113893442467e-07\n",
            "Iteration 2056/6000, G loss: 0.46834200620651245, D loss: 0.0003560544573701918\n",
            "Iteration 2057/6000, G loss: 0.4243870973587036, D loss: 0.0012138421880081296\n",
            "Iteration 2058/6000, G loss: 0.6094968318939209, D loss: 0.0\n",
            "Iteration 2059/6000, G loss: 0.6485176086425781, D loss: 1.0728835775353218e-07\n",
            "Iteration 2060/6000, G loss: 0.6162267327308655, D loss: 0.0\n",
            "Iteration 2061/6000, G loss: 0.5333855152130127, D loss: 1.4603128875023685e-06\n",
            "Iteration 2062/6000, G loss: 0.6004925966262817, D loss: 0.0\n",
            "Iteration 2063/6000, G loss: 0.5518210530281067, D loss: 0.0\n",
            "Iteration 2064/6000, G loss: 0.510059654712677, D loss: 0.0006674607284367085\n",
            "Iteration 2065/6000, G loss: 0.5757820010185242, D loss: 7.864042709115893e-05\n",
            "Iteration 2066/6000, G loss: 0.646124005317688, D loss: 1.192092824453539e-08\n",
            "Iteration 2067/6000, G loss: 0.6890181303024292, D loss: 0.0\n",
            "Iteration 2068/6000, G loss: 0.5483803153038025, D loss: 5.95737510593608e-05\n",
            "Iteration 2069/6000, G loss: 0.5900579690933228, D loss: 0.0\n",
            "Iteration 2070/6000, G loss: 0.5063685178756714, D loss: 4.55967165180482e-05\n",
            "Iteration 2071/6000, G loss: 0.5408343076705933, D loss: 6.616114660573658e-07\n",
            "Iteration 2072/6000, G loss: 0.6489202976226807, D loss: 0.0\n",
            "Iteration 2073/6000, G loss: 0.5329285264015198, D loss: 0.0018401052802801132\n",
            "Iteration 2074/6000, G loss: 0.6588446497917175, D loss: 0.0\n",
            "Iteration 2075/6000, G loss: 0.46186479926109314, D loss: 0.05115189775824547\n",
            "Iteration 2076/6000, G loss: 0.6481802463531494, D loss: 1.8537034520704765e-06\n",
            "Iteration 2077/6000, G loss: 0.5728957653045654, D loss: 4.4619104301091284e-05\n",
            "Iteration 2078/6000, G loss: 0.7030614018440247, D loss: 8.774867455940694e-05\n",
            "Iteration 2079/6000, G loss: 0.634781539440155, D loss: 0.00399908609688282\n",
            "Iteration 2080/6000, G loss: 0.44143378734588623, D loss: 0.00846746563911438\n",
            "Iteration 2081/6000, G loss: 0.4637223184108734, D loss: 0.0137624591588974\n",
            "Iteration 2082/6000, G loss: 0.4439762234687805, D loss: 0.00537168700248003\n",
            "Iteration 2083/6000, G loss: 0.6976242661476135, D loss: 0.20404231548309326\n",
            "Iteration 2084/6000, G loss: 0.6016707420349121, D loss: 3.0994411304163805e-07\n",
            "Iteration 2085/6000, G loss: 0.5243298411369324, D loss: 0.024843337014317513\n",
            "Iteration 2086/6000, G loss: 0.7068262100219727, D loss: 0.30842965841293335\n",
            "Iteration 2087/6000, G loss: 0.5517398118972778, D loss: 0.23235240578651428\n",
            "Iteration 2088/6000, G loss: 0.5397021174430847, D loss: 0.7266231775283813\n",
            "Iteration 2089/6000, G loss: 0.7602031826972961, D loss: 4.881612312601646e-06\n",
            "Iteration 2090/6000, G loss: 0.5299314260482788, D loss: 0.10914207249879837\n",
            "Iteration 2091/6000, G loss: 0.650687575340271, D loss: 1.4483921404462308e-06\n",
            "Iteration 2092/6000, G loss: 0.5553468465805054, D loss: 0.227763369679451\n",
            "Iteration 2093/6000, G loss: 0.6810820698738098, D loss: 0.06699876487255096\n",
            "Iteration 2094/6000, G loss: 0.5062259435653687, D loss: 4.518203258514404\n",
            "Iteration 2095/6000, G loss: 0.6921238899230957, D loss: 2.1398043372755637e-06\n",
            "Iteration 2096/6000, G loss: 0.6194090843200684, D loss: 2.4038003175519407e-05\n",
            "Iteration 2097/6000, G loss: 0.6293334364891052, D loss: 1.0579675290500745e-05\n",
            "Iteration 2098/6000, G loss: 0.460222065448761, D loss: 0.1370965838432312\n",
            "Iteration 2099/6000, G loss: 0.5325093269348145, D loss: 0.02585878223180771\n",
            "Iteration 2100/6000, G loss: 0.6506184339523315, D loss: 2.6738103770185262e-05\n",
            "Iteration 2101/6000, G loss: 0.5165541172027588, D loss: 0.017379237338900566\n",
            "Iteration 2102/6000, G loss: 0.5937176942825317, D loss: 0.016784900799393654\n",
            "Iteration 2103/6000, G loss: 0.6250827312469482, D loss: 0.0005063810967840254\n",
            "Iteration 2104/6000, G loss: 0.5150898694992065, D loss: 0.004549184814095497\n",
            "Iteration 2105/6000, G loss: 0.5461100339889526, D loss: 0.0065666185691952705\n",
            "Iteration 2106/6000, G loss: 0.5974288582801819, D loss: 0.007989675737917423\n",
            "Iteration 2107/6000, G loss: 0.6030205488204956, D loss: 0.00012090313248336315\n",
            "Iteration 2108/6000, G loss: 0.5142127275466919, D loss: 0.003086616052314639\n",
            "Iteration 2109/6000, G loss: 0.48991063237190247, D loss: 0.03539545089006424\n",
            "Iteration 2110/6000, G loss: 0.660423994064331, D loss: 6.806818419136107e-06\n",
            "Iteration 2111/6000, G loss: 0.4591224789619446, D loss: 0.0017422749660909176\n",
            "Iteration 2112/6000, G loss: 0.5676466226577759, D loss: 0.0002590143703855574\n",
            "Iteration 2113/6000, G loss: 0.5919992923736572, D loss: 2.445541394990869e-05\n",
            "Iteration 2114/6000, G loss: 0.5897974967956543, D loss: 0.004520046524703503\n",
            "Iteration 2115/6000, G loss: 0.5579037070274353, D loss: 0.0007174448110163212\n",
            "Iteration 2116/6000, G loss: 0.5323291420936584, D loss: 0.0003694737679325044\n",
            "Iteration 2117/6000, G loss: 0.5079438090324402, D loss: 0.00030315446201711893\n",
            "Iteration 2118/6000, G loss: 0.6056423783302307, D loss: 0.0002623121836222708\n",
            "Iteration 2119/6000, G loss: 0.5115492343902588, D loss: 0.00885925255715847\n",
            "Iteration 2120/6000, G loss: 0.6258155703544617, D loss: 0.00037487217923626304\n",
            "Iteration 2121/6000, G loss: 0.5611579418182373, D loss: 0.0003397337277419865\n",
            "Iteration 2122/6000, G loss: 0.6801803708076477, D loss: 0.00010404970817035064\n",
            "Iteration 2123/6000, G loss: 0.613226592540741, D loss: 0.0008407372515648603\n",
            "Iteration 2124/6000, G loss: 0.4570543169975281, D loss: 0.0003112526028417051\n",
            "Iteration 2125/6000, G loss: 0.6060896515846252, D loss: 9.161007255897857e-06\n",
            "Iteration 2126/6000, G loss: 0.6021578311920166, D loss: 2.946808672277257e-05\n",
            "Iteration 2127/6000, G loss: 0.5134167075157166, D loss: 0.006466022692620754\n",
            "Iteration 2128/6000, G loss: 0.5128226280212402, D loss: 0.0065782577730715275\n",
            "Iteration 2129/6000, G loss: 0.5785930752754211, D loss: 1.9490471458993852e-05\n",
            "Iteration 2130/6000, G loss: 0.57876056432724, D loss: 2.26497618882604e-07\n",
            "Iteration 2131/6000, G loss: 0.5700534582138062, D loss: 0.0001868779945652932\n",
            "Iteration 2132/6000, G loss: 0.5695033073425293, D loss: 0.0001438479230273515\n",
            "Iteration 2133/6000, G loss: 0.4908146262168884, D loss: 0.003852304071187973\n",
            "Iteration 2134/6000, G loss: 0.5251742601394653, D loss: 0.001445487141609192\n",
            "Iteration 2135/6000, G loss: 0.6477485299110413, D loss: 6.9882495154161e-05\n",
            "Iteration 2136/6000, G loss: 0.560106635093689, D loss: 0.0001686202740529552\n",
            "Iteration 2137/6000, G loss: 0.6205231547355652, D loss: 2.9385046218521893e-06\n",
            "Iteration 2138/6000, G loss: 0.7121357321739197, D loss: 0.0\n",
            "Iteration 2139/6000, G loss: 0.4820195436477661, D loss: 0.0006229859427548945\n",
            "Iteration 2140/6000, G loss: 0.6312353014945984, D loss: 2.131426845153328e-05\n",
            "Iteration 2141/6000, G loss: 0.6319394707679749, D loss: 4.172324707951702e-08\n",
            "Iteration 2142/6000, G loss: 0.5719531178474426, D loss: 1.8977119907503948e-05\n",
            "Iteration 2143/6000, G loss: 0.536782443523407, D loss: 0.0001876595342764631\n",
            "Iteration 2144/6000, G loss: 0.4637044072151184, D loss: 0.06451793015003204\n",
            "Iteration 2145/6000, G loss: 0.4775385558605194, D loss: 0.0030812970362603664\n",
            "Iteration 2146/6000, G loss: 0.6255264282226562, D loss: 4.452451321412809e-06\n",
            "Iteration 2147/6000, G loss: 0.45287901163101196, D loss: 0.010536547750234604\n",
            "Iteration 2148/6000, G loss: 0.5044826865196228, D loss: 0.0004939155769534409\n",
            "Iteration 2149/6000, G loss: 0.46638572216033936, D loss: 0.0006515393615700305\n",
            "Iteration 2150/6000, G loss: 0.5711644887924194, D loss: 0.0011930018663406372\n",
            "Iteration 2151/6000, G loss: 0.6034331321716309, D loss: 2.872934146580519e-06\n",
            "Iteration 2152/6000, G loss: 0.5175072550773621, D loss: 0.0007246721652336419\n",
            "Iteration 2153/6000, G loss: 0.522723913192749, D loss: 0.0026445677503943443\n",
            "Iteration 2154/6000, G loss: 0.5550570487976074, D loss: 0.000259935186477378\n",
            "Iteration 2155/6000, G loss: 0.5407767295837402, D loss: 5.3809882956556976e-05\n",
            "Iteration 2156/6000, G loss: 0.5462679266929626, D loss: 0.000953670940361917\n",
            "Iteration 2157/6000, G loss: 0.4906022250652313, D loss: 0.0018308385042473674\n",
            "Iteration 2158/6000, G loss: 0.5159942507743835, D loss: 0.0006242315284907818\n",
            "Iteration 2159/6000, G loss: 0.5185577869415283, D loss: 1.826270090532489e-05\n",
            "Iteration 2160/6000, G loss: 0.5222377181053162, D loss: 3.461768937995657e-05\n",
            "Iteration 2161/6000, G loss: 0.5617548227310181, D loss: 2.6291276299161837e-05\n",
            "Iteration 2162/6000, G loss: 0.7054244875907898, D loss: 0.0\n",
            "Iteration 2163/6000, G loss: 0.4314379394054413, D loss: 0.0007331238593906164\n",
            "Iteration 2164/6000, G loss: 0.39978328347206116, D loss: 0.0016400661552324891\n",
            "Iteration 2165/6000, G loss: 0.6398202180862427, D loss: 1.0132789185490765e-07\n",
            "Iteration 2166/6000, G loss: 0.5826062560081482, D loss: 5.137910193298012e-06\n",
            "Iteration 2167/6000, G loss: 0.6609858870506287, D loss: 6.1392620409606025e-06\n",
            "Iteration 2168/6000, G loss: 0.5547779202461243, D loss: 3.0934770620660856e-06\n",
            "Iteration 2169/6000, G loss: 0.4087343215942383, D loss: 0.04239291697740555\n",
            "Iteration 2170/6000, G loss: 0.4493774175643921, D loss: 0.0071537187322974205\n",
            "Iteration 2171/6000, G loss: 0.4900190532207489, D loss: 0.00037103530485183\n",
            "Iteration 2172/6000, G loss: 0.5926929116249084, D loss: 0.0001273752423003316\n",
            "Iteration 2173/6000, G loss: 0.6631230711936951, D loss: 2.324580918866559e-07\n",
            "Iteration 2174/6000, G loss: 0.554069995880127, D loss: 0.0006170577835291624\n",
            "Iteration 2175/6000, G loss: 0.5102106928825378, D loss: 0.0015498772263526917\n",
            "Iteration 2176/6000, G loss: 0.49410778284072876, D loss: 2.503394682662474e-07\n",
            "Iteration 2177/6000, G loss: 0.5132132172584534, D loss: 4.139486190979369e-05\n",
            "Iteration 2178/6000, G loss: 0.5111552476882935, D loss: 0.00013289421622175723\n",
            "Iteration 2179/6000, G loss: 0.6811102628707886, D loss: 1.938330206030514e-05\n",
            "Iteration 2180/6000, G loss: 0.4688543677330017, D loss: 3.424251917749643e-05\n",
            "Iteration 2181/6000, G loss: 0.5419775247573853, D loss: 2.16065291169798e-05\n",
            "Iteration 2182/6000, G loss: 0.4853340685367584, D loss: 0.002102409955114126\n",
            "Iteration 2183/6000, G loss: 0.4038497805595398, D loss: 0.0010683247819542885\n",
            "Iteration 2184/6000, G loss: 0.49307382106781006, D loss: 9.943537588696927e-05\n",
            "Iteration 2185/6000, G loss: 0.660650372505188, D loss: 1.4722340893058572e-06\n",
            "Iteration 2186/6000, G loss: 0.7091716527938843, D loss: 3.8146950487316644e-07\n",
            "Iteration 2187/6000, G loss: 0.7394953966140747, D loss: 1.7905113054439425e-05\n",
            "Iteration 2188/6000, G loss: 0.6377416849136353, D loss: 0.00011004955740645528\n",
            "Iteration 2189/6000, G loss: 0.6598035097122192, D loss: 5.751839125878178e-06\n",
            "Iteration 2190/6000, G loss: 0.4753706455230713, D loss: 0.00018697022460401058\n",
            "Iteration 2191/6000, G loss: 0.6233335137367249, D loss: 2.0265576949896058e-07\n",
            "Iteration 2192/6000, G loss: 0.5767547488212585, D loss: 9.582379425410181e-05\n",
            "Iteration 2193/6000, G loss: 0.6170382499694824, D loss: 7.5518896665016655e-06\n",
            "Iteration 2194/6000, G loss: 0.5290477275848389, D loss: 0.009203216060996056\n",
            "Iteration 2195/6000, G loss: 0.5509399771690369, D loss: 1.5175244698184542e-05\n",
            "Iteration 2196/6000, G loss: 0.5859024524688721, D loss: 0.00011946512677241117\n",
            "Iteration 2197/6000, G loss: 0.5108863711357117, D loss: 0.0023534579668194056\n",
            "Iteration 2198/6000, G loss: 0.6257506012916565, D loss: 1.4156041288515553e-05\n",
            "Iteration 2199/6000, G loss: 0.5443205237388611, D loss: 0.002653678646311164\n",
            "Iteration 2200/6000, G loss: 0.5796332955360413, D loss: 1.8566752260085195e-05\n",
            "Iteration 2201/6000, G loss: 0.5992388129234314, D loss: 5.400138434197288e-06\n",
            "Iteration 2202/6000, G loss: 0.575268030166626, D loss: 0.00020032518659718335\n",
            "Iteration 2203/6000, G loss: 0.6037358641624451, D loss: 0.004558239132165909\n",
            "Iteration 2204/6000, G loss: 0.630934476852417, D loss: 0.0019149199360981584\n",
            "Iteration 2205/6000, G loss: 0.4282163083553314, D loss: 0.023607661947607994\n",
            "Iteration 2206/6000, G loss: 0.5367997288703918, D loss: 0.001140524516813457\n",
            "Iteration 2207/6000, G loss: 0.40784719586372375, D loss: 0.03694137558341026\n",
            "Iteration 2208/6000, G loss: 0.5013227462768555, D loss: 0.002843344584107399\n",
            "Iteration 2209/6000, G loss: 0.544525146484375, D loss: 6.490933628811035e-06\n",
            "Iteration 2210/6000, G loss: 0.5594034790992737, D loss: 0.00040191522566601634\n",
            "Iteration 2211/6000, G loss: 0.5093948841094971, D loss: 0.00033592121326364577\n",
            "Iteration 2212/6000, G loss: 0.6283644437789917, D loss: 0.000707289029378444\n",
            "Iteration 2213/6000, G loss: 0.5163331031799316, D loss: 1.0812248547154013e-05\n",
            "Iteration 2214/6000, G loss: 0.5007753372192383, D loss: 0.006838830187916756\n",
            "Iteration 2215/6000, G loss: 0.5219888091087341, D loss: 0.0015569960232824087\n",
            "Iteration 2216/6000, G loss: 0.5417237281799316, D loss: 2.4944372853497043e-05\n",
            "Iteration 2217/6000, G loss: 0.7025705575942993, D loss: 0.00010475733870407566\n",
            "Iteration 2218/6000, G loss: 0.5850123763084412, D loss: 0.0009566631633788347\n",
            "Iteration 2219/6000, G loss: 0.5015822649002075, D loss: 0.023179609328508377\n",
            "Iteration 2220/6000, G loss: 0.4979335069656372, D loss: 0.028066273778676987\n",
            "Iteration 2221/6000, G loss: 0.5935767292976379, D loss: 0.0019001878099516034\n",
            "Iteration 2222/6000, G loss: 0.595643162727356, D loss: 0.0013974191388115287\n",
            "Iteration 2223/6000, G loss: 0.5572053790092468, D loss: 0.019288573414087296\n",
            "Iteration 2224/6000, G loss: 0.616504430770874, D loss: 1.4576878547668457\n",
            "Iteration 2225/6000, G loss: 0.5518161058425903, D loss: 0.028257152065634727\n",
            "Iteration 2226/6000, G loss: 0.6449103355407715, D loss: 0.00017988882609643042\n",
            "Iteration 2227/6000, G loss: 0.4870879650115967, D loss: 0.00025271117920055985\n",
            "Iteration 2228/6000, G loss: 0.570012092590332, D loss: 2.92596232611686e-05\n",
            "Iteration 2229/6000, G loss: 0.6266634464263916, D loss: 0.00012641458306461573\n",
            "Iteration 2230/6000, G loss: 0.6009863615036011, D loss: 0.010089343413710594\n",
            "Iteration 2231/6000, G loss: 0.5601012110710144, D loss: 0.046034134924411774\n",
            "Iteration 2232/6000, G loss: 0.5962809324264526, D loss: 0.009451799094676971\n",
            "Iteration 2233/6000, G loss: 0.5054047703742981, D loss: 8.002571848919615e-05\n",
            "Iteration 2234/6000, G loss: 0.7253005504608154, D loss: 0.00089406652841717\n",
            "Iteration 2235/6000, G loss: 0.7278107404708862, D loss: 4.357084435469005e-06\n",
            "Iteration 2236/6000, G loss: 0.5414476990699768, D loss: 0.00012481919839046896\n",
            "Iteration 2237/6000, G loss: 0.4611756205558777, D loss: 0.020807724446058273\n",
            "Iteration 2238/6000, G loss: 0.5215808749198914, D loss: 0.00045256511657498777\n",
            "Iteration 2239/6000, G loss: 0.5029239058494568, D loss: 0.31832242012023926\n",
            "Iteration 2240/6000, G loss: 0.5813471078872681, D loss: 0.022068845108151436\n",
            "Iteration 2241/6000, G loss: 0.6082227230072021, D loss: 0.009838930331170559\n",
            "Iteration 2242/6000, G loss: 0.6748981475830078, D loss: 2.3186182716017356e-06\n",
            "Iteration 2243/6000, G loss: 0.5836898684501648, D loss: 0.018747080117464066\n",
            "Iteration 2244/6000, G loss: 0.594194233417511, D loss: 0.0007580261444672942\n",
            "Iteration 2245/6000, G loss: 0.6997895240783691, D loss: 0.09053941071033478\n",
            "Iteration 2246/6000, G loss: 0.4412688612937927, D loss: 0.2772982716560364\n",
            "Iteration 2247/6000, G loss: 0.6264913082122803, D loss: 0.00011293987336102873\n",
            "Iteration 2248/6000, G loss: 0.48473304510116577, D loss: 0.028320882469415665\n",
            "Iteration 2249/6000, G loss: 0.6328376531600952, D loss: 0.0026853024028241634\n",
            "Iteration 2250/6000, G loss: 0.45065560936927795, D loss: 0.24141520261764526\n",
            "Iteration 2251/6000, G loss: 0.560055673122406, D loss: 5.631911277770996\n",
            "Iteration 2252/6000, G loss: 0.6493331789970398, D loss: 0.26309365034103394\n",
            "Iteration 2253/6000, G loss: 0.5362847447395325, D loss: 0.0\n",
            "Iteration 2254/6000, G loss: 0.5519567131996155, D loss: 4.309407813707367e-06\n",
            "Iteration 2255/6000, G loss: 0.5497999787330627, D loss: 0.04297851771116257\n",
            "Iteration 2256/6000, G loss: 0.5772813558578491, D loss: 0.0002940315753221512\n",
            "Iteration 2257/6000, G loss: 0.6112884283065796, D loss: 0.001064955722540617\n",
            "Iteration 2258/6000, G loss: 0.648890495300293, D loss: 0.000434207177022472\n",
            "Iteration 2259/6000, G loss: 0.6608442664146423, D loss: 4.4392389099812135e-05\n",
            "Iteration 2260/6000, G loss: 0.5873426795005798, D loss: 0.00020379690977279097\n",
            "Iteration 2261/6000, G loss: 0.5937254428863525, D loss: 9.31855320231989e-05\n",
            "Iteration 2262/6000, G loss: 0.5698129534721375, D loss: 0.00021061275037936866\n",
            "Iteration 2263/6000, G loss: 0.4590594172477722, D loss: 0.0023018312640488148\n",
            "Iteration 2264/6000, G loss: 0.647579550743103, D loss: 0.0014702480984851718\n",
            "Iteration 2265/6000, G loss: 0.45180729031562805, D loss: 0.003215252421796322\n",
            "Iteration 2266/6000, G loss: 0.5638719797134399, D loss: 0.008319096639752388\n",
            "Iteration 2267/6000, G loss: 0.5512811541557312, D loss: 0.024566777050495148\n",
            "Iteration 2268/6000, G loss: 0.4877261519432068, D loss: 0.008731313981115818\n",
            "Iteration 2269/6000, G loss: 0.47695088386535645, D loss: 0.00840693898499012\n",
            "Iteration 2270/6000, G loss: 0.5705280900001526, D loss: 0.00043779975385405123\n",
            "Iteration 2271/6000, G loss: 0.539774477481842, D loss: 0.009618857875466347\n",
            "Iteration 2272/6000, G loss: 0.47550931572914124, D loss: 0.00436371099203825\n",
            "Iteration 2273/6000, G loss: 0.6207337379455566, D loss: 0.0012827594764530659\n",
            "Iteration 2274/6000, G loss: 0.6055343151092529, D loss: 0.0003523563500493765\n",
            "Iteration 2275/6000, G loss: 0.5390005111694336, D loss: 0.01163051649928093\n",
            "Iteration 2276/6000, G loss: 0.5626514554023743, D loss: 0.003080526599660516\n",
            "Iteration 2277/6000, G loss: 0.4497509300708771, D loss: 0.08586465567350388\n",
            "Iteration 2278/6000, G loss: 0.5516166090965271, D loss: 0.00028401712188497186\n",
            "Iteration 2279/6000, G loss: 0.5752925872802734, D loss: 0.00013913815200794488\n",
            "Iteration 2280/6000, G loss: 0.46416786313056946, D loss: 0.0601886510848999\n",
            "Iteration 2281/6000, G loss: 0.6006016731262207, D loss: 0.00012549746315926313\n",
            "Iteration 2282/6000, G loss: 0.6104927659034729, D loss: 0.00041552179027348757\n",
            "Iteration 2283/6000, G loss: 0.5037438273429871, D loss: 0.00930369459092617\n",
            "Iteration 2284/6000, G loss: 0.6430433392524719, D loss: 8.548809273634106e-05\n",
            "Iteration 2285/6000, G loss: 0.5034867525100708, D loss: 0.008700699545443058\n",
            "Iteration 2286/6000, G loss: 0.5296297073364258, D loss: 0.011209240183234215\n",
            "Iteration 2287/6000, G loss: 0.503991961479187, D loss: 0.014030583202838898\n",
            "Iteration 2288/6000, G loss: 0.4957888722419739, D loss: 0.0010964646935462952\n",
            "Iteration 2289/6000, G loss: 0.6429771184921265, D loss: 0.0001185307337436825\n",
            "Iteration 2290/6000, G loss: 0.5874859094619751, D loss: 0.0018477581907063723\n",
            "Iteration 2291/6000, G loss: 0.566233217716217, D loss: 0.0004542535752989352\n",
            "Iteration 2292/6000, G loss: 0.5003364086151123, D loss: 0.0028948038816452026\n",
            "Iteration 2293/6000, G loss: 0.45248934626579285, D loss: 0.0007085782708600163\n",
            "Iteration 2294/6000, G loss: 0.6867241263389587, D loss: 2.2983407689025626e-05\n",
            "Iteration 2295/6000, G loss: 0.7563039660453796, D loss: 1.9532333681127056e-05\n",
            "Iteration 2296/6000, G loss: 0.7125222086906433, D loss: 5.5383767175953835e-05\n",
            "Iteration 2297/6000, G loss: 0.5413103699684143, D loss: 0.010309120640158653\n",
            "Iteration 2298/6000, G loss: 0.6095462441444397, D loss: 0.0006600901251658797\n",
            "Iteration 2299/6000, G loss: 0.5662386417388916, D loss: 0.015622930601239204\n",
            "Iteration 2300/6000, G loss: 0.5730687379837036, D loss: 0.006658424157649279\n",
            "Iteration 2301/6000, G loss: 0.599392831325531, D loss: 0.007262743078172207\n",
            "Iteration 2302/6000, G loss: 0.5232385396957397, D loss: 0.034305818378925323\n",
            "Iteration 2303/6000, G loss: 0.539858877658844, D loss: 0.003544297767803073\n",
            "Iteration 2304/6000, G loss: 0.6268057227134705, D loss: 0.00022662087576463819\n",
            "Iteration 2305/6000, G loss: 0.5429167151451111, D loss: 0.0004497820627875626\n",
            "Iteration 2306/6000, G loss: 0.5866735577583313, D loss: 0.0006965958746150136\n",
            "Iteration 2307/6000, G loss: 0.5446648001670837, D loss: 0.008119571954011917\n",
            "Iteration 2308/6000, G loss: 0.4344075620174408, D loss: 0.15148583054542542\n",
            "Iteration 2309/6000, G loss: 0.6359369158744812, D loss: 0.0003249467699788511\n",
            "Iteration 2310/6000, G loss: 0.5135101675987244, D loss: 0.00013184690033085644\n",
            "Iteration 2311/6000, G loss: 0.5357260704040527, D loss: 0.00033387812436558306\n",
            "Iteration 2312/6000, G loss: 0.49433642625808716, D loss: 0.015197886154055595\n",
            "Iteration 2313/6000, G loss: 0.6168422698974609, D loss: 0.0012340517714619637\n",
            "Iteration 2314/6000, G loss: 0.583080530166626, D loss: 0.0002614600816741586\n",
            "Iteration 2315/6000, G loss: 0.6074375510215759, D loss: 0.0045007080771028996\n",
            "Iteration 2316/6000, G loss: 0.4441503882408142, D loss: 0.21309000253677368\n",
            "Iteration 2317/6000, G loss: 0.6125281453132629, D loss: 1.6480604244861752e-05\n",
            "Iteration 2318/6000, G loss: 0.5422423481941223, D loss: 5.2087754738749936e-05\n",
            "Iteration 2319/6000, G loss: 0.5164065361022949, D loss: 0.030866723507642746\n",
            "Iteration 2320/6000, G loss: 0.7090293169021606, D loss: 6.556507514687837e-08\n",
            "Iteration 2321/6000, G loss: 0.5508550405502319, D loss: 4.881613222096348e-06\n",
            "Iteration 2322/6000, G loss: 0.48005205392837524, D loss: 0.0023295064456760883\n",
            "Iteration 2323/6000, G loss: 0.541926383972168, D loss: 4.7247896873159334e-05\n",
            "Iteration 2324/6000, G loss: 0.5206131339073181, D loss: 0.005940368864685297\n",
            "Iteration 2325/6000, G loss: 0.5020694136619568, D loss: 0.007424443960189819\n",
            "Iteration 2326/6000, G loss: 0.5805370807647705, D loss: 0.005957022309303284\n",
            "Iteration 2327/6000, G loss: 0.4300152063369751, D loss: 0.002294266829267144\n",
            "Iteration 2328/6000, G loss: 0.5190784931182861, D loss: 0.00015115093265194446\n",
            "Iteration 2329/6000, G loss: 0.6560438871383667, D loss: 0.002582613844424486\n",
            "Iteration 2330/6000, G loss: 0.6229466795921326, D loss: 0.9649283289909363\n",
            "Iteration 2331/6000, G loss: 0.6223382949829102, D loss: 0.0863608717918396\n",
            "Iteration 2332/6000, G loss: 0.5121512413024902, D loss: 0.03022601455450058\n",
            "Iteration 2333/6000, G loss: 0.661030650138855, D loss: 0.00024813524214550853\n",
            "Iteration 2334/6000, G loss: 0.41846343874931335, D loss: 0.0010698423720896244\n",
            "Iteration 2335/6000, G loss: 0.595095157623291, D loss: 0.0002923654392361641\n",
            "Iteration 2336/6000, G loss: 0.5489673018455505, D loss: 0.006094094831496477\n",
            "Iteration 2337/6000, G loss: 0.5537127256393433, D loss: 4.7748180804774165e-05\n",
            "Iteration 2338/6000, G loss: 0.5036068558692932, D loss: 0.002107863314449787\n",
            "Iteration 2339/6000, G loss: 0.6625937223434448, D loss: 0.0016827797517180443\n",
            "Iteration 2340/6000, G loss: 0.5688832402229309, D loss: 0.059164468199014664\n",
            "Iteration 2341/6000, G loss: 0.5683606863021851, D loss: 3.9935105178301455e-07\n",
            "Iteration 2342/6000, G loss: 0.5803878307342529, D loss: 0.000601097010076046\n",
            "Iteration 2343/6000, G loss: 0.6710011959075928, D loss: 0.12041538953781128\n",
            "Iteration 2344/6000, G loss: 0.5899770259857178, D loss: 0.06907813251018524\n",
            "Iteration 2345/6000, G loss: 0.6173248887062073, D loss: 39.06328201293945\n",
            "Iteration 2346/6000, G loss: 0.5709438323974609, D loss: 0.7062261700630188\n",
            "Iteration 2347/6000, G loss: 0.3956931233406067, D loss: 0.7346669435501099\n",
            "Iteration 2348/6000, G loss: 0.4942840039730072, D loss: 0.4975762367248535\n",
            "Iteration 2349/6000, G loss: 0.6375520825386047, D loss: 0.865288496017456\n",
            "Iteration 2350/6000, G loss: 0.6096742749214172, D loss: 1.0546056032180786\n",
            "Iteration 2351/6000, G loss: 0.5758960843086243, D loss: 0.9805905818939209\n",
            "Iteration 2352/6000, G loss: 0.5681294798851013, D loss: 0.9540987014770508\n",
            "Iteration 2353/6000, G loss: 0.44704771041870117, D loss: 1.1370384693145752\n",
            "Iteration 2354/6000, G loss: 0.4493611454963684, D loss: 1.1077450513839722\n",
            "Iteration 2355/6000, G loss: 0.5643137097358704, D loss: 1.113295555114746\n",
            "Iteration 2356/6000, G loss: 0.574240505695343, D loss: 1.044632077217102\n",
            "Iteration 2357/6000, G loss: 0.5917331576347351, D loss: 1.0090320110321045\n",
            "Iteration 2358/6000, G loss: 0.5985862016677856, D loss: 0.9732528924942017\n",
            "Iteration 2359/6000, G loss: 0.5538651347160339, D loss: 0.9297823905944824\n",
            "Iteration 2360/6000, G loss: 0.4323803186416626, D loss: 0.9124249219894409\n",
            "Iteration 2361/6000, G loss: 0.5393179059028625, D loss: 0.7493364810943604\n",
            "Iteration 2362/6000, G loss: 0.40631207823753357, D loss: 0.8216593861579895\n",
            "Iteration 2363/6000, G loss: 0.5447834134101868, D loss: 0.5664060115814209\n",
            "Iteration 2364/6000, G loss: 0.4670279324054718, D loss: 0.4945594370365143\n",
            "Iteration 2365/6000, G loss: 0.5347550511360168, D loss: 0.48567602038383484\n",
            "Iteration 2366/6000, G loss: 0.47760462760925293, D loss: 0.2843101918697357\n",
            "Iteration 2367/6000, G loss: 0.6167783737182617, D loss: 0.206872820854187\n",
            "Iteration 2368/6000, G loss: 0.5726666450500488, D loss: 0.10099741816520691\n",
            "Iteration 2369/6000, G loss: 0.5356716513633728, D loss: 0.08679335564374924\n",
            "Iteration 2370/6000, G loss: 0.5117512941360474, D loss: 0.08971612900495529\n",
            "Iteration 2371/6000, G loss: 0.3982999324798584, D loss: 0.04501761868596077\n",
            "Iteration 2372/6000, G loss: 0.5590745210647583, D loss: 0.011884238570928574\n",
            "Iteration 2373/6000, G loss: 0.4940294921398163, D loss: 0.021007731556892395\n",
            "Iteration 2374/6000, G loss: 0.6392872333526611, D loss: 0.0029891703743487597\n",
            "Iteration 2375/6000, G loss: 0.5203912258148193, D loss: 0.0009152271086350083\n",
            "Iteration 2376/6000, G loss: 0.5587959885597229, D loss: 0.002243216149508953\n",
            "Iteration 2377/6000, G loss: 0.5908276438713074, D loss: 0.008504082448780537\n",
            "Iteration 2378/6000, G loss: 0.5720912218093872, D loss: 0.0006628188421018422\n",
            "Iteration 2379/6000, G loss: 0.5018793940544128, D loss: 0.0018705637194216251\n",
            "Iteration 2380/6000, G loss: 0.49652671813964844, D loss: 0.0007658837712369859\n",
            "Iteration 2381/6000, G loss: 0.7034981846809387, D loss: 0.00016839589807204902\n",
            "Iteration 2382/6000, G loss: 0.4719105660915375, D loss: 0.0004706245381385088\n",
            "Iteration 2383/6000, G loss: 0.48597604036331177, D loss: 0.0014864567201584578\n",
            "Iteration 2384/6000, G loss: 0.5356221199035645, D loss: 0.0006938257720321417\n",
            "Iteration 2385/6000, G loss: 0.6386238932609558, D loss: 0.00109440041705966\n",
            "Iteration 2386/6000, G loss: 0.5619655251502991, D loss: 0.0001648118341108784\n",
            "Iteration 2387/6000, G loss: 0.5542423725128174, D loss: 7.016855670372024e-05\n",
            "Iteration 2388/6000, G loss: 0.5706315040588379, D loss: 7.230720802908763e-05\n",
            "Iteration 2389/6000, G loss: 0.5570304989814758, D loss: 0.0011658298317342997\n",
            "Iteration 2390/6000, G loss: 0.6015769839286804, D loss: 7.169625314418226e-05\n",
            "Iteration 2391/6000, G loss: 0.4950951039791107, D loss: 0.00016574343317188323\n",
            "Iteration 2392/6000, G loss: 0.560517430305481, D loss: 4.9946764193009585e-05\n",
            "Iteration 2393/6000, G loss: 0.6403226852416992, D loss: 0.0003194226010236889\n",
            "Iteration 2394/6000, G loss: 0.5488286018371582, D loss: 0.00011048905435018241\n",
            "Iteration 2395/6000, G loss: 0.5025544762611389, D loss: 0.00014722168270964175\n",
            "Iteration 2396/6000, G loss: 0.6209871172904968, D loss: 0.00015035881369840354\n",
            "Iteration 2397/6000, G loss: 0.673865795135498, D loss: 0.0031445170752704144\n",
            "Iteration 2398/6000, G loss: 0.49463820457458496, D loss: 3.082625335082412e-05\n",
            "Iteration 2399/6000, G loss: 0.5782111883163452, D loss: 4.9665897677186877e-05\n",
            "Iteration 2400/6000, G loss: 0.7114576101303101, D loss: 7.178806117735803e-05\n",
            "Iteration 2401/6000, G loss: 0.4856519103050232, D loss: 0.0009706916753202677\n",
            "Iteration 2402/6000, G loss: 0.6142392754554749, D loss: 0.0015943998005241156\n",
            "Iteration 2403/6000, G loss: 0.5643752813339233, D loss: 0.0006772330962121487\n",
            "Iteration 2404/6000, G loss: 0.4721929430961609, D loss: 0.0005963381263427436\n",
            "Iteration 2405/6000, G loss: 0.5425587892532349, D loss: 0.0004198970564175397\n",
            "Iteration 2406/6000, G loss: 0.5258709788322449, D loss: 0.003912976942956448\n",
            "Iteration 2407/6000, G loss: 0.5470618009567261, D loss: 0.0006070799427106977\n",
            "Iteration 2408/6000, G loss: 0.46569404006004333, D loss: 0.00026328852982260287\n",
            "Iteration 2409/6000, G loss: 0.4813803732395172, D loss: 0.00024040059361141175\n",
            "Iteration 2410/6000, G loss: 0.4878881871700287, D loss: 0.0009375388617627323\n",
            "Iteration 2411/6000, G loss: 0.6037395596504211, D loss: 0.004676328971982002\n",
            "Iteration 2412/6000, G loss: 0.5311332941055298, D loss: 0.0020117999520152807\n",
            "Iteration 2413/6000, G loss: 0.5775686502456665, D loss: 0.00035144976573064923\n",
            "Iteration 2414/6000, G loss: 0.6502286195755005, D loss: 0.0028563467785716057\n",
            "Iteration 2415/6000, G loss: 0.5521661639213562, D loss: 0.0018319983500987291\n",
            "Iteration 2416/6000, G loss: 0.5970308184623718, D loss: 0.003988555166870356\n",
            "Iteration 2417/6000, G loss: 0.5055155754089355, D loss: 0.0008832521270960569\n",
            "Iteration 2418/6000, G loss: 0.44405993819236755, D loss: 0.012161860242486\n",
            "Iteration 2419/6000, G loss: 0.6647723913192749, D loss: 0.012058237567543983\n",
            "Iteration 2420/6000, G loss: 0.5130847692489624, D loss: 0.08889121562242508\n",
            "Iteration 2421/6000, G loss: 0.5106035470962524, D loss: 0.004173404537141323\n",
            "Iteration 2422/6000, G loss: 0.5376984477043152, D loss: 0.004159540869295597\n",
            "Iteration 2423/6000, G loss: 0.4592534303665161, D loss: 0.014095716178417206\n",
            "Iteration 2424/6000, G loss: 0.5857665538787842, D loss: 0.008291443809866905\n",
            "Iteration 2425/6000, G loss: 0.5118482112884521, D loss: 0.3225105106830597\n",
            "Iteration 2426/6000, G loss: 0.5095824003219604, D loss: 0.0008658641017973423\n",
            "Iteration 2427/6000, G loss: 0.37145912647247314, D loss: 0.018303263932466507\n",
            "Iteration 2428/6000, G loss: 0.5303711295127869, D loss: 0.006572577636688948\n",
            "Iteration 2429/6000, G loss: 0.5064560770988464, D loss: 0.022940970957279205\n",
            "Iteration 2430/6000, G loss: 0.539745569229126, D loss: 0.012966138310730457\n",
            "Iteration 2431/6000, G loss: 0.5090174674987793, D loss: 0.03788866475224495\n",
            "Iteration 2432/6000, G loss: 0.5098495483398438, D loss: 0.008484210819005966\n",
            "Iteration 2433/6000, G loss: 0.5722165703773499, D loss: 0.0010042646899819374\n",
            "Iteration 2434/6000, G loss: 0.5904988050460815, D loss: 0.0017377586336806417\n",
            "Iteration 2435/6000, G loss: 0.5904178023338318, D loss: 0.001150734955444932\n",
            "Iteration 2436/6000, G loss: 0.5768378376960754, D loss: 0.01006408967077732\n",
            "Iteration 2437/6000, G loss: 0.5605449676513672, D loss: 0.023579712957143784\n",
            "Iteration 2438/6000, G loss: 0.6107663512229919, D loss: 0.001762182218953967\n",
            "Iteration 2439/6000, G loss: 0.6251692175865173, D loss: 0.00303144589997828\n",
            "Iteration 2440/6000, G loss: 0.5668976306915283, D loss: 0.004094697535037994\n",
            "Iteration 2441/6000, G loss: 0.6199317574501038, D loss: 0.00015914793766569346\n",
            "Iteration 2442/6000, G loss: 0.5067517757415771, D loss: 0.0021922034211456776\n",
            "Iteration 2443/6000, G loss: 0.6314041614532471, D loss: 0.0041690729558467865\n",
            "Iteration 2444/6000, G loss: 0.5447031259536743, D loss: 0.0036032088100910187\n",
            "Iteration 2445/6000, G loss: 0.5473917126655579, D loss: 8.303811046062037e-05\n",
            "Iteration 2446/6000, G loss: 0.556412935256958, D loss: 0.0005466345464810729\n",
            "Iteration 2447/6000, G loss: 0.4545772969722748, D loss: 0.030202161520719528\n",
            "Iteration 2448/6000, G loss: 0.5892353057861328, D loss: 0.00029059022199362516\n",
            "Iteration 2449/6000, G loss: 0.404272198677063, D loss: 0.05369178205728531\n",
            "Iteration 2450/6000, G loss: 0.4572569727897644, D loss: 0.018380405381321907\n",
            "Iteration 2451/6000, G loss: 0.5859264731407166, D loss: 0.017416981980204582\n",
            "Iteration 2452/6000, G loss: 0.5776407718658447, D loss: 0.00023813404550310224\n",
            "Iteration 2453/6000, G loss: 0.5550411343574524, D loss: 0.08871439099311829\n",
            "Iteration 2454/6000, G loss: 0.5981503129005432, D loss: 0.004921885207295418\n",
            "Iteration 2455/6000, G loss: 0.5268306136131287, D loss: 0.00510523421689868\n",
            "Iteration 2456/6000, G loss: 0.6248891353607178, D loss: 0.0022106245160102844\n",
            "Iteration 2457/6000, G loss: 0.7838103771209717, D loss: 0.04350672662258148\n",
            "Iteration 2458/6000, G loss: 0.5857093334197998, D loss: 0.013110794126987457\n",
            "Iteration 2459/6000, G loss: 0.3885438144207001, D loss: 1.5061044692993164\n",
            "Iteration 2460/6000, G loss: 0.4361748993396759, D loss: 0.0205962173640728\n",
            "Iteration 2461/6000, G loss: 0.4169755280017853, D loss: 0.03315674886107445\n",
            "Iteration 2462/6000, G loss: 0.5495110154151917, D loss: 0.02067587897181511\n",
            "Iteration 2463/6000, G loss: 0.5442347526550293, D loss: 0.15803982317447662\n",
            "Iteration 2464/6000, G loss: 0.6151215434074402, D loss: 0.01101903896778822\n",
            "Iteration 2465/6000, G loss: 0.6285224556922913, D loss: 0.002436013426631689\n",
            "Iteration 2466/6000, G loss: 0.6279425621032715, D loss: 0.0013273467775434256\n",
            "Iteration 2467/6000, G loss: 0.5049965977668762, D loss: 0.006280627101659775\n",
            "Iteration 2468/6000, G loss: 0.4635413587093353, D loss: 0.018380828201770782\n",
            "Iteration 2469/6000, G loss: 0.43636471033096313, D loss: 0.3009701371192932\n",
            "Iteration 2470/6000, G loss: 0.6203618049621582, D loss: 0.0008187281200662255\n",
            "Iteration 2471/6000, G loss: 0.5930458307266235, D loss: 0.001463772845454514\n",
            "Iteration 2472/6000, G loss: 0.5673078894615173, D loss: 0.00011465007264632732\n",
            "Iteration 2473/6000, G loss: 0.6834845542907715, D loss: 0.0001699031563475728\n",
            "Iteration 2474/6000, G loss: 0.5538026690483093, D loss: 0.00012341057299636304\n",
            "Iteration 2475/6000, G loss: 0.41051796078681946, D loss: 0.0009349908214062452\n",
            "Iteration 2476/6000, G loss: 0.4831532835960388, D loss: 0.030541040003299713\n",
            "Iteration 2477/6000, G loss: 0.5489115118980408, D loss: 6.291522004175931e-05\n",
            "Iteration 2478/6000, G loss: 0.5496631264686584, D loss: 0.00011810411524493247\n",
            "Iteration 2479/6000, G loss: 0.42404234409332275, D loss: 0.009535451419651508\n",
            "Iteration 2480/6000, G loss: 0.4252970218658447, D loss: 0.006335904821753502\n",
            "Iteration 2481/6000, G loss: 0.5845571160316467, D loss: 0.00014264493074733764\n",
            "Iteration 2482/6000, G loss: 0.6198835372924805, D loss: 0.047719404101371765\n",
            "Iteration 2483/6000, G loss: 0.5190140008926392, D loss: 0.0006700854864902794\n",
            "Iteration 2484/6000, G loss: 0.6502076387405396, D loss: 0.0008234470151364803\n",
            "Iteration 2485/6000, G loss: 0.5509157776832581, D loss: 4.21095646743197e-05\n",
            "Iteration 2486/6000, G loss: 0.5068302154541016, D loss: 0.00017342274077236652\n",
            "Iteration 2487/6000, G loss: 0.5391323566436768, D loss: 6.049845069355797e-06\n",
            "Iteration 2488/6000, G loss: 0.5426343083381653, D loss: 0.0008219655137509108\n",
            "Iteration 2489/6000, G loss: 0.42426085472106934, D loss: 0.001447972608730197\n",
            "Iteration 2490/6000, G loss: 0.5217384099960327, D loss: 0.00033786616404540837\n",
            "Iteration 2491/6000, G loss: 0.4416057765483856, D loss: 0.0004215385124552995\n",
            "Iteration 2492/6000, G loss: 0.5217604637145996, D loss: 1.1801672371802852e-05\n",
            "Iteration 2493/6000, G loss: 0.6472945213317871, D loss: 3.626089892350137e-05\n",
            "Iteration 2494/6000, G loss: 0.588351309299469, D loss: 1.1104290024377406e-05\n",
            "Iteration 2495/6000, G loss: 0.46402451395988464, D loss: 6.100404425524175e-05\n",
            "Iteration 2496/6000, G loss: 0.607570230960846, D loss: 9.466061601415277e-05\n",
            "Iteration 2497/6000, G loss: 0.5791460275650024, D loss: 0.00010283759911544621\n",
            "Iteration 2498/6000, G loss: 0.45098960399627686, D loss: 0.006043686531484127\n",
            "Iteration 2499/6000, G loss: 0.4538687765598297, D loss: 0.02332775667309761\n",
            "Iteration 2500/6000, G loss: 0.6169990301132202, D loss: 1.0067164112115279e-05\n",
            "Iteration 2501/6000, G loss: 0.6551936864852905, D loss: 0.00028895624564029276\n",
            "Iteration 2502/6000, G loss: 0.7411775588989258, D loss: 0.0037873894907534122\n",
            "Iteration 2503/6000, G loss: 0.49035072326660156, D loss: 0.008250455372035503\n",
            "Iteration 2504/6000, G loss: 0.5907425284385681, D loss: 0.026044026017189026\n",
            "Iteration 2505/6000, G loss: 0.5085992217063904, D loss: 0.00013898540055379272\n",
            "Iteration 2506/6000, G loss: 0.6094034910202026, D loss: 9.316987416241318e-05\n",
            "Iteration 2507/6000, G loss: 0.6175838708877563, D loss: 2.6529578462941572e-05\n",
            "Iteration 2508/6000, G loss: 0.5269101858139038, D loss: 1.4787678082939237e-05\n",
            "Iteration 2509/6000, G loss: 0.5601502060890198, D loss: 0.001003614510409534\n",
            "Iteration 2510/6000, G loss: 0.581156313419342, D loss: 0.0002629747614264488\n",
            "Iteration 2511/6000, G loss: 0.4742993712425232, D loss: 0.0004633772186934948\n",
            "Iteration 2512/6000, G loss: 0.6940839886665344, D loss: 0.007641948293894529\n",
            "Iteration 2513/6000, G loss: 0.566892683506012, D loss: 0.00026148324832320213\n",
            "Iteration 2514/6000, G loss: 0.40080469846725464, D loss: 0.0012804295402020216\n",
            "Iteration 2515/6000, G loss: 0.6689150333404541, D loss: 0.0017357377801090479\n",
            "Iteration 2516/6000, G loss: 0.47331738471984863, D loss: 0.20543038845062256\n",
            "Iteration 2517/6000, G loss: 0.5873345732688904, D loss: 0.0004965776461176574\n",
            "Iteration 2518/6000, G loss: 0.4549427926540375, D loss: 0.17146679759025574\n",
            "Iteration 2519/6000, G loss: 0.44969794154167175, D loss: 0.04042948782444\n",
            "Iteration 2520/6000, G loss: 0.6938306093215942, D loss: 0.18520578742027283\n",
            "Iteration 2521/6000, G loss: 0.6906384229660034, D loss: 2.44254469871521\n",
            "Iteration 2522/6000, G loss: 0.543115496635437, D loss: 0.0012141213519498706\n",
            "Iteration 2523/6000, G loss: 0.5572744011878967, D loss: 0.2662810683250427\n",
            "Iteration 2524/6000, G loss: 0.5048185586929321, D loss: 0.00930999219417572\n",
            "Iteration 2525/6000, G loss: 0.6185414791107178, D loss: 0.01747778430581093\n",
            "Iteration 2526/6000, G loss: 0.5382404923439026, D loss: 0.0030558134894818068\n",
            "Iteration 2527/6000, G loss: 0.45988166332244873, D loss: 0.024073738604784012\n",
            "Iteration 2528/6000, G loss: 0.49095767736434937, D loss: 0.027417510747909546\n",
            "Iteration 2529/6000, G loss: 0.6361122727394104, D loss: 0.038991205394268036\n",
            "Iteration 2530/6000, G loss: 0.4806883633136749, D loss: 0.08359405398368835\n",
            "Iteration 2531/6000, G loss: 0.6563974022865295, D loss: 0.03398924693465233\n",
            "Iteration 2532/6000, G loss: 0.4889799952507019, D loss: 0.002503820229321718\n",
            "Iteration 2533/6000, G loss: 0.557918906211853, D loss: 0.0014504629652947187\n",
            "Iteration 2534/6000, G loss: 0.7278006672859192, D loss: 0.002396817784756422\n",
            "Iteration 2535/6000, G loss: 0.48879289627075195, D loss: 0.014298729598522186\n",
            "Iteration 2536/6000, G loss: 0.5610818266868591, D loss: 0.0011225822381675243\n",
            "Iteration 2537/6000, G loss: 0.5939158797264099, D loss: 4.326602356741205e-05\n",
            "Iteration 2538/6000, G loss: 0.3918975293636322, D loss: 0.008824062533676624\n",
            "Iteration 2539/6000, G loss: 0.5407686829566956, D loss: 5.8691250160336494e-05\n",
            "Iteration 2540/6000, G loss: 0.5667972564697266, D loss: 0.0013065156526863575\n",
            "Iteration 2541/6000, G loss: 0.5376321077346802, D loss: 8.601290755905211e-05\n",
            "Iteration 2542/6000, G loss: 0.5240554809570312, D loss: 0.0008007592405192554\n",
            "Iteration 2543/6000, G loss: 0.45981287956237793, D loss: 0.0018832990899682045\n",
            "Iteration 2544/6000, G loss: 0.5293670296669006, D loss: 3.1363419111585245e-05\n",
            "Iteration 2545/6000, G loss: 0.48346370458602905, D loss: 0.002241870854049921\n",
            "Iteration 2546/6000, G loss: 0.4825862646102905, D loss: 0.0008720805053599179\n",
            "Iteration 2547/6000, G loss: 0.594075083732605, D loss: 0.00027737737400457263\n",
            "Iteration 2548/6000, G loss: 0.631552517414093, D loss: 0.00012659937783610076\n",
            "Iteration 2549/6000, G loss: 0.49511581659317017, D loss: 6.722613761667162e-05\n",
            "Iteration 2550/6000, G loss: 0.6262973546981812, D loss: 3.90044369851239e-05\n",
            "Iteration 2551/6000, G loss: 0.558258056640625, D loss: 0.0001242793950950727\n",
            "Iteration 2552/6000, G loss: 0.5310475826263428, D loss: 0.00017790345009416342\n",
            "Iteration 2553/6000, G loss: 0.46463239192962646, D loss: 0.00018664865638129413\n",
            "Iteration 2554/6000, G loss: 0.5089258551597595, D loss: 0.005691017955541611\n",
            "Iteration 2555/6000, G loss: 0.5219321846961975, D loss: 0.00012764238636009395\n",
            "Iteration 2556/6000, G loss: 0.5095885992050171, D loss: 9.806311572901905e-05\n",
            "Iteration 2557/6000, G loss: 0.45435482263565063, D loss: 0.0015180385671555996\n",
            "Iteration 2558/6000, G loss: 0.48270392417907715, D loss: 0.000573036028072238\n",
            "Iteration 2559/6000, G loss: 0.5872402191162109, D loss: 0.0004521997761912644\n",
            "Iteration 2560/6000, G loss: 0.48291152715682983, D loss: 0.0001230369380209595\n",
            "Iteration 2561/6000, G loss: 0.5272676348686218, D loss: 5.797023914055899e-05\n",
            "Iteration 2562/6000, G loss: 0.5366876721382141, D loss: 0.0002652412513270974\n",
            "Iteration 2563/6000, G loss: 0.544248640537262, D loss: 4.195476503809914e-05\n",
            "Iteration 2564/6000, G loss: 0.5906472206115723, D loss: 2.3156124370871112e-05\n",
            "Iteration 2565/6000, G loss: 0.6003537774085999, D loss: 4.652627831092104e-05\n",
            "Iteration 2566/6000, G loss: 0.5824249982833862, D loss: 0.0007557501667179167\n",
            "Iteration 2567/6000, G loss: 0.5628880262374878, D loss: 0.00013181711256038398\n",
            "Iteration 2568/6000, G loss: 0.4780997037887573, D loss: 2.9849605198251083e-05\n",
            "Iteration 2569/6000, G loss: 0.5685986280441284, D loss: 0.0003054295084439218\n",
            "Iteration 2570/6000, G loss: 0.5288231372833252, D loss: 0.0055402726866304874\n",
            "Iteration 2571/6000, G loss: 0.5223316550254822, D loss: 4.2074105294886976e-05\n",
            "Iteration 2572/6000, G loss: 0.5130637288093567, D loss: 0.0005765511305071414\n",
            "Iteration 2573/6000, G loss: 0.5755099058151245, D loss: 5.7585326430853456e-05\n",
            "Iteration 2574/6000, G loss: 0.4938718378543854, D loss: 6.092026160331443e-05\n",
            "Iteration 2575/6000, G loss: 0.5093361139297485, D loss: 0.0001992632751353085\n",
            "Iteration 2576/6000, G loss: 0.528306782245636, D loss: 0.00018001560238189995\n",
            "Iteration 2577/6000, G loss: 0.49712055921554565, D loss: 2.181511626986321e-05\n",
            "Iteration 2578/6000, G loss: 0.5390368700027466, D loss: 8.29097189125605e-06\n",
            "Iteration 2579/6000, G loss: 0.4904506504535675, D loss: 0.0006168573163449764\n",
            "Iteration 2580/6000, G loss: 0.5780974626541138, D loss: 0.0002526380121707916\n",
            "Iteration 2581/6000, G loss: 0.4932689368724823, D loss: 0.0009886487387120724\n",
            "Iteration 2582/6000, G loss: 0.42474114894866943, D loss: 0.0035462439991533756\n",
            "Iteration 2583/6000, G loss: 0.5476223230361938, D loss: 4.2436757212271914e-05\n",
            "Iteration 2584/6000, G loss: 0.5118845105171204, D loss: 2.634521479194518e-06\n",
            "Iteration 2585/6000, G loss: 0.5643277168273926, D loss: 1.976476050913334e-05\n",
            "Iteration 2586/6000, G loss: 0.6043413281440735, D loss: 0.00012717153003904969\n",
            "Iteration 2587/6000, G loss: 0.5614988803863525, D loss: 8.146361506078392e-05\n",
            "Iteration 2588/6000, G loss: 0.4745459258556366, D loss: 0.00010553781612543389\n",
            "Iteration 2589/6000, G loss: 0.558765172958374, D loss: 2.020595729845809e-06\n",
            "Iteration 2590/6000, G loss: 0.5199933052062988, D loss: 0.0007553368341177702\n",
            "Iteration 2591/6000, G loss: 0.5252671837806702, D loss: 4.7086767153814435e-05\n",
            "Iteration 2592/6000, G loss: 0.5629730820655823, D loss: 0.00035349634708836675\n",
            "Iteration 2593/6000, G loss: 0.6189914345741272, D loss: 5.793994205305353e-05\n",
            "Iteration 2594/6000, G loss: 0.57707679271698, D loss: 0.00022587949933949858\n",
            "Iteration 2595/6000, G loss: 0.5453801155090332, D loss: 0.00021685140382032841\n",
            "Iteration 2596/6000, G loss: 0.5796328783035278, D loss: 5.832777242176235e-05\n",
            "Iteration 2597/6000, G loss: 0.5425495505332947, D loss: 0.00019022799097001553\n",
            "Iteration 2598/6000, G loss: 0.6677243709564209, D loss: 0.0025775539688766003\n",
            "Iteration 2599/6000, G loss: 0.5802668333053589, D loss: 7.979014480952173e-05\n",
            "Iteration 2600/6000, G loss: 0.5490297675132751, D loss: 1.60812141984934e-05\n",
            "Iteration 2601/6000, G loss: 0.5202460885047913, D loss: 0.00033589533995836973\n",
            "Iteration 2602/6000, G loss: 0.5021541714668274, D loss: 0.00020980113185942173\n",
            "Iteration 2603/6000, G loss: 0.5098547339439392, D loss: 7.775080302963033e-05\n",
            "Iteration 2604/6000, G loss: 0.6484847664833069, D loss: 7.4326753747300245e-06\n",
            "Iteration 2605/6000, G loss: 0.4106239676475525, D loss: 0.00045705639058724046\n",
            "Iteration 2606/6000, G loss: 0.6041294932365417, D loss: 0.00020847610721830279\n",
            "Iteration 2607/6000, G loss: 0.5748142600059509, D loss: 0.00010467875108588487\n",
            "Iteration 2608/6000, G loss: 0.5550758838653564, D loss: 0.00027480360586196184\n",
            "Iteration 2609/6000, G loss: 0.5164356827735901, D loss: 1.586064900038764e-05\n",
            "Iteration 2610/6000, G loss: 0.5232851505279541, D loss: 5.346526450011879e-06\n",
            "Iteration 2611/6000, G loss: 0.49685826897621155, D loss: 0.00020476602367125452\n",
            "Iteration 2612/6000, G loss: 0.46553346514701843, D loss: 0.0013344984035938978\n",
            "Iteration 2613/6000, G loss: 0.5023836493492126, D loss: 0.0006247934652492404\n",
            "Iteration 2614/6000, G loss: 0.5248083472251892, D loss: 0.0001392580452375114\n",
            "Iteration 2615/6000, G loss: 0.5058606863021851, D loss: 1.4185837244440336e-05\n",
            "Iteration 2616/6000, G loss: 0.5556283593177795, D loss: 0.00014472399197984487\n",
            "Iteration 2617/6000, G loss: 0.5806573629379272, D loss: 0.0005652826512232423\n",
            "Iteration 2618/6000, G loss: 0.5214496850967407, D loss: 6.711848254781216e-05\n",
            "Iteration 2619/6000, G loss: 0.48456743359565735, D loss: 0.0026473253965377808\n",
            "Iteration 2620/6000, G loss: 0.47982969880104065, D loss: 0.010579336434602737\n",
            "Iteration 2621/6000, G loss: 0.6263635158538818, D loss: 3.95231727452483e-05\n",
            "Iteration 2622/6000, G loss: 0.5144805908203125, D loss: 3.010030695804744e-06\n",
            "Iteration 2623/6000, G loss: 0.6018103361129761, D loss: 1.9383212929824367e-05\n",
            "Iteration 2624/6000, G loss: 0.5591131448745728, D loss: 3.5344673960935324e-05\n",
            "Iteration 2625/6000, G loss: 0.4689371585845947, D loss: 0.0012871199287474155\n",
            "Iteration 2626/6000, G loss: 0.571370005607605, D loss: 5.54370490135625e-05\n",
            "Iteration 2627/6000, G loss: 0.5530027747154236, D loss: 3.560725599527359e-05\n",
            "Iteration 2628/6000, G loss: 0.508332371711731, D loss: 0.00015183424693532288\n",
            "Iteration 2629/6000, G loss: 0.7105944156646729, D loss: 4.153591726208106e-05\n",
            "Iteration 2630/6000, G loss: 0.5116457343101501, D loss: 0.00022187471040524542\n",
            "Iteration 2631/6000, G loss: 0.5034300088882446, D loss: 0.0006044107722118497\n",
            "Iteration 2632/6000, G loss: 0.6371427178382874, D loss: 5.093713116366416e-05\n",
            "Iteration 2633/6000, G loss: 0.5067368745803833, D loss: 5.0215647206641734e-05\n",
            "Iteration 2634/6000, G loss: 0.6364030241966248, D loss: 3.4313892683712766e-05\n",
            "Iteration 2635/6000, G loss: 0.6356444954872131, D loss: 1.347058241663035e-05\n",
            "Iteration 2636/6000, G loss: 0.6361880302429199, D loss: 3.8146918086567894e-06\n",
            "Iteration 2637/6000, G loss: 0.5927522778511047, D loss: 8.577078915550373e-06\n",
            "Iteration 2638/6000, G loss: 0.48738282918930054, D loss: 0.00020330700499471277\n",
            "Iteration 2639/6000, G loss: 0.43835699558258057, D loss: 0.003033286426216364\n",
            "Iteration 2640/6000, G loss: 0.4554644227027893, D loss: 2.1391937480075285e-05\n",
            "Iteration 2641/6000, G loss: 0.4531305730342865, D loss: 0.001095062354579568\n",
            "Iteration 2642/6000, G loss: 0.48321935534477234, D loss: 0.0009539653547108173\n",
            "Iteration 2643/6000, G loss: 0.6385034322738647, D loss: 0.00012187923130113631\n",
            "Iteration 2644/6000, G loss: 0.5953712463378906, D loss: 0.003004507627338171\n",
            "Iteration 2645/6000, G loss: 0.5783163905143738, D loss: 9.713243343867362e-05\n",
            "Iteration 2646/6000, G loss: 0.5000014901161194, D loss: 0.0012173947179690003\n",
            "Iteration 2647/6000, G loss: 0.5609049797058105, D loss: 0.0003805131418630481\n",
            "Iteration 2648/6000, G loss: 0.5550611019134521, D loss: 4.464891389943659e-05\n",
            "Iteration 2649/6000, G loss: 0.45894524455070496, D loss: 0.00032358005410060287\n",
            "Iteration 2650/6000, G loss: 0.5543116927146912, D loss: 0.00348567683249712\n",
            "Iteration 2651/6000, G loss: 0.598812460899353, D loss: 6.07967194810044e-07\n",
            "Iteration 2652/6000, G loss: 0.515869677066803, D loss: 0.0005141099100001156\n",
            "Iteration 2653/6000, G loss: 0.6028710007667542, D loss: 0.0002560098946560174\n",
            "Iteration 2654/6000, G loss: 0.4153057038784027, D loss: 0.0010852146660909057\n",
            "Iteration 2655/6000, G loss: 0.6518369913101196, D loss: 0.10930787026882172\n",
            "Iteration 2656/6000, G loss: 0.5993094444274902, D loss: 5.620018782792613e-05\n",
            "Iteration 2657/6000, G loss: 0.6094043850898743, D loss: 3.200737410224974e-05\n",
            "Iteration 2658/6000, G loss: 0.43128135800361633, D loss: 1.1855313459818717e-05\n",
            "Iteration 2659/6000, G loss: 0.5195364952087402, D loss: 0.0010442228522151709\n",
            "Iteration 2660/6000, G loss: 0.5498809218406677, D loss: 0.0013691228814423084\n",
            "Iteration 2661/6000, G loss: 0.5519315004348755, D loss: 0.00042762624798342586\n",
            "Iteration 2662/6000, G loss: 0.5644267797470093, D loss: 0.0003249260480515659\n",
            "Iteration 2663/6000, G loss: 0.5516073107719421, D loss: 2.1058182028355077e-05\n",
            "Iteration 2664/6000, G loss: 0.6278715133666992, D loss: 0.0001178119273390621\n",
            "Iteration 2665/6000, G loss: 0.6887799501419067, D loss: 6.303594273049384e-05\n",
            "Iteration 2666/6000, G loss: 0.47288164496421814, D loss: 4.802868716069497e-05\n",
            "Iteration 2667/6000, G loss: 0.5016921162605286, D loss: 0.0004577227809932083\n",
            "Iteration 2668/6000, G loss: 0.49873989820480347, D loss: 3.3926589821930975e-05\n",
            "Iteration 2669/6000, G loss: 0.47075894474983215, D loss: 0.0003870112122967839\n",
            "Iteration 2670/6000, G loss: 0.5241774320602417, D loss: 0.0033087595365941525\n",
            "Iteration 2671/6000, G loss: 0.6326423287391663, D loss: 3.35573531629052e-06\n",
            "Iteration 2672/6000, G loss: 0.5846006274223328, D loss: 5.1092039939248934e-05\n",
            "Iteration 2673/6000, G loss: 0.5441479682922363, D loss: 0.00048543396405875683\n",
            "Iteration 2674/6000, G loss: 0.5926750302314758, D loss: 0.0006874593673273921\n",
            "Iteration 2675/6000, G loss: 0.5526109337806702, D loss: 0.0005604535108432174\n",
            "Iteration 2676/6000, G loss: 0.5553089380264282, D loss: 0.003922725096344948\n",
            "Iteration 2677/6000, G loss: 0.5482710003852844, D loss: 7.19291710993275e-05\n",
            "Iteration 2678/6000, G loss: 0.5354107022285461, D loss: 9.548630259814672e-06\n",
            "Iteration 2679/6000, G loss: 0.4193331301212311, D loss: 0.004183567129075527\n",
            "Iteration 2680/6000, G loss: 0.52146315574646, D loss: 1.1849357179016806e-05\n",
            "Iteration 2681/6000, G loss: 0.5587778091430664, D loss: 1.84653872565832e-05\n",
            "Iteration 2682/6000, G loss: 0.5269976854324341, D loss: 0.00031748609035275877\n",
            "Iteration 2683/6000, G loss: 0.6004895567893982, D loss: 0.0005221726605668664\n",
            "Iteration 2684/6000, G loss: 0.5581732988357544, D loss: 0.0018375581130385399\n",
            "Iteration 2685/6000, G loss: 0.5386878252029419, D loss: 0.007869120687246323\n",
            "Iteration 2686/6000, G loss: 0.5367003083229065, D loss: 3.617996753746411e-06\n",
            "Iteration 2687/6000, G loss: 0.5733087658882141, D loss: 2.521274609534885e-06\n",
            "Iteration 2688/6000, G loss: 0.6027308702468872, D loss: 1.3589794434665237e-05\n",
            "Iteration 2689/6000, G loss: 0.49042055010795593, D loss: 0.005602598190307617\n",
            "Iteration 2690/6000, G loss: 0.6510361433029175, D loss: 3.0875166885380168e-06\n",
            "Iteration 2691/6000, G loss: 0.5599773526191711, D loss: 4.9333597416989505e-05\n",
            "Iteration 2692/6000, G loss: 0.5852674245834351, D loss: 3.5196128010284156e-05\n",
            "Iteration 2693/6000, G loss: 0.6133630871772766, D loss: 0.00012236459588166326\n",
            "Iteration 2694/6000, G loss: 0.5636720061302185, D loss: 6.31259536021389e-05\n",
            "Iteration 2695/6000, G loss: 0.5634483098983765, D loss: 1.2433472875272855e-05\n",
            "Iteration 2696/6000, G loss: 0.4938790798187256, D loss: 7.409242971334606e-05\n",
            "Iteration 2697/6000, G loss: 0.5749310255050659, D loss: 0.00029530993197113276\n",
            "Iteration 2698/6000, G loss: 0.4094412326812744, D loss: 0.008107396773993969\n",
            "Iteration 2699/6000, G loss: 0.5171183943748474, D loss: 0.0003504569176584482\n",
            "Iteration 2700/6000, G loss: 0.5212011933326721, D loss: 1.2129483366152272e-05\n",
            "Iteration 2701/6000, G loss: 0.41785091161727905, D loss: 0.0013301477301865816\n",
            "Iteration 2702/6000, G loss: 0.4566303491592407, D loss: 0.0002080278645735234\n",
            "Iteration 2703/6000, G loss: 0.5492516756057739, D loss: 0.0002944481966551393\n",
            "Iteration 2704/6000, G loss: 0.6284816265106201, D loss: 9.07178400666453e-06\n",
            "Iteration 2705/6000, G loss: 0.5021147727966309, D loss: 6.949717499082908e-05\n",
            "Iteration 2706/6000, G loss: 0.5299522876739502, D loss: 0.00017164504970423877\n",
            "Iteration 2707/6000, G loss: 0.5886121988296509, D loss: 0.0005094605148769915\n",
            "Iteration 2708/6000, G loss: 0.596670925617218, D loss: 1.9305785826873034e-05\n",
            "Iteration 2709/6000, G loss: 0.4767966866493225, D loss: 0.00015872553922235966\n",
            "Iteration 2710/6000, G loss: 0.4555026888847351, D loss: 0.00012602291826624423\n",
            "Iteration 2711/6000, G loss: 0.5521464347839355, D loss: 2.123100966855418e-05\n",
            "Iteration 2712/6000, G loss: 0.5380023717880249, D loss: 2.5737066607689485e-05\n",
            "Iteration 2713/6000, G loss: 0.4903276562690735, D loss: 7.00873788446188e-05\n",
            "Iteration 2714/6000, G loss: 0.5481489896774292, D loss: 0.00015420923591591418\n",
            "Iteration 2715/6000, G loss: 0.5588999390602112, D loss: 7.259078120114282e-05\n",
            "Iteration 2716/6000, G loss: 0.6324033141136169, D loss: 0.0393897145986557\n",
            "Iteration 2717/6000, G loss: 0.5300325751304626, D loss: 0.00035946822026744485\n",
            "Iteration 2718/6000, G loss: 0.541998565196991, D loss: 0.000490353733766824\n",
            "Iteration 2719/6000, G loss: 0.5473158955574036, D loss: 4.762403477798216e-06\n",
            "Iteration 2720/6000, G loss: 0.451593279838562, D loss: 0.0010407690424472094\n",
            "Iteration 2721/6000, G loss: 0.38596054911613464, D loss: 0.00277371471747756\n",
            "Iteration 2722/6000, G loss: 0.6172328591346741, D loss: 1.3649457741848892e-06\n",
            "Iteration 2723/6000, G loss: 0.5485323071479797, D loss: 0.000641526363324374\n",
            "Iteration 2724/6000, G loss: 0.46394413709640503, D loss: 0.0025671320036053658\n",
            "Iteration 2725/6000, G loss: 0.5738919973373413, D loss: 0.0005602941382676363\n",
            "Iteration 2726/6000, G loss: 0.5822616219520569, D loss: 7.057713810354471e-05\n",
            "Iteration 2727/6000, G loss: 0.5042601227760315, D loss: 0.0009289580630138516\n",
            "Iteration 2728/6000, G loss: 0.45337894558906555, D loss: 0.0009564864449203014\n",
            "Iteration 2729/6000, G loss: 0.5400198101997375, D loss: 0.00013433618005365133\n",
            "Iteration 2730/6000, G loss: 0.5062459707260132, D loss: 1.8358108718530275e-05\n",
            "Iteration 2731/6000, G loss: 0.698104739189148, D loss: 0.00010220160766039044\n",
            "Iteration 2732/6000, G loss: 0.6273882985115051, D loss: 6.721906538587064e-05\n",
            "Iteration 2733/6000, G loss: 0.43086713552474976, D loss: 0.03753542900085449\n",
            "Iteration 2734/6000, G loss: 0.5328513383865356, D loss: 0.00012786375009454787\n",
            "Iteration 2735/6000, G loss: 0.5298424959182739, D loss: 0.00076607137452811\n",
            "Iteration 2736/6000, G loss: 0.4725438356399536, D loss: 0.0027789552696049213\n",
            "Iteration 2737/6000, G loss: 0.4625043570995331, D loss: 0.0003172008728142828\n",
            "Iteration 2738/6000, G loss: 0.48100486397743225, D loss: 1.2862623407272622e-05\n",
            "Iteration 2739/6000, G loss: 0.6342736482620239, D loss: 0.00016296067042276263\n",
            "Iteration 2740/6000, G loss: 0.5483580231666565, D loss: 9.655514440964907e-05\n",
            "Iteration 2741/6000, G loss: 0.4350397288799286, D loss: 0.0020945891737937927\n",
            "Iteration 2742/6000, G loss: 0.5441867709159851, D loss: 0.000908948015421629\n",
            "Iteration 2743/6000, G loss: 0.46160295605659485, D loss: 0.007403102703392506\n",
            "Iteration 2744/6000, G loss: 0.6484158635139465, D loss: 0.0014748340472579002\n",
            "Iteration 2745/6000, G loss: 0.6904919743537903, D loss: 2.158849747502245e-05\n",
            "Iteration 2746/6000, G loss: 0.6480481624603271, D loss: 0.0001295486290473491\n",
            "Iteration 2747/6000, G loss: 0.5971165895462036, D loss: 0.0061901528388261795\n",
            "Iteration 2748/6000, G loss: 0.547150731086731, D loss: 0.136219784617424\n",
            "Iteration 2749/6000, G loss: 0.5330163240432739, D loss: 0.0002527465985622257\n",
            "Iteration 2750/6000, G loss: 0.4791903495788574, D loss: 0.0660356730222702\n",
            "Iteration 2751/6000, G loss: 0.5503020882606506, D loss: 0.00025514024309813976\n",
            "Iteration 2752/6000, G loss: 0.5697997212409973, D loss: 3.5058954381383955e-05\n",
            "Iteration 2753/6000, G loss: 0.5259034633636475, D loss: 2.641472816467285\n",
            "Iteration 2754/6000, G loss: 0.5719022154808044, D loss: 0.00037121627246961\n",
            "Iteration 2755/6000, G loss: 0.5782963633537292, D loss: 0.00017797329928725958\n",
            "Iteration 2756/6000, G loss: 0.5322392582893372, D loss: 0.059217751026153564\n",
            "Iteration 2757/6000, G loss: 0.5270019769668579, D loss: 0.004398157820105553\n",
            "Iteration 2758/6000, G loss: 0.6303046941757202, D loss: 0.03114752471446991\n",
            "Iteration 2759/6000, G loss: 0.49149253964424133, D loss: 0.0012833436485379934\n",
            "Iteration 2760/6000, G loss: 0.4567384719848633, D loss: 0.014867926016449928\n",
            "Iteration 2761/6000, G loss: 0.618212878704071, D loss: 0.006970273330807686\n",
            "Iteration 2762/6000, G loss: 0.48154762387275696, D loss: 0.018567010760307312\n",
            "Iteration 2763/6000, G loss: 0.6390091776847839, D loss: 0.004412116948515177\n",
            "Iteration 2764/6000, G loss: 0.46540647745132446, D loss: 0.00031302604475058615\n",
            "Iteration 2765/6000, G loss: 0.6988511085510254, D loss: 0.0012067307252436876\n",
            "Iteration 2766/6000, G loss: 0.6345113515853882, D loss: 0.008576130494475365\n",
            "Iteration 2767/6000, G loss: 0.44357070326805115, D loss: 0.02610451728105545\n",
            "Iteration 2768/6000, G loss: 0.5017660856246948, D loss: 0.0007213769713416696\n",
            "Iteration 2769/6000, G loss: 0.5022981762886047, D loss: 0.006647173780947924\n",
            "Iteration 2770/6000, G loss: 0.5471881628036499, D loss: 0.00048195445560850203\n",
            "Iteration 2771/6000, G loss: 0.4920217990875244, D loss: 0.0007785559864714742\n",
            "Iteration 2772/6000, G loss: 0.45258158445358276, D loss: 0.00011298563185846433\n",
            "Iteration 2773/6000, G loss: 0.5189075469970703, D loss: 0.00022386234195437282\n",
            "Iteration 2774/6000, G loss: 0.44056883454322815, D loss: 0.0070646218955516815\n",
            "Iteration 2775/6000, G loss: 0.5240148901939392, D loss: 0.00020339188631623983\n",
            "Iteration 2776/6000, G loss: 0.5151585936546326, D loss: 0.002880550455302\n",
            "Iteration 2777/6000, G loss: 0.6240158677101135, D loss: 0.019622575491666794\n",
            "Iteration 2778/6000, G loss: 0.5509592890739441, D loss: 0.000700354459695518\n",
            "Iteration 2779/6000, G loss: 0.4910805821418762, D loss: 0.0004280191205907613\n",
            "Iteration 2780/6000, G loss: 0.695967972278595, D loss: 0.0038921681698411703\n",
            "Iteration 2781/6000, G loss: 0.5826805233955383, D loss: 0.0012235521571710706\n",
            "Iteration 2782/6000, G loss: 0.5267409086227417, D loss: 0.0010663986904546618\n",
            "Iteration 2783/6000, G loss: 0.5230008959770203, D loss: 0.00462914165109396\n",
            "Iteration 2784/6000, G loss: 0.6543731689453125, D loss: 9.399310511071235e-05\n",
            "Iteration 2785/6000, G loss: 0.4616696238517761, D loss: 0.0016366972122341394\n",
            "Iteration 2786/6000, G loss: 0.617375373840332, D loss: 4.5394212065730244e-05\n",
            "Iteration 2787/6000, G loss: 0.5219694972038269, D loss: 0.0004804433847311884\n",
            "Iteration 2788/6000, G loss: 0.5490136742591858, D loss: 0.006126516964286566\n",
            "Iteration 2789/6000, G loss: 0.5111483931541443, D loss: 0.0006066770874895155\n",
            "Iteration 2790/6000, G loss: 0.548640251159668, D loss: 0.0003033000975847244\n",
            "Iteration 2791/6000, G loss: 0.4858212471008301, D loss: 0.13135741651058197\n",
            "Iteration 2792/6000, G loss: 0.4493711292743683, D loss: 0.00904910359531641\n",
            "Iteration 2793/6000, G loss: 0.6130349636077881, D loss: 2.4378277885261923e-06\n",
            "Iteration 2794/6000, G loss: 0.5593172311782837, D loss: 1.3226179362391122e-05\n",
            "Iteration 2795/6000, G loss: 0.45871779322624207, D loss: 0.00248239329084754\n",
            "Iteration 2796/6000, G loss: 0.5597677826881409, D loss: 7.754935359116644e-05\n",
            "Iteration 2797/6000, G loss: 0.5505735874176025, D loss: 0.004111371003091335\n",
            "Iteration 2798/6000, G loss: 0.46087294816970825, D loss: 7.499719504266977e-05\n",
            "Iteration 2799/6000, G loss: 0.4889850616455078, D loss: 0.00010649723117239773\n",
            "Iteration 2800/6000, G loss: 0.42795032262802124, D loss: 0.00028806610498577356\n",
            "Iteration 2801/6000, G loss: 0.5286520719528198, D loss: 0.0005480325780808926\n",
            "Iteration 2802/6000, G loss: 0.4514687657356262, D loss: 0.0004421817429829389\n",
            "Iteration 2803/6000, G loss: 0.5095224380493164, D loss: 0.0016974338795989752\n",
            "Iteration 2804/6000, G loss: 0.5471735596656799, D loss: 0.002174785826355219\n",
            "Iteration 2805/6000, G loss: 0.4820089340209961, D loss: 0.001782145001925528\n",
            "Iteration 2806/6000, G loss: 0.5789309740066528, D loss: 0.001128269825130701\n",
            "Iteration 2807/6000, G loss: 0.42731645703315735, D loss: 0.004300893284380436\n",
            "Iteration 2808/6000, G loss: 0.5914134979248047, D loss: 7.445592927979305e-05\n",
            "Iteration 2809/6000, G loss: 0.5639246702194214, D loss: 3.681729867821559e-05\n",
            "Iteration 2810/6000, G loss: 0.5179942846298218, D loss: 0.18421851098537445\n",
            "Iteration 2811/6000, G loss: 0.5366248488426208, D loss: 0.0001100680383387953\n",
            "Iteration 2812/6000, G loss: 0.6243674755096436, D loss: 8.839338988764212e-06\n",
            "Iteration 2813/6000, G loss: 0.6569908261299133, D loss: 9.421301365364343e-05\n",
            "Iteration 2814/6000, G loss: 0.4748961329460144, D loss: 0.003523864783346653\n",
            "Iteration 2815/6000, G loss: 0.5615613460540771, D loss: 0.0005480079562403262\n",
            "Iteration 2816/6000, G loss: 0.5553104877471924, D loss: 0.00034575839526951313\n",
            "Iteration 2817/6000, G loss: 0.565292477607727, D loss: 7.855384319555014e-05\n",
            "Iteration 2818/6000, G loss: 0.461288720369339, D loss: 0.004165674559772015\n",
            "Iteration 2819/6000, G loss: 0.5858216285705566, D loss: 0.003784523345530033\n",
            "Iteration 2820/6000, G loss: 0.5067967176437378, D loss: 9.578288882039487e-06\n",
            "Iteration 2821/6000, G loss: 0.6239286661148071, D loss: 0.014000985771417618\n",
            "Iteration 2822/6000, G loss: 0.6928820610046387, D loss: 0.0011612336384132504\n",
            "Iteration 2823/6000, G loss: 0.6184499859809875, D loss: 1.2824375629425049\n",
            "Iteration 2824/6000, G loss: 0.5158284902572632, D loss: 0.2204495072364807\n",
            "Iteration 2825/6000, G loss: 0.6265579462051392, D loss: 0.00019329902715981007\n",
            "Iteration 2826/6000, G loss: 0.48432689905166626, D loss: 0.14760971069335938\n",
            "Iteration 2827/6000, G loss: 0.40170058608055115, D loss: 0.3890848159790039\n",
            "Iteration 2828/6000, G loss: 0.6459451913833618, D loss: 0.1240793764591217\n",
            "Iteration 2829/6000, G loss: 0.5643094778060913, D loss: 3.0346867788466625e-05\n",
            "Iteration 2830/6000, G loss: 0.5989413261413574, D loss: 5.978325134492479e-06\n",
            "Iteration 2831/6000, G loss: 0.5111992359161377, D loss: 0.0001755959092406556\n",
            "Iteration 2832/6000, G loss: 0.49048322439193726, D loss: 2.9068693038425408e-05\n",
            "Iteration 2833/6000, G loss: 0.41021835803985596, D loss: 0.004407858941704035\n",
            "Iteration 2834/6000, G loss: 0.6294334530830383, D loss: 0.03238239511847496\n",
            "Iteration 2835/6000, G loss: 0.4794324040412903, D loss: 0.00679922616109252\n",
            "Iteration 2836/6000, G loss: 0.5666062235832214, D loss: 0.0008398577338084579\n",
            "Iteration 2837/6000, G loss: 0.5524810552597046, D loss: 0.004531359300017357\n",
            "Iteration 2838/6000, G loss: 0.6016062498092651, D loss: 0.0006840215064585209\n",
            "Iteration 2839/6000, G loss: 0.567186176776886, D loss: 0.008392548188567162\n",
            "Iteration 2840/6000, G loss: 0.5959249138832092, D loss: 0.00045911752386018634\n",
            "Iteration 2841/6000, G loss: 0.46794191002845764, D loss: 1.2084226608276367\n",
            "Iteration 2842/6000, G loss: 0.5623387694358826, D loss: 0.002089058980345726\n",
            "Iteration 2843/6000, G loss: 0.460308313369751, D loss: 0.0799996629357338\n",
            "Iteration 2844/6000, G loss: 0.5716286897659302, D loss: 0.04398258402943611\n",
            "Iteration 2845/6000, G loss: 0.5197988748550415, D loss: 7.212146897472849e-07\n",
            "Iteration 2846/6000, G loss: 0.5736163854598999, D loss: 0.0013873125426471233\n",
            "Iteration 2847/6000, G loss: 0.5054966807365417, D loss: 0.0008282101480290294\n",
            "Iteration 2848/6000, G loss: 0.6133281588554382, D loss: 0.14516790211200714\n",
            "Iteration 2849/6000, G loss: 0.6691347360610962, D loss: 8.659825107315555e-05\n",
            "Iteration 2850/6000, G loss: 0.5041741728782654, D loss: 5.888901796424761e-06\n",
            "Iteration 2851/6000, G loss: 0.6902714371681213, D loss: 0.07725510746240616\n",
            "Iteration 2852/6000, G loss: 0.4949694275856018, D loss: 0.0003004730388056487\n",
            "Iteration 2853/6000, G loss: 0.505957305431366, D loss: 0.002006408292800188\n",
            "Iteration 2854/6000, G loss: 0.5465818047523499, D loss: 0.005539075471460819\n",
            "Iteration 2855/6000, G loss: 0.43647345900535583, D loss: 0.1713915765285492\n",
            "Iteration 2856/6000, G loss: 0.5059966444969177, D loss: 2.1766954887425527e-05\n",
            "Iteration 2857/6000, G loss: 0.4326077997684479, D loss: 0.04318097233772278\n",
            "Iteration 2858/6000, G loss: 0.5335216522216797, D loss: 3.993502105004154e-06\n",
            "Iteration 2859/6000, G loss: 0.4870803952217102, D loss: 0.0035379030741751194\n",
            "Iteration 2860/6000, G loss: 0.5575982332229614, D loss: 1.3977214621263556e-05\n",
            "Iteration 2861/6000, G loss: 0.45064517855644226, D loss: 0.024925440549850464\n",
            "Iteration 2862/6000, G loss: 0.5438544154167175, D loss: 3.084503623540513e-05\n",
            "Iteration 2863/6000, G loss: 0.5718079209327698, D loss: 3.230562015232863e-06\n",
            "Iteration 2864/6000, G loss: 0.5899635553359985, D loss: 0.004160531330853701\n",
            "Iteration 2865/6000, G loss: 0.5649814605712891, D loss: 3.343815478729084e-06\n",
            "Iteration 2866/6000, G loss: 0.4720911681652069, D loss: 1.0812234904733486e-05\n",
            "Iteration 2867/6000, G loss: 0.6429847478866577, D loss: 3.6268651456339285e-05\n",
            "Iteration 2868/6000, G loss: 0.4030947983264923, D loss: 0.006193176843225956\n",
            "Iteration 2869/6000, G loss: 0.47516894340515137, D loss: 0.00276676332578063\n",
            "Iteration 2870/6000, G loss: 0.5187714099884033, D loss: 0.00032693601679056883\n",
            "Iteration 2871/6000, G loss: 0.6700595617294312, D loss: 0.00937103945761919\n",
            "Iteration 2872/6000, G loss: 0.5974626541137695, D loss: 0.15510286390781403\n",
            "Iteration 2873/6000, G loss: 0.6677271127700806, D loss: 0.35812604427337646\n",
            "Iteration 2874/6000, G loss: 0.6035988926887512, D loss: 9.477134881308302e-07\n",
            "Iteration 2875/6000, G loss: 0.5547000169754028, D loss: 0.428493857383728\n",
            "Iteration 2876/6000, G loss: 0.5595235824584961, D loss: 0.02219870314002037\n",
            "Iteration 2877/6000, G loss: 0.612281858921051, D loss: 0.10033358633518219\n",
            "Iteration 2878/6000, G loss: 0.5788323879241943, D loss: 0.0029893217142671347\n",
            "Iteration 2879/6000, G loss: 0.5671199560165405, D loss: 5.3130646847421303e-05\n",
            "Iteration 2880/6000, G loss: 0.48184722661972046, D loss: 0.06557111442089081\n",
            "Iteration 2881/6000, G loss: 0.5135420560836792, D loss: 0.001959022367373109\n",
            "Iteration 2882/6000, G loss: 0.5552858114242554, D loss: 0.002134673297405243\n",
            "Iteration 2883/6000, G loss: 0.595279335975647, D loss: 9.217541082762182e-05\n",
            "Iteration 2884/6000, G loss: 0.5069896578788757, D loss: 0.0030137402936816216\n",
            "Iteration 2885/6000, G loss: 0.5195749998092651, D loss: 0.00016418226005043834\n",
            "Iteration 2886/6000, G loss: 0.483300119638443, D loss: 0.005909589119255543\n",
            "Iteration 2887/6000, G loss: 0.5108115077018738, D loss: 0.00014596997061744332\n",
            "Iteration 2888/6000, G loss: 0.4344049096107483, D loss: 0.0034778653644025326\n",
            "Iteration 2889/6000, G loss: 0.49662619829177856, D loss: 0.00011574178643058985\n",
            "Iteration 2890/6000, G loss: 0.5308631062507629, D loss: 0.0016923443181440234\n",
            "Iteration 2891/6000, G loss: 0.6806030869483948, D loss: 0.0\n",
            "Iteration 2892/6000, G loss: 0.6649050712585449, D loss: 3.5881923849956365e-06\n",
            "Iteration 2893/6000, G loss: 0.6302361488342285, D loss: 0.009928351268172264\n",
            "Iteration 2894/6000, G loss: 0.6045258045196533, D loss: 4.250917845638469e-05\n",
            "Iteration 2895/6000, G loss: 0.486880362033844, D loss: 0.005872150417417288\n",
            "Iteration 2896/6000, G loss: 0.6204231381416321, D loss: 0.00259866239503026\n",
            "Iteration 2897/6000, G loss: 0.5463336110115051, D loss: 0.001279471325688064\n",
            "Iteration 2898/6000, G loss: 0.45240092277526855, D loss: 0.0005832505994476378\n",
            "Iteration 2899/6000, G loss: 0.502783477306366, D loss: 0.0012289416044950485\n",
            "Iteration 2900/6000, G loss: 0.5381660461425781, D loss: 0.0007178037194535136\n",
            "Iteration 2901/6000, G loss: 0.5624479651451111, D loss: 7.903554433141835e-06\n",
            "Iteration 2902/6000, G loss: 0.5956698656082153, D loss: 1.3041413694736548e-05\n",
            "Iteration 2903/6000, G loss: 0.634750485420227, D loss: 9.179110520562972e-07\n",
            "Iteration 2904/6000, G loss: 0.6193004846572876, D loss: 3.691866004373878e-05\n",
            "Iteration 2905/6000, G loss: 0.5279527902603149, D loss: 4.136556526646018e-06\n",
            "Iteration 2906/6000, G loss: 0.5138716101646423, D loss: 6.679115176666528e-05\n",
            "Iteration 2907/6000, G loss: 0.4259195625782013, D loss: 0.0004343732725828886\n",
            "Iteration 2908/6000, G loss: 0.5814783573150635, D loss: 1.5616408290952677e-06\n",
            "Iteration 2909/6000, G loss: 0.5600547194480896, D loss: 3.1113577279029414e-06\n",
            "Iteration 2910/6000, G loss: 0.5181941986083984, D loss: 0.00013799544831272215\n",
            "Iteration 2911/6000, G loss: 0.4851597547531128, D loss: 0.0008862324175424874\n",
            "Iteration 2912/6000, G loss: 0.5865694880485535, D loss: 5.882964615011588e-06\n",
            "Iteration 2913/6000, G loss: 0.5088725686073303, D loss: 0.0022436229046434164\n",
            "Iteration 2914/6000, G loss: 0.46880802512168884, D loss: 0.0016525955870747566\n",
            "Iteration 2915/6000, G loss: 0.4552670121192932, D loss: 0.0008726504165679216\n",
            "Iteration 2916/6000, G loss: 0.4837466776371002, D loss: 6.210067658685148e-05\n",
            "Iteration 2917/6000, G loss: 0.6859842538833618, D loss: 9.417530577593425e-07\n",
            "Iteration 2918/6000, G loss: 0.5113686919212341, D loss: 0.0026309634558856487\n",
            "Iteration 2919/6000, G loss: 0.5475612878799438, D loss: 4.374974196252879e-06\n",
            "Iteration 2920/6000, G loss: 0.4525378942489624, D loss: 0.0003561125777196139\n",
            "Iteration 2921/6000, G loss: 0.4601389169692993, D loss: 0.006637591868638992\n",
            "Iteration 2922/6000, G loss: 0.5605434775352478, D loss: 0.0008511024061590433\n",
            "Iteration 2923/6000, G loss: 0.37944886088371277, D loss: 0.0012922591995447874\n",
            "Iteration 2924/6000, G loss: 0.532715916633606, D loss: 3.8283564208541065e-05\n",
            "Iteration 2925/6000, G loss: 0.6015155911445618, D loss: 0.0001026912868837826\n",
            "Iteration 2926/6000, G loss: 0.7307708263397217, D loss: 1.400708242726978e-06\n",
            "Iteration 2927/6000, G loss: 0.6237645745277405, D loss: 7.395507418550551e-05\n",
            "Iteration 2928/6000, G loss: 0.5949541926383972, D loss: 5.209321534493938e-05\n",
            "Iteration 2929/6000, G loss: 0.6518657207489014, D loss: 3.192369331372902e-05\n",
            "Iteration 2930/6000, G loss: 0.4068492352962494, D loss: 0.025816597044467926\n",
            "Iteration 2931/6000, G loss: 0.4358602464199066, D loss: 0.012372453697025776\n",
            "Iteration 2932/6000, G loss: 0.6558744311332703, D loss: 2.2732985598850064e-05\n",
            "Iteration 2933/6000, G loss: 0.6177817583084106, D loss: 1.696902654657606e-05\n",
            "Iteration 2934/6000, G loss: 0.5141403675079346, D loss: 0.008318386040627956\n",
            "Iteration 2935/6000, G loss: 0.5367469191551208, D loss: 4.406408697832376e-05\n",
            "Iteration 2936/6000, G loss: 0.7074447870254517, D loss: 0.00014962369459681213\n",
            "Iteration 2937/6000, G loss: 0.5177229642868042, D loss: 0.001499323989264667\n",
            "Iteration 2938/6000, G loss: 0.5988748669624329, D loss: 9.883826714940369e-05\n",
            "Iteration 2939/6000, G loss: 0.45519474148750305, D loss: 0.3197574317455292\n",
            "Iteration 2940/6000, G loss: 0.5549188852310181, D loss: 0.002225002972409129\n",
            "Iteration 2941/6000, G loss: 0.46425682306289673, D loss: 0.0019641611725091934\n",
            "Iteration 2942/6000, G loss: 0.38991981744766235, D loss: 46.29296875\n",
            "Iteration 2943/6000, G loss: 0.5668173432350159, D loss: 1.849520776886493e-05\n",
            "Iteration 2944/6000, G loss: 0.4948464035987854, D loss: 3.531533729983494e-05\n",
            "Iteration 2945/6000, G loss: 0.5033495426177979, D loss: 6.261930684559047e-05\n",
            "Iteration 2946/6000, G loss: 0.48604604601860046, D loss: 0.00012897503620479256\n",
            "Iteration 2947/6000, G loss: 0.5363809466362, D loss: 1.223082381329732e-05\n",
            "Iteration 2948/6000, G loss: 0.5861655473709106, D loss: 2.2756757971365005e-05\n",
            "Iteration 2949/6000, G loss: 0.4535366892814636, D loss: 0.0002459676470607519\n",
            "Iteration 2950/6000, G loss: 0.5839270353317261, D loss: 6.588044166564941\n",
            "Iteration 2951/6000, G loss: 0.5268300175666809, D loss: 0.0005897380178794265\n",
            "Iteration 2952/6000, G loss: 0.5471229553222656, D loss: 1.5628205801476724e-05\n",
            "Iteration 2953/6000, G loss: 0.4721169173717499, D loss: 0.06701961904764175\n",
            "Iteration 2954/6000, G loss: 0.5514428615570068, D loss: 0.015495575964450836\n",
            "Iteration 2955/6000, G loss: 0.47700995206832886, D loss: 0.02276826649904251\n",
            "Iteration 2956/6000, G loss: 0.5458440184593201, D loss: 0.35678258538246155\n",
            "Iteration 2957/6000, G loss: 0.48748376965522766, D loss: 0.0979621484875679\n",
            "Iteration 2958/6000, G loss: 0.6371335983276367, D loss: 0.352750301361084\n",
            "Iteration 2959/6000, G loss: 0.6227100491523743, D loss: 0.11171504855155945\n",
            "Iteration 2960/6000, G loss: 0.45245644450187683, D loss: 0.11794398725032806\n",
            "Iteration 2961/6000, G loss: 0.596024215221405, D loss: 0.11351867020130157\n",
            "Iteration 2962/6000, G loss: 0.4942970871925354, D loss: 0.043950051069259644\n",
            "Iteration 2963/6000, G loss: 0.5118264555931091, D loss: 0.07345010340213776\n",
            "Iteration 2964/6000, G loss: 0.5607826113700867, D loss: 0.08146703988313675\n",
            "Iteration 2965/6000, G loss: 0.43031659722328186, D loss: 0.1318131983280182\n",
            "Iteration 2966/6000, G loss: 0.5068127512931824, D loss: 0.03077145293354988\n",
            "Iteration 2967/6000, G loss: 0.5492636561393738, D loss: 0.039417143911123276\n",
            "Iteration 2968/6000, G loss: 0.4649534523487091, D loss: 0.01528538390994072\n",
            "Iteration 2969/6000, G loss: 0.4918031394481659, D loss: 0.015445826575160027\n",
            "Iteration 2970/6000, G loss: 0.6923140287399292, D loss: 0.00779986847192049\n",
            "Iteration 2971/6000, G loss: 0.50775545835495, D loss: 0.05005625635385513\n",
            "Iteration 2972/6000, G loss: 0.561087429523468, D loss: 0.011888694949448109\n",
            "Iteration 2973/6000, G loss: 0.5560011267662048, D loss: 0.010919982567429543\n",
            "Iteration 2974/6000, G loss: 0.460530549287796, D loss: 0.0053308457136154175\n",
            "Iteration 2975/6000, G loss: 0.5426726341247559, D loss: 0.0015171742998063564\n",
            "Iteration 2976/6000, G loss: 0.4410104751586914, D loss: 0.007357761729508638\n",
            "Iteration 2977/6000, G loss: 0.5560334920883179, D loss: 0.003163126762956381\n",
            "Iteration 2978/6000, G loss: 0.4641377329826355, D loss: 0.020473379641771317\n",
            "Iteration 2979/6000, G loss: 0.6606335639953613, D loss: 0.006857326719909906\n",
            "Iteration 2980/6000, G loss: 0.48195454478263855, D loss: 0.001504702027887106\n",
            "Iteration 2981/6000, G loss: 0.43968090415000916, D loss: 0.0007590249879285693\n",
            "Iteration 2982/6000, G loss: 0.5808585286140442, D loss: 0.0007849949761293828\n",
            "Iteration 2983/6000, G loss: 0.6910876631736755, D loss: 0.0007960774237290025\n",
            "Iteration 2984/6000, G loss: 0.5238327383995056, D loss: 0.0019390055676922202\n",
            "Iteration 2985/6000, G loss: 0.5178970098495483, D loss: 0.003956083673983812\n",
            "Iteration 2986/6000, G loss: 0.47233518958091736, D loss: 7.844297215342522e-05\n",
            "Iteration 2987/6000, G loss: 0.5586119294166565, D loss: 0.0025229318998754025\n",
            "Iteration 2988/6000, G loss: 0.4953763782978058, D loss: 0.0012889762874692678\n",
            "Iteration 2989/6000, G loss: 0.5273599028587341, D loss: 9.240815415978432e-05\n",
            "Iteration 2990/6000, G loss: 0.6417267918586731, D loss: 0.000492490769829601\n",
            "Iteration 2991/6000, G loss: 0.42471325397491455, D loss: 0.004059054423123598\n",
            "Iteration 2992/6000, G loss: 0.4556281566619873, D loss: 0.0983208417892456\n",
            "Iteration 2993/6000, G loss: 0.6442800760269165, D loss: 0.00029857392655685544\n",
            "Iteration 2994/6000, G loss: 0.5479102730751038, D loss: 0.011889985762536526\n",
            "Iteration 2995/6000, G loss: 0.5116890668869019, D loss: 0.00010320056753698736\n",
            "Iteration 2996/6000, G loss: 0.5110886693000793, D loss: 7.5230622314848e-05\n",
            "Iteration 2997/6000, G loss: 0.46181991696357727, D loss: 0.0003606709651648998\n",
            "Iteration 2998/6000, G loss: 0.5058729648590088, D loss: 8.808741404209286e-05\n",
            "Iteration 2999/6000, G loss: 0.6590150594711304, D loss: 8.167614578269422e-05\n",
            "Iteration 3000/6000, G loss: 0.45319467782974243, D loss: 0.0003895290137734264\n",
            "Iteration 3001/6000, G loss: 0.5170776844024658, D loss: 0.0008952736388891935\n",
            "Iteration 3002/6000, G loss: 0.48525965213775635, D loss: 0.001179783372208476\n",
            "Iteration 3003/6000, G loss: 0.4654945135116577, D loss: 0.0006620651693083346\n",
            "Iteration 3004/6000, G loss: 0.5380863547325134, D loss: 0.004430272150784731\n",
            "Iteration 3005/6000, G loss: 0.5752890706062317, D loss: 0.0016826638020575047\n",
            "Iteration 3006/6000, G loss: 0.5540530681610107, D loss: 0.028965111821889877\n",
            "Iteration 3007/6000, G loss: 0.6069837212562561, D loss: 0.00027485142345540226\n",
            "Iteration 3008/6000, G loss: 0.6114997863769531, D loss: 0.0012796887895092368\n",
            "Iteration 3009/6000, G loss: 0.5723893046379089, D loss: 8.00136913312599e-05\n",
            "Iteration 3010/6000, G loss: 0.5531197190284729, D loss: 2.3620972569915466e-05\n",
            "Iteration 3011/6000, G loss: 0.4836878776550293, D loss: 8.678390258864965e-06\n",
            "Iteration 3012/6000, G loss: 0.5320647954940796, D loss: 9.200751082971692e-05\n",
            "Iteration 3013/6000, G loss: 0.525550365447998, D loss: 0.0003477264544926584\n",
            "Iteration 3014/6000, G loss: 0.4509142339229584, D loss: 0.0005745525122620165\n",
            "Iteration 3015/6000, G loss: 0.48708006739616394, D loss: 0.00240968051366508\n",
            "Iteration 3016/6000, G loss: 0.5864186882972717, D loss: 0.0004129273584112525\n",
            "Iteration 3017/6000, G loss: 0.5210118293762207, D loss: 0.0009934469126164913\n",
            "Iteration 3018/6000, G loss: 0.5948929786682129, D loss: 0.015288911759853363\n",
            "Iteration 3019/6000, G loss: 0.5427822470664978, D loss: 0.0001084368268493563\n",
            "Iteration 3020/6000, G loss: 0.5287870764732361, D loss: 0.0002185803750762716\n",
            "Iteration 3021/6000, G loss: 0.5635101199150085, D loss: 1.613483254914172e-05\n",
            "Iteration 3022/6000, G loss: 0.6574684977531433, D loss: 9.814649820327759e-05\n",
            "Iteration 3023/6000, G loss: 0.5326836109161377, D loss: 0.005707752425223589\n",
            "Iteration 3024/6000, G loss: 0.5403247475624084, D loss: 0.03608721122145653\n",
            "Iteration 3025/6000, G loss: 0.5566231608390808, D loss: 0.014234479516744614\n",
            "Iteration 3026/6000, G loss: 0.5655796527862549, D loss: 0.02183111011981964\n",
            "Iteration 3027/6000, G loss: 0.49616605043411255, D loss: 0.00026780078769661486\n",
            "Iteration 3028/6000, G loss: 0.56233811378479, D loss: 0.009583227336406708\n",
            "Iteration 3029/6000, G loss: 0.4673226773738861, D loss: 0.010959295555949211\n",
            "Iteration 3030/6000, G loss: 0.4932224750518799, D loss: 0.004381480626761913\n",
            "Iteration 3031/6000, G loss: 0.5916139483451843, D loss: 0.015040002763271332\n",
            "Iteration 3032/6000, G loss: 0.5867602229118347, D loss: 0.019055940210819244\n",
            "Iteration 3033/6000, G loss: 0.6009273529052734, D loss: 0.011276522651314735\n",
            "Iteration 3034/6000, G loss: 0.6361064910888672, D loss: 0.08089244365692139\n",
            "Iteration 3035/6000, G loss: 0.36658531427383423, D loss: 0.0012699656654149294\n",
            "Iteration 3036/6000, G loss: 0.505730390548706, D loss: 4.787794387084432e-05\n",
            "Iteration 3037/6000, G loss: 0.6034390330314636, D loss: 3.891449159709737e-05\n",
            "Iteration 3038/6000, G loss: 0.47546684741973877, D loss: 5.804142710985616e-05\n",
            "Iteration 3039/6000, G loss: 0.46411651372909546, D loss: 0.005254716146737337\n",
            "Iteration 3040/6000, G loss: 0.57914799451828, D loss: 6.705467058054637e-06\n",
            "Iteration 3041/6000, G loss: 0.6289486885070801, D loss: 0.011515913531184196\n",
            "Iteration 3042/6000, G loss: 0.5417657494544983, D loss: 0.002097517251968384\n",
            "Iteration 3043/6000, G loss: 0.6130688190460205, D loss: 2.161743640899658\n",
            "Iteration 3044/6000, G loss: 0.5447391867637634, D loss: 1.7076505173463374e-05\n",
            "Iteration 3045/6000, G loss: 0.4339616298675537, D loss: 0.01983233354985714\n",
            "Iteration 3046/6000, G loss: 0.38938507437705994, D loss: 3.749114966922207e-06\n",
            "Iteration 3047/6000, G loss: 0.5479980707168579, D loss: 8.445826097158715e-06\n",
            "Iteration 3048/6000, G loss: 0.5520636439323425, D loss: 8.046609991652076e-07\n",
            "Iteration 3049/6000, G loss: 0.5607222318649292, D loss: 2.7500889700604603e-05\n",
            "Iteration 3050/6000, G loss: 0.6063356399536133, D loss: 0.45633721351623535\n",
            "Iteration 3051/6000, G loss: 0.467374324798584, D loss: 0.0008259529713541269\n",
            "Iteration 3052/6000, G loss: 0.43167316913604736, D loss: 0.0030965725891292095\n",
            "Iteration 3053/6000, G loss: 0.4914661645889282, D loss: 9.693721221992746e-05\n",
            "Iteration 3054/6000, G loss: 0.5802844166755676, D loss: 0.0017257025465369225\n",
            "Iteration 3055/6000, G loss: 0.5198361277580261, D loss: 0.002452829387038946\n",
            "Iteration 3056/6000, G loss: 0.5966765880584717, D loss: 0.00028557321638800204\n",
            "Iteration 3057/6000, G loss: 0.5236272811889648, D loss: 0.007546709850430489\n",
            "Iteration 3058/6000, G loss: 0.4665297865867615, D loss: 0.9869223237037659\n",
            "Iteration 3059/6000, G loss: 0.6621928811073303, D loss: 0.00815255381166935\n",
            "Iteration 3060/6000, G loss: 0.5748794674873352, D loss: 0.0004446344100870192\n",
            "Iteration 3061/6000, G loss: 0.5365252494812012, D loss: 0.00018813151109497994\n",
            "Iteration 3062/6000, G loss: 0.6502184867858887, D loss: 0.006631826050579548\n",
            "Iteration 3063/6000, G loss: 0.6021504402160645, D loss: 9.23725456232205e-05\n",
            "Iteration 3064/6000, G loss: 0.41728276014328003, D loss: 0.009686613455414772\n",
            "Iteration 3065/6000, G loss: 0.7318524718284607, D loss: 1.052595234796172e-05\n",
            "Iteration 3066/6000, G loss: 0.45337310433387756, D loss: 0.011126525700092316\n",
            "Iteration 3067/6000, G loss: 0.5509099364280701, D loss: 1.3876309394836426\n",
            "Iteration 3068/6000, G loss: 0.6017058491706848, D loss: 5.982762377243489e-05\n",
            "Iteration 3069/6000, G loss: 0.46523743867874146, D loss: 0.00018077355343848467\n",
            "Iteration 3070/6000, G loss: 0.5653377175331116, D loss: 0.11532348394393921\n",
            "Iteration 3071/6000, G loss: 0.4442267417907715, D loss: 1.0141265392303467\n",
            "Iteration 3072/6000, G loss: 0.596674382686615, D loss: 0.0017916718497872353\n",
            "Iteration 3073/6000, G loss: 0.49437129497528076, D loss: 0.551252007484436\n",
            "Iteration 3074/6000, G loss: 0.5424377918243408, D loss: 1.9937390089035034\n",
            "Iteration 3075/6000, G loss: 0.5219206809997559, D loss: 0.0017394765745848417\n",
            "Iteration 3076/6000, G loss: 0.45800814032554626, D loss: 1.2308304576436058e-05\n",
            "Iteration 3077/6000, G loss: 0.5022892951965332, D loss: 5.5234097089851275e-05\n",
            "Iteration 3078/6000, G loss: 0.7638379335403442, D loss: 0.02057238668203354\n",
            "Iteration 3079/6000, G loss: 0.47534868121147156, D loss: 0.00023410923313349485\n",
            "Iteration 3080/6000, G loss: 0.6381113529205322, D loss: 0.0072686499916017056\n",
            "Iteration 3081/6000, G loss: 0.5554666519165039, D loss: 7.867810154493782e-07\n",
            "Iteration 3082/6000, G loss: 0.47577619552612305, D loss: 3.463022494543111e-06\n",
            "Iteration 3083/6000, G loss: 0.5769129395484924, D loss: 5.636571950162761e-05\n",
            "Iteration 3084/6000, G loss: 0.6553771495819092, D loss: 0.2246861755847931\n",
            "Iteration 3085/6000, G loss: 0.5658344626426697, D loss: 0.0005113257793709636\n",
            "Iteration 3086/6000, G loss: 0.5440277457237244, D loss: 1.0388396978378296\n",
            "Iteration 3087/6000, G loss: 0.6467550992965698, D loss: 7.272147178649902\n",
            "Iteration 3088/6000, G loss: 0.5539698600769043, D loss: 0.9345749616622925\n",
            "Iteration 3089/6000, G loss: 0.5449698567390442, D loss: 0.16257403790950775\n",
            "Iteration 3090/6000, G loss: 0.48240867257118225, D loss: 0.3579291105270386\n",
            "Iteration 3091/6000, G loss: 0.5476005673408508, D loss: 0.6080692410469055\n",
            "Iteration 3092/6000, G loss: 0.510086715221405, D loss: 0.6920585036277771\n",
            "Iteration 3093/6000, G loss: 0.62058025598526, D loss: 0.5172854065895081\n",
            "Iteration 3094/6000, G loss: 0.5898688435554504, D loss: 0.7278379201889038\n",
            "Iteration 3095/6000, G loss: 0.5582212209701538, D loss: 0.3918825387954712\n",
            "Iteration 3096/6000, G loss: 0.5547119379043579, D loss: 0.3872562050819397\n",
            "Iteration 3097/6000, G loss: 0.5236749649047852, D loss: 0.3116230070590973\n",
            "Iteration 3098/6000, G loss: 0.5310043096542358, D loss: 0.18690812587738037\n",
            "Iteration 3099/6000, G loss: 0.6011142730712891, D loss: 0.07856261730194092\n",
            "Iteration 3100/6000, G loss: 0.5346709489822388, D loss: 0.05279121175408363\n",
            "Iteration 3101/6000, G loss: 0.46535250544548035, D loss: 0.05011753365397453\n",
            "Iteration 3102/6000, G loss: 0.505983293056488, D loss: 0.08603538572788239\n",
            "Iteration 3103/6000, G loss: 0.6005562543869019, D loss: 0.03832703083753586\n",
            "Iteration 3104/6000, G loss: 0.4929097592830658, D loss: 0.023624394088983536\n",
            "Iteration 3105/6000, G loss: 0.4947241544723511, D loss: 0.015599006786942482\n",
            "Iteration 3106/6000, G loss: 0.45242059230804443, D loss: 0.006406501866877079\n",
            "Iteration 3107/6000, G loss: 0.4310336112976074, D loss: 0.01046371553093195\n",
            "Iteration 3108/6000, G loss: 0.41284024715423584, D loss: 0.013292724266648293\n",
            "Iteration 3109/6000, G loss: 0.5091118812561035, D loss: 0.007196526974439621\n",
            "Iteration 3110/6000, G loss: 0.4532603919506073, D loss: 0.07242217659950256\n",
            "Iteration 3111/6000, G loss: 0.5628411769866943, D loss: 0.0020481576211750507\n",
            "Iteration 3112/6000, G loss: 0.6730413436889648, D loss: 0.00234239362180233\n",
            "Iteration 3113/6000, G loss: 0.5663844347000122, D loss: 0.004596086218953133\n",
            "Iteration 3114/6000, G loss: 0.5600775480270386, D loss: 0.0018279639771208167\n",
            "Iteration 3115/6000, G loss: 0.48513370752334595, D loss: 0.026724396273493767\n",
            "Iteration 3116/6000, G loss: 0.5815640687942505, D loss: 0.0037947767414152622\n",
            "Iteration 3117/6000, G loss: 0.3319978415966034, D loss: 0.0041162315756082535\n",
            "Iteration 3118/6000, G loss: 0.5056058168411255, D loss: 0.006725371349602938\n",
            "Iteration 3119/6000, G loss: 0.5622112154960632, D loss: 0.000577797181904316\n",
            "Iteration 3120/6000, G loss: 0.5616754293441772, D loss: 0.0030429719481617212\n",
            "Iteration 3121/6000, G loss: 0.5286481380462646, D loss: 0.011488193646073341\n",
            "Iteration 3122/6000, G loss: 0.5011360049247742, D loss: 0.0010634344071149826\n",
            "Iteration 3123/6000, G loss: 0.5021969079971313, D loss: 0.0005251760012470186\n",
            "Iteration 3124/6000, G loss: 0.6900061964988708, D loss: 0.0005626512574963272\n",
            "Iteration 3125/6000, G loss: 0.593008816242218, D loss: 0.0004131194727960974\n",
            "Iteration 3126/6000, G loss: 0.468473881483078, D loss: 0.0008221659227274358\n",
            "Iteration 3127/6000, G loss: 0.700703501701355, D loss: 0.0024959701113402843\n",
            "Iteration 3128/6000, G loss: 0.5205242037773132, D loss: 0.010130460374057293\n",
            "Iteration 3129/6000, G loss: 0.5243940949440002, D loss: 0.11670172959566116\n",
            "Iteration 3130/6000, G loss: 0.5289862751960754, D loss: 0.02294684574007988\n",
            "Iteration 3131/6000, G loss: 0.5520910620689392, D loss: 0.005199279636144638\n",
            "Iteration 3132/6000, G loss: 0.5108987092971802, D loss: 0.0010584932751953602\n",
            "Iteration 3133/6000, G loss: 0.5892425775527954, D loss: 0.05382504314184189\n",
            "Iteration 3134/6000, G loss: 0.5693964958190918, D loss: 0.00048207968939095736\n",
            "Iteration 3135/6000, G loss: 0.4396311342716217, D loss: 0.00021549710072577\n",
            "Iteration 3136/6000, G loss: 0.5629051327705383, D loss: 0.002828312339261174\n",
            "Iteration 3137/6000, G loss: 0.5306583642959595, D loss: 0.0006395236123353243\n",
            "Iteration 3138/6000, G loss: 0.5028336644172668, D loss: 0.08987732231616974\n",
            "Iteration 3139/6000, G loss: 0.5562180876731873, D loss: 0.0007750375079922378\n",
            "Iteration 3140/6000, G loss: 0.5130585432052612, D loss: 0.0012879406567662954\n",
            "Iteration 3141/6000, G loss: 0.5802100896835327, D loss: 0.006990029476583004\n",
            "Iteration 3142/6000, G loss: 0.46962663531303406, D loss: 0.012736455537378788\n",
            "Iteration 3143/6000, G loss: 0.461555153131485, D loss: 2.2130878278403543e-05\n",
            "Iteration 3144/6000, G loss: 0.4748339354991913, D loss: 0.0013453932479023933\n",
            "Iteration 3145/6000, G loss: 0.5869245529174805, D loss: 0.0017706311773508787\n",
            "Iteration 3146/6000, G loss: 0.5612471103668213, D loss: 0.0006896652048453689\n",
            "Iteration 3147/6000, G loss: 0.5435889363288879, D loss: 0.009060630574822426\n",
            "Iteration 3148/6000, G loss: 0.4343128800392151, D loss: 0.0013904188526794314\n",
            "Iteration 3149/6000, G loss: 0.5534864068031311, D loss: 0.0013244743458926678\n",
            "Iteration 3150/6000, G loss: 0.5206683278083801, D loss: 0.0009747002040967345\n",
            "Iteration 3151/6000, G loss: 0.5493595004081726, D loss: 0.0016380229499191046\n",
            "Iteration 3152/6000, G loss: 0.4861662983894348, D loss: 0.0009340494871139526\n",
            "Iteration 3153/6000, G loss: 0.49480023980140686, D loss: 0.01469802763313055\n",
            "Iteration 3154/6000, G loss: 0.4111344814300537, D loss: 0.018280308693647385\n",
            "Iteration 3155/6000, G loss: 0.5055422782897949, D loss: 0.014685170724987984\n",
            "Iteration 3156/6000, G loss: 0.5734989643096924, D loss: 0.0001246671163244173\n",
            "Iteration 3157/6000, G loss: 0.5428341031074524, D loss: 0.0005299580516293645\n",
            "Iteration 3158/6000, G loss: 0.5440120100975037, D loss: 6.13307420280762e-05\n",
            "Iteration 3159/6000, G loss: 0.5599738359451294, D loss: 0.0009258684003725648\n",
            "Iteration 3160/6000, G loss: 0.5069490075111389, D loss: 0.01676269620656967\n",
            "Iteration 3161/6000, G loss: 0.5144652128219604, D loss: 0.001731588039547205\n",
            "Iteration 3162/6000, G loss: 0.6677941679954529, D loss: 0.046845585107803345\n",
            "Iteration 3163/6000, G loss: 0.4940522313117981, D loss: 0.00801149569451809\n",
            "Iteration 3164/6000, G loss: 0.5012451410293579, D loss: 1.489491478423588e-05\n",
            "Iteration 3165/6000, G loss: 0.563841700553894, D loss: 0.01958182081580162\n",
            "Iteration 3166/6000, G loss: 0.4864906370639801, D loss: 0.0006855538813397288\n",
            "Iteration 3167/6000, G loss: 0.4949686527252197, D loss: 0.008269112557172775\n",
            "Iteration 3168/6000, G loss: 0.5279788970947266, D loss: 0.007119147572666407\n",
            "Iteration 3169/6000, G loss: 0.5096805691719055, D loss: 0.001144241075962782\n",
            "Iteration 3170/6000, G loss: 0.5008506178855896, D loss: 0.0012043791357427835\n",
            "Iteration 3171/6000, G loss: 0.4740520715713501, D loss: 0.0006839424604550004\n",
            "Iteration 3172/6000, G loss: 0.5833480954170227, D loss: 0.000819116597995162\n",
            "Iteration 3173/6000, G loss: 0.5535229444503784, D loss: 0.00031011138344183564\n",
            "Iteration 3174/6000, G loss: 0.658654510974884, D loss: 0.004004661459475756\n",
            "Iteration 3175/6000, G loss: 0.49644139409065247, D loss: 0.037458859384059906\n",
            "Iteration 3176/6000, G loss: 0.5605584979057312, D loss: 0.0014894518535584211\n",
            "Iteration 3177/6000, G loss: 0.5795560479164124, D loss: 1.118595838546753\n",
            "Iteration 3178/6000, G loss: 0.5012350678443909, D loss: 0.0013468482065945864\n",
            "Iteration 3179/6000, G loss: 0.5207732319831848, D loss: 0.004355115350335836\n",
            "Iteration 3180/6000, G loss: 0.636167585849762, D loss: 0.0003988030948676169\n",
            "Iteration 3181/6000, G loss: 0.5162225365638733, D loss: 0.0010128358844667673\n",
            "Iteration 3182/6000, G loss: 0.449837327003479, D loss: 0.004833840765058994\n",
            "Iteration 3183/6000, G loss: 0.5782173275947571, D loss: 0.0016317262779921293\n",
            "Iteration 3184/6000, G loss: 0.6115176677703857, D loss: 0.00046736630611121655\n",
            "Iteration 3185/6000, G loss: 0.4889944791793823, D loss: 0.039446014910936356\n",
            "Iteration 3186/6000, G loss: 0.564407229423523, D loss: 0.0020132435020059347\n",
            "Iteration 3187/6000, G loss: 0.5520787239074707, D loss: 5.664710624841973e-05\n",
            "Iteration 3188/6000, G loss: 0.3094978630542755, D loss: 0.10014679282903671\n",
            "Iteration 3189/6000, G loss: 0.4974406957626343, D loss: 6.0021720855729654e-06\n",
            "Iteration 3190/6000, G loss: 0.5763262510299683, D loss: 0.0008355436148121953\n",
            "Iteration 3191/6000, G loss: 0.49312761425971985, D loss: 1.449283480644226\n",
            "Iteration 3192/6000, G loss: 0.5485303997993469, D loss: 5.387561395764351e-05\n",
            "Iteration 3193/6000, G loss: 0.5970060229301453, D loss: 0.0009522485779598355\n",
            "Iteration 3194/6000, G loss: 0.4732937216758728, D loss: 0.0006169866537675261\n",
            "Iteration 3195/6000, G loss: 0.5916862487792969, D loss: 0.004178022965788841\n",
            "Iteration 3196/6000, G loss: 0.5433060526847839, D loss: 0.0008328617550432682\n",
            "Iteration 3197/6000, G loss: 0.49287426471710205, D loss: 0.0010216450318694115\n",
            "Iteration 3198/6000, G loss: 0.6563959121704102, D loss: 0.002176647773012519\n",
            "Iteration 3199/6000, G loss: 0.502998948097229, D loss: 0.002435903763398528\n",
            "Iteration 3200/6000, G loss: 0.5569576025009155, D loss: 0.00129621010273695\n",
            "Iteration 3201/6000, G loss: 0.5764068961143494, D loss: 0.36666637659072876\n",
            "Iteration 3202/6000, G loss: 0.5220029950141907, D loss: 0.004849435295909643\n",
            "Iteration 3203/6000, G loss: 0.597885012626648, D loss: 0.0275241881608963\n",
            "Iteration 3204/6000, G loss: 0.6111339926719666, D loss: 0.002772162901237607\n",
            "Iteration 3205/6000, G loss: 0.6547623872756958, D loss: 0.05886165797710419\n",
            "Iteration 3206/6000, G loss: 0.5371876358985901, D loss: 0.0026332461275160313\n",
            "Iteration 3207/6000, G loss: 0.5487033724784851, D loss: 0.0025054574944078922\n",
            "Iteration 3208/6000, G loss: 0.5007791519165039, D loss: 0.003619823604822159\n",
            "Iteration 3209/6000, G loss: 0.44815880060195923, D loss: 0.06743491441011429\n",
            "Iteration 3210/6000, G loss: 0.5265794396400452, D loss: 0.007383085787296295\n",
            "Iteration 3211/6000, G loss: 0.6023052930831909, D loss: 0.09381350874900818\n",
            "Iteration 3212/6000, G loss: 0.5324812531471252, D loss: 9.47269058227539\n",
            "Iteration 3213/6000, G loss: 0.6265242099761963, D loss: 2.114300489425659\n",
            "Iteration 3214/6000, G loss: 0.39975330233573914, D loss: 0.005254595540463924\n",
            "Iteration 3215/6000, G loss: 0.5450702905654907, D loss: 0.15605221688747406\n",
            "Iteration 3216/6000, G loss: 0.5063338875770569, D loss: 0.2659928500652313\n",
            "Iteration 3217/6000, G loss: 0.4851309061050415, D loss: 0.25206446647644043\n",
            "Iteration 3218/6000, G loss: 0.5987546443939209, D loss: 0.33371514081954956\n",
            "Iteration 3219/6000, G loss: 0.4378719925880432, D loss: 0.2948259711265564\n",
            "Iteration 3220/6000, G loss: 0.5624561905860901, D loss: 0.3779926896095276\n",
            "Iteration 3221/6000, G loss: 0.4350214898586273, D loss: 0.443011999130249\n",
            "Iteration 3222/6000, G loss: 0.5783030390739441, D loss: 0.18342313170433044\n",
            "Iteration 3223/6000, G loss: 0.5499029159545898, D loss: 0.3056877553462982\n",
            "Iteration 3224/6000, G loss: 0.45161715149879456, D loss: 0.1845162957906723\n",
            "Iteration 3225/6000, G loss: 0.5698326826095581, D loss: 0.18152731657028198\n",
            "Iteration 3226/6000, G loss: 0.5433878898620605, D loss: 0.16242378950119019\n",
            "Iteration 3227/6000, G loss: 0.4240987002849579, D loss: 0.10299351811408997\n",
            "Iteration 3228/6000, G loss: 0.5726994276046753, D loss: 0.08879232406616211\n",
            "Iteration 3229/6000, G loss: 0.549137532711029, D loss: 0.19562488794326782\n",
            "Iteration 3230/6000, G loss: 0.698767900466919, D loss: 0.30913829803466797\n",
            "Iteration 3231/6000, G loss: 0.4568093717098236, D loss: 0.06063390523195267\n",
            "Iteration 3232/6000, G loss: 0.481862336397171, D loss: 0.07054562866687775\n",
            "Iteration 3233/6000, G loss: 0.577635645866394, D loss: 0.029970962554216385\n",
            "Iteration 3234/6000, G loss: 0.4894549250602722, D loss: 0.028914794325828552\n",
            "Iteration 3235/6000, G loss: 0.4966970384120941, D loss: 0.010527027770876884\n",
            "Iteration 3236/6000, G loss: 0.4644252359867096, D loss: 0.0052648913115262985\n",
            "Iteration 3237/6000, G loss: 0.5125148892402649, D loss: 0.009138884022831917\n",
            "Iteration 3238/6000, G loss: 0.4704979360103607, D loss: 0.002536178333684802\n",
            "Iteration 3239/6000, G loss: 0.40419137477874756, D loss: 0.059043753892183304\n",
            "Iteration 3240/6000, G loss: 0.4194473326206207, D loss: 0.07973629236221313\n",
            "Iteration 3241/6000, G loss: 0.5444481372833252, D loss: 0.06463103741407394\n",
            "Iteration 3242/6000, G loss: 0.5620104074478149, D loss: 0.006883257068693638\n",
            "Iteration 3243/6000, G loss: 0.5257664918899536, D loss: 0.0017659381264820695\n",
            "Iteration 3244/6000, G loss: 0.6265732645988464, D loss: 0.0030013457871973515\n",
            "Iteration 3245/6000, G loss: 0.5559608340263367, D loss: 0.004553718026727438\n",
            "Iteration 3246/6000, G loss: 0.551727831363678, D loss: 0.0019894058350473642\n",
            "Iteration 3247/6000, G loss: 0.4906327724456787, D loss: 0.00037671663449145854\n",
            "Iteration 3248/6000, G loss: 0.40902796387672424, D loss: 0.0006631866563111544\n",
            "Iteration 3249/6000, G loss: 0.4144818186759949, D loss: 0.0037158597260713577\n",
            "Iteration 3250/6000, G loss: 0.44502580165863037, D loss: 0.00014720586477778852\n",
            "Iteration 3251/6000, G loss: 0.5172286629676819, D loss: 0.016735253855586052\n",
            "Iteration 3252/6000, G loss: 0.4874782860279083, D loss: 0.0014637710992246866\n",
            "Iteration 3253/6000, G loss: 0.5571874380111694, D loss: 0.010983963496983051\n",
            "Iteration 3254/6000, G loss: 0.5973747968673706, D loss: 0.005925677716732025\n",
            "Iteration 3255/6000, G loss: 0.6638533473014832, D loss: 0.07463987171649933\n",
            "Iteration 3256/6000, G loss: 0.4315846562385559, D loss: 0.040989041328430176\n",
            "Iteration 3257/6000, G loss: 0.613612711429596, D loss: 0.0072782887145876884\n",
            "Iteration 3258/6000, G loss: 0.6444193720817566, D loss: 0.0217684768140316\n",
            "Iteration 3259/6000, G loss: 0.5287029147148132, D loss: 0.0013951221480965614\n",
            "Iteration 3260/6000, G loss: 0.5419027805328369, D loss: 0.0061643607914447784\n",
            "Iteration 3261/6000, G loss: 0.528633177280426, D loss: 0.011949747800827026\n",
            "Iteration 3262/6000, G loss: 0.45468685030937195, D loss: 0.005406730342656374\n",
            "Iteration 3263/6000, G loss: 0.5835265517234802, D loss: 0.03749274089932442\n",
            "Iteration 3264/6000, G loss: 0.5267833471298218, D loss: 0.004575541242957115\n",
            "Iteration 3265/6000, G loss: 0.5394976735115051, D loss: 0.005063442047685385\n",
            "Iteration 3266/6000, G loss: 0.46064335107803345, D loss: 0.002424566075205803\n",
            "Iteration 3267/6000, G loss: 0.6085646748542786, D loss: 0.002780016977339983\n",
            "Iteration 3268/6000, G loss: 0.5545275211334229, D loss: 0.0032252203673124313\n",
            "Iteration 3269/6000, G loss: 0.5834226012229919, D loss: 0.006903349421918392\n",
            "Iteration 3270/6000, G loss: 0.47130584716796875, D loss: 0.0008614865364506841\n",
            "Iteration 3271/6000, G loss: 0.5101393461227417, D loss: 0.005007100757211447\n",
            "Iteration 3272/6000, G loss: 0.49571165442466736, D loss: 0.000223861716222018\n",
            "Iteration 3273/6000, G loss: 0.6024590134620667, D loss: 0.0019899040926247835\n",
            "Iteration 3274/6000, G loss: 0.37437084317207336, D loss: 0.01557639054954052\n",
            "Iteration 3275/6000, G loss: 0.662240743637085, D loss: 0.0211953055113554\n",
            "Iteration 3276/6000, G loss: 0.40329816937446594, D loss: 0.006289588287472725\n",
            "Iteration 3277/6000, G loss: 0.4896206259727478, D loss: 0.0005702465423382819\n",
            "Iteration 3278/6000, G loss: 0.44563671946525574, D loss: 0.0016985820839181542\n",
            "Iteration 3279/6000, G loss: 0.5671003460884094, D loss: 0.0024462982546538115\n",
            "Iteration 3280/6000, G loss: 0.5123154520988464, D loss: 0.016177788376808167\n",
            "Iteration 3281/6000, G loss: 0.5051448941230774, D loss: 0.017482683062553406\n",
            "Iteration 3282/6000, G loss: 0.5419305562973022, D loss: 0.06084675341844559\n",
            "Iteration 3283/6000, G loss: 0.47338879108428955, D loss: 0.0004319854488130659\n",
            "Iteration 3284/6000, G loss: 0.6110116839408875, D loss: 0.0027719708159565926\n",
            "Iteration 3285/6000, G loss: 0.5470942854881287, D loss: 0.14582006633281708\n",
            "Iteration 3286/6000, G loss: 0.5435603260993958, D loss: 0.0015035845572128892\n",
            "Iteration 3287/6000, G loss: 0.39335158467292786, D loss: 0.04553234204649925\n",
            "Iteration 3288/6000, G loss: 0.5407304763793945, D loss: 0.003402864094823599\n",
            "Iteration 3289/6000, G loss: 0.4814826250076294, D loss: 0.0004494996683206409\n",
            "Iteration 3290/6000, G loss: 0.5595172643661499, D loss: 0.520625114440918\n",
            "Iteration 3291/6000, G loss: 0.5628902912139893, D loss: 0.000371975707821548\n",
            "Iteration 3292/6000, G loss: 0.5916089415550232, D loss: 0.0005039325915277004\n",
            "Iteration 3293/6000, G loss: 0.4285564720630646, D loss: 0.07729805260896683\n",
            "Iteration 3294/6000, G loss: 0.4579523205757141, D loss: 0.0062854415737092495\n",
            "Iteration 3295/6000, G loss: 0.4524887204170227, D loss: 0.0026531461626291275\n",
            "Iteration 3296/6000, G loss: 0.4945945143699646, D loss: 8.802584488876164e-05\n",
            "Iteration 3297/6000, G loss: 0.6100884079933167, D loss: 0.00427178293466568\n",
            "Iteration 3298/6000, G loss: 0.6147258877754211, D loss: 7.3015489761019126e-06\n",
            "Iteration 3299/6000, G loss: 0.49681708216667175, D loss: 4.539404471870512e-05\n",
            "Iteration 3300/6000, G loss: 0.5466607809066772, D loss: 0.00020809401758015156\n",
            "Iteration 3301/6000, G loss: 0.5081268548965454, D loss: 0.0010054270969703794\n",
            "Iteration 3302/6000, G loss: 0.5321742296218872, D loss: 0.0007799929589964449\n",
            "Iteration 3303/6000, G loss: 0.41883596777915955, D loss: 0.0049051991663873196\n",
            "Iteration 3304/6000, G loss: 0.6503297090530396, D loss: 2.3865310140536167e-05\n",
            "Iteration 3305/6000, G loss: 0.4398970901966095, D loss: 0.001457365695387125\n",
            "Iteration 3306/6000, G loss: 0.5735206604003906, D loss: 0.00019895410514436662\n",
            "Iteration 3307/6000, G loss: 0.5038508176803589, D loss: 1.5467318007722497e-05\n",
            "Iteration 3308/6000, G loss: 0.5434640645980835, D loss: 0.0005851001478731632\n",
            "Iteration 3309/6000, G loss: 0.564831018447876, D loss: 0.0037457761354744434\n",
            "Iteration 3310/6000, G loss: 0.543708860874176, D loss: 0.005694178864359856\n",
            "Iteration 3311/6000, G loss: 0.6307294368743896, D loss: 0.0073029231280088425\n",
            "Iteration 3312/6000, G loss: 0.5596415996551514, D loss: 0.00010191955516347662\n",
            "Iteration 3313/6000, G loss: 0.48099425435066223, D loss: 0.0008363309316337109\n",
            "Iteration 3314/6000, G loss: 0.6606225967407227, D loss: 0.0003464635810814798\n",
            "Iteration 3315/6000, G loss: 0.4551946222782135, D loss: 0.00037653016624972224\n",
            "Iteration 3316/6000, G loss: 0.5055578947067261, D loss: 0.00012205295206513256\n",
            "Iteration 3317/6000, G loss: 0.5848694443702698, D loss: 2.5629995548115403e-07\n",
            "Iteration 3318/6000, G loss: 0.6635995507240295, D loss: 6.191807915456593e-05\n",
            "Iteration 3319/6000, G loss: 0.5056447386741638, D loss: 0.003177303820848465\n",
            "Iteration 3320/6000, G loss: 0.5839635133743286, D loss: 0.0003750119940377772\n",
            "Iteration 3321/6000, G loss: 0.38672375679016113, D loss: 0.006545010954141617\n",
            "Iteration 3322/6000, G loss: 0.5168404579162598, D loss: 9.448769560549408e-05\n",
            "Iteration 3323/6000, G loss: 0.608475923538208, D loss: 0.27076151967048645\n",
            "Iteration 3324/6000, G loss: 0.5466770529747009, D loss: 0.0005029115709476173\n",
            "Iteration 3325/6000, G loss: 0.5624122619628906, D loss: 0.00010050144192064181\n",
            "Iteration 3326/6000, G loss: 0.6194214820861816, D loss: 0.0003084090421907604\n",
            "Iteration 3327/6000, G loss: 0.4249253273010254, D loss: 5.1300714403623715e-05\n",
            "Iteration 3328/6000, G loss: 0.4889604449272156, D loss: 0.0003454005054663867\n",
            "Iteration 3329/6000, G loss: 0.5975066423416138, D loss: 4.814611020265147e-05\n",
            "Iteration 3330/6000, G loss: 0.5855180025100708, D loss: 0.0006059866864234209\n",
            "Iteration 3331/6000, G loss: 0.49994513392448425, D loss: 0.020549137145280838\n",
            "Iteration 3332/6000, G loss: 0.5014856457710266, D loss: 0.0015998738817870617\n",
            "Iteration 3333/6000, G loss: 0.4335886836051941, D loss: 0.0003602333599701524\n",
            "Iteration 3334/6000, G loss: 0.6209048628807068, D loss: 0.005856090225279331\n",
            "Iteration 3335/6000, G loss: 0.5155996084213257, D loss: 0.030395040288567543\n",
            "Iteration 3336/6000, G loss: 0.5236201286315918, D loss: 1.4853281754767522e-05\n",
            "Iteration 3337/6000, G loss: 0.5974348783493042, D loss: 0.0001420456392224878\n",
            "Iteration 3338/6000, G loss: 0.5767641663551331, D loss: 0.00028502108762040734\n",
            "Iteration 3339/6000, G loss: 0.6482276916503906, D loss: 0.6896988749504089\n",
            "Iteration 3340/6000, G loss: 0.49624496698379517, D loss: 0.06016727164387703\n",
            "Iteration 3341/6000, G loss: 0.6179437041282654, D loss: 0.10001853108406067\n",
            "Iteration 3342/6000, G loss: 0.6293869018554688, D loss: 6.781936826882884e-05\n",
            "Iteration 3343/6000, G loss: 0.509537398815155, D loss: 0.0013625817373394966\n",
            "Iteration 3344/6000, G loss: 0.5255113840103149, D loss: 0.00027858471730723977\n",
            "Iteration 3345/6000, G loss: 0.5360984206199646, D loss: 1.8232953152619302e-05\n",
            "Iteration 3346/6000, G loss: 0.5239728689193726, D loss: 4.138226509094238\n",
            "Iteration 3347/6000, G loss: 0.43609416484832764, D loss: 0.0010045566596090794\n",
            "Iteration 3348/6000, G loss: 0.6427000164985657, D loss: 0.0010907896794378757\n",
            "Iteration 3349/6000, G loss: 0.6089338660240173, D loss: 0.00023046083515509963\n",
            "Iteration 3350/6000, G loss: 0.5047386288642883, D loss: 0.00019308296032249928\n",
            "Iteration 3351/6000, G loss: 0.5813348889350891, D loss: 0.002690567634999752\n",
            "Iteration 3352/6000, G loss: 0.4501475393772125, D loss: 0.0008011525496840477\n",
            "Iteration 3353/6000, G loss: 0.6742706298828125, D loss: 0.05297466367483139\n",
            "Iteration 3354/6000, G loss: 0.49692249298095703, D loss: 0.02674049884080887\n",
            "Iteration 3355/6000, G loss: 0.6553645730018616, D loss: 0.02177024632692337\n",
            "Iteration 3356/6000, G loss: 0.45006880164146423, D loss: 0.029155608266592026\n",
            "Iteration 3357/6000, G loss: 0.4488743841648102, D loss: 0.0565350316464901\n",
            "Iteration 3358/6000, G loss: 0.4482026994228363, D loss: 0.06742813438177109\n",
            "Iteration 3359/6000, G loss: 0.5510749220848083, D loss: 0.0044363802298903465\n",
            "Iteration 3360/6000, G loss: 0.581426739692688, D loss: 0.017528517171740532\n",
            "Iteration 3361/6000, G loss: 0.5877235531806946, D loss: 0.007033408619463444\n",
            "Iteration 3362/6000, G loss: 0.47863054275512695, D loss: 0.002755393274128437\n",
            "Iteration 3363/6000, G loss: 0.6476954221725464, D loss: 0.010830981656908989\n",
            "Iteration 3364/6000, G loss: 0.49845799803733826, D loss: 0.025163589045405388\n",
            "Iteration 3365/6000, G loss: 0.44304507970809937, D loss: 0.03567500039935112\n",
            "Iteration 3366/6000, G loss: 0.5611159205436707, D loss: 0.008999192155897617\n",
            "Iteration 3367/6000, G loss: 0.5470918416976929, D loss: 0.010133758187294006\n",
            "Iteration 3368/6000, G loss: 0.6002238392829895, D loss: 0.016109585762023926\n",
            "Iteration 3369/6000, G loss: 0.5438551902770996, D loss: 0.020100528374314308\n",
            "Iteration 3370/6000, G loss: 0.503090500831604, D loss: 0.0006322094704955816\n",
            "Iteration 3371/6000, G loss: 0.5720484256744385, D loss: 0.002570274518802762\n",
            "Iteration 3372/6000, G loss: 0.5842974781990051, D loss: 0.000883198925293982\n",
            "Iteration 3373/6000, G loss: 0.7334461808204651, D loss: 0.006862238515168428\n",
            "Iteration 3374/6000, G loss: 0.5021815299987793, D loss: 0.0023999095428735018\n",
            "Iteration 3375/6000, G loss: 0.5406419634819031, D loss: 0.00027772889006882906\n",
            "Iteration 3376/6000, G loss: 0.439279168844223, D loss: 0.012693339958786964\n",
            "Iteration 3377/6000, G loss: 0.5351296067237854, D loss: 0.008488290011882782\n",
            "Iteration 3378/6000, G loss: 0.4427177608013153, D loss: 0.0002496343804523349\n",
            "Iteration 3379/6000, G loss: 0.535241425037384, D loss: 0.0009174146689474583\n",
            "Iteration 3380/6000, G loss: 0.5462430119514465, D loss: 0.0011090969201177359\n",
            "Iteration 3381/6000, G loss: 0.5605669617652893, D loss: 0.0036850767210125923\n",
            "Iteration 3382/6000, G loss: 0.5264135599136353, D loss: 0.0003714001504704356\n",
            "Iteration 3383/6000, G loss: 0.49931633472442627, D loss: 0.008989620953798294\n",
            "Iteration 3384/6000, G loss: 0.5561634302139282, D loss: 0.0013801043387502432\n",
            "Iteration 3385/6000, G loss: 0.45183703303337097, D loss: 0.003238969948142767\n",
            "Iteration 3386/6000, G loss: 0.5580260157585144, D loss: 0.0014490657486021519\n",
            "Iteration 3387/6000, G loss: 0.4861096143722534, D loss: 0.003518300596624613\n",
            "Iteration 3388/6000, G loss: 0.5772974491119385, D loss: 0.019530311226844788\n",
            "Iteration 3389/6000, G loss: 0.654731035232544, D loss: 0.012224864214658737\n",
            "Iteration 3390/6000, G loss: 0.4767211079597473, D loss: 0.0006505409255623817\n",
            "Iteration 3391/6000, G loss: 0.521248996257782, D loss: 0.005004002247005701\n",
            "Iteration 3392/6000, G loss: 0.5208995938301086, D loss: 0.0013556820340454578\n",
            "Iteration 3393/6000, G loss: 0.589867889881134, D loss: 0.027797291055321693\n",
            "Iteration 3394/6000, G loss: 0.5101578235626221, D loss: 6.175004273245577e-06\n",
            "Iteration 3395/6000, G loss: 0.5594906806945801, D loss: 0.00029486004495993257\n",
            "Iteration 3396/6000, G loss: 0.39018887281417847, D loss: 0.08460773527622223\n",
            "Iteration 3397/6000, G loss: 0.5268516540527344, D loss: 0.0008668381487950683\n",
            "Iteration 3398/6000, G loss: 0.5580437779426575, D loss: 0.007347158621996641\n",
            "Iteration 3399/6000, G loss: 0.5790197849273682, D loss: 0.006020506843924522\n",
            "Iteration 3400/6000, G loss: 0.5589079260826111, D loss: 0.011268148198723793\n",
            "Iteration 3401/6000, G loss: 0.5265594720840454, D loss: 0.11821350455284119\n",
            "Iteration 3402/6000, G loss: 0.49007880687713623, D loss: 0.0030487366020679474\n",
            "Iteration 3403/6000, G loss: 0.5539680123329163, D loss: 4.0858354623196647e-05\n",
            "Iteration 3404/6000, G loss: 0.5383990406990051, D loss: 0.04752447083592415\n",
            "Iteration 3405/6000, G loss: 0.5261774063110352, D loss: 0.00021042115986347198\n",
            "Iteration 3406/6000, G loss: 0.5309053659439087, D loss: 0.00520112132653594\n",
            "Iteration 3407/6000, G loss: 0.5141779184341431, D loss: 0.002043845597654581\n",
            "Iteration 3408/6000, G loss: 0.47182387113571167, D loss: 0.0025054276920855045\n",
            "Iteration 3409/6000, G loss: 0.4791184067726135, D loss: 0.12329362332820892\n",
            "Iteration 3410/6000, G loss: 0.6228181719779968, D loss: 0.03886548429727554\n",
            "Iteration 3411/6000, G loss: 0.602473258972168, D loss: 0.006504567340016365\n",
            "Iteration 3412/6000, G loss: 0.5776649713516235, D loss: 0.003336124587804079\n",
            "Iteration 3413/6000, G loss: 0.5612934827804565, D loss: 0.08221422135829926\n",
            "Iteration 3414/6000, G loss: 0.5011194348335266, D loss: 0.08711442351341248\n",
            "Iteration 3415/6000, G loss: 0.409584641456604, D loss: 0.018825378268957138\n",
            "Iteration 3416/6000, G loss: 0.4815184473991394, D loss: 5.933874130249023\n",
            "Iteration 3417/6000, G loss: 0.5729520916938782, D loss: 0.012602478265762329\n",
            "Iteration 3418/6000, G loss: 0.4524567425251007, D loss: 0.00015066200285218656\n",
            "Iteration 3419/6000, G loss: 0.4390893876552582, D loss: 0.11768684536218643\n",
            "Iteration 3420/6000, G loss: 0.5138707756996155, D loss: 0.01665556989610195\n",
            "Iteration 3421/6000, G loss: 0.5278852581977844, D loss: 0.013957548886537552\n",
            "Iteration 3422/6000, G loss: 0.6093859672546387, D loss: 0.046657294034957886\n",
            "Iteration 3423/6000, G loss: 0.47998085618019104, D loss: 0.07233268767595291\n",
            "Iteration 3424/6000, G loss: 0.5997461080551147, D loss: 0.09448333829641342\n",
            "Iteration 3425/6000, G loss: 0.5394856333732605, D loss: 0.034571900963783264\n",
            "Iteration 3426/6000, G loss: 0.4745906591415405, D loss: 0.031381018459796906\n",
            "Iteration 3427/6000, G loss: 0.4877752363681793, D loss: 0.041753243654966354\n",
            "Iteration 3428/6000, G loss: 0.4953092038631439, D loss: 0.05682714283466339\n",
            "Iteration 3429/6000, G loss: 0.4483833611011505, D loss: 0.009671535342931747\n",
            "Iteration 3430/6000, G loss: 0.5709624886512756, D loss: 0.07618845999240875\n",
            "Iteration 3431/6000, G loss: 0.5570360422134399, D loss: 0.021855104714632034\n",
            "Iteration 3432/6000, G loss: 0.43343856930732727, D loss: 0.02877170965075493\n",
            "Iteration 3433/6000, G loss: 0.6950752139091492, D loss: 0.036048874258995056\n",
            "Iteration 3434/6000, G loss: 0.45301780104637146, D loss: 0.015343435108661652\n",
            "Iteration 3435/6000, G loss: 0.509032666683197, D loss: 0.016674451529979706\n",
            "Iteration 3436/6000, G loss: 0.5253534913063049, D loss: 0.00894903764128685\n",
            "Iteration 3437/6000, G loss: 0.479844868183136, D loss: 0.007661213167011738\n",
            "Iteration 3438/6000, G loss: 0.5845579504966736, D loss: 0.13793635368347168\n",
            "Iteration 3439/6000, G loss: 0.5778672099113464, D loss: 0.0031533194705843925\n",
            "Iteration 3440/6000, G loss: 0.5651133060455322, D loss: 0.00859006680548191\n",
            "Iteration 3441/6000, G loss: 0.5283634662628174, D loss: 0.014324147254228592\n",
            "Iteration 3442/6000, G loss: 0.5335731506347656, D loss: 0.009695594199001789\n",
            "Iteration 3443/6000, G loss: 0.47227412462234497, D loss: 0.0036379355005919933\n",
            "Iteration 3444/6000, G loss: 0.5613595843315125, D loss: 0.0015530847012996674\n",
            "Iteration 3445/6000, G loss: 0.5255844593048096, D loss: 0.01000470295548439\n",
            "Iteration 3446/6000, G loss: 0.49232879281044006, D loss: 0.002307314658537507\n",
            "Iteration 3447/6000, G loss: 0.5716245174407959, D loss: 0.004589805845171213\n",
            "Iteration 3448/6000, G loss: 0.3907953202724457, D loss: 0.0009900887962430716\n",
            "Iteration 3449/6000, G loss: 0.5134835839271545, D loss: 0.025045637041330338\n",
            "Iteration 3450/6000, G loss: 0.5295311808586121, D loss: 0.009911945089697838\n",
            "Iteration 3451/6000, G loss: 0.5677182078361511, D loss: 0.00035313694388605654\n",
            "Iteration 3452/6000, G loss: 0.44304463267326355, D loss: 0.0026369085535407066\n",
            "Iteration 3453/6000, G loss: 0.5477302074432373, D loss: 0.000728570215869695\n",
            "Iteration 3454/6000, G loss: 0.6110783815383911, D loss: 0.0017744903452694416\n",
            "Iteration 3455/6000, G loss: 0.5274289846420288, D loss: 0.0418785884976387\n",
            "Iteration 3456/6000, G loss: 0.46763139963150024, D loss: 0.000966086401604116\n",
            "Iteration 3457/6000, G loss: 0.5802175998687744, D loss: 0.003468365641310811\n",
            "Iteration 3458/6000, G loss: 0.4826318025588989, D loss: 0.0015163698699325323\n",
            "Iteration 3459/6000, G loss: 0.5211436748504639, D loss: 0.0013446671655401587\n",
            "Iteration 3460/6000, G loss: 0.4804832935333252, D loss: 1.5699766663601622e-05\n",
            "Iteration 3461/6000, G loss: 0.5301949977874756, D loss: 0.00028350393404252827\n",
            "Iteration 3462/6000, G loss: 0.4573013484477997, D loss: 0.006054116412997246\n",
            "Iteration 3463/6000, G loss: 0.44507983326911926, D loss: 0.0026170159690082073\n",
            "Iteration 3464/6000, G loss: 0.5016569495201111, D loss: 0.005523059982806444\n",
            "Iteration 3465/6000, G loss: 0.5424661040306091, D loss: 0.0009397656540386379\n",
            "Iteration 3466/6000, G loss: 0.47057804465293884, D loss: 0.00097756483592093\n",
            "Iteration 3467/6000, G loss: 0.5764148235321045, D loss: 0.022237636148929596\n",
            "Iteration 3468/6000, G loss: 0.44578230381011963, D loss: 0.0023041320964694023\n",
            "Iteration 3469/6000, G loss: 0.5312700867652893, D loss: 0.00422912510111928\n",
            "Iteration 3470/6000, G loss: 0.5143727660179138, D loss: 0.0005290232948027551\n",
            "Iteration 3471/6000, G loss: 0.5168712139129639, D loss: 0.004361880011856556\n",
            "Iteration 3472/6000, G loss: 0.5018417239189148, D loss: 0.015774976462125778\n",
            "Iteration 3473/6000, G loss: 0.5447685122489929, D loss: 0.015349898487329483\n",
            "Iteration 3474/6000, G loss: 0.5375893712043762, D loss: 0.006087981164455414\n",
            "Iteration 3475/6000, G loss: 0.5888550281524658, D loss: 0.0004661607090383768\n",
            "Iteration 3476/6000, G loss: 0.6234822273254395, D loss: 0.003994378261268139\n",
            "Iteration 3477/6000, G loss: 0.5225428938865662, D loss: 9.862416482064873e-05\n",
            "Iteration 3478/6000, G loss: 0.4619949162006378, D loss: 0.36339011788368225\n",
            "Iteration 3479/6000, G loss: 0.5104283690452576, D loss: 0.011504918336868286\n",
            "Iteration 3480/6000, G loss: 0.6110121011734009, D loss: 0.003907228820025921\n",
            "Iteration 3481/6000, G loss: 0.5110962986946106, D loss: 0.03857474401593208\n",
            "Iteration 3482/6000, G loss: 0.518332302570343, D loss: 0.0009881667792797089\n",
            "Iteration 3483/6000, G loss: 0.6039997339248657, D loss: 0.0001692679652478546\n",
            "Iteration 3484/6000, G loss: 0.4833126962184906, D loss: 0.0003437540144659579\n",
            "Iteration 3485/6000, G loss: 0.5782001614570618, D loss: 0.07079267501831055\n",
            "Iteration 3486/6000, G loss: 0.5822579860687256, D loss: 0.0024545560590922832\n",
            "Iteration 3487/6000, G loss: 0.5322021245956421, D loss: 0.1916302591562271\n",
            "Iteration 3488/6000, G loss: 0.46496498584747314, D loss: 0.022694166749715805\n",
            "Iteration 3489/6000, G loss: 0.5451740622520447, D loss: 0.0010996032506227493\n",
            "Iteration 3490/6000, G loss: 0.5499056577682495, D loss: 0.7815083265304565\n",
            "Iteration 3491/6000, G loss: 0.5413941740989685, D loss: 0.0592954158782959\n",
            "Iteration 3492/6000, G loss: 0.562097430229187, D loss: 0.00012036073894705623\n",
            "Iteration 3493/6000, G loss: 0.41664737462997437, D loss: 0.005511487368494272\n",
            "Iteration 3494/6000, G loss: 0.4235056936740875, D loss: 0.0011456571519374847\n",
            "Iteration 3495/6000, G loss: 0.4350144565105438, D loss: 0.0019078145269304514\n",
            "Iteration 3496/6000, G loss: 0.5064721703529358, D loss: 0.005964946933090687\n",
            "Iteration 3497/6000, G loss: 0.3760738968849182, D loss: 0.04940120130777359\n",
            "Iteration 3498/6000, G loss: 0.5862700343132019, D loss: 0.6462011933326721\n",
            "Iteration 3499/6000, G loss: 0.5215000510215759, D loss: 0.00034468548255972564\n",
            "Iteration 3500/6000, G loss: 0.5269886255264282, D loss: 0.0003500780730973929\n",
            "Iteration 3501/6000, G loss: 0.6388024091720581, D loss: 0.000310019328026101\n",
            "Iteration 3502/6000, G loss: 0.6545042991638184, D loss: 0.006438640411943197\n",
            "Iteration 3503/6000, G loss: 0.5284231901168823, D loss: 0.0006600746419280767\n",
            "Iteration 3504/6000, G loss: 0.5137495994567871, D loss: 2.13980501939659e-06\n",
            "Iteration 3505/6000, G loss: 0.5437739491462708, D loss: 1.4501730220217723e-05\n",
            "Iteration 3506/6000, G loss: 0.5608164072036743, D loss: 0.003125459421426058\n",
            "Iteration 3507/6000, G loss: 0.5849609375, D loss: 0.28998735547065735\n",
            "Iteration 3508/6000, G loss: 0.5777286291122437, D loss: 0.00302963238209486\n",
            "Iteration 3509/6000, G loss: 0.43060678243637085, D loss: 0.06309916079044342\n",
            "Iteration 3510/6000, G loss: 0.5774304270744324, D loss: 4.821099281311035\n",
            "Iteration 3511/6000, G loss: 0.5360912084579468, D loss: 4.5299520934349857e-07\n",
            "Iteration 3512/6000, G loss: 0.4987611770629883, D loss: 2.3245790998771554e-06\n",
            "Iteration 3513/6000, G loss: 0.5263652801513672, D loss: 8.642671218694886e-07\n",
            "Iteration 3514/6000, G loss: 0.5541467666625977, D loss: 0.06887765973806381\n",
            "Iteration 3515/6000, G loss: 0.5622977018356323, D loss: 0.0022145998664200306\n",
            "Iteration 3516/6000, G loss: 0.4280591309070587, D loss: 0.009274918586015701\n",
            "Iteration 3517/6000, G loss: 0.4854491949081421, D loss: 0.001825464190915227\n",
            "Iteration 3518/6000, G loss: 0.5311871767044067, D loss: 0.01099142525345087\n",
            "Iteration 3519/6000, G loss: 0.5092172026634216, D loss: 0.009982425719499588\n",
            "Iteration 3520/6000, G loss: 0.6183090209960938, D loss: 0.07331226021051407\n",
            "Iteration 3521/6000, G loss: 0.5043090581893921, D loss: 0.029339395463466644\n",
            "Iteration 3522/6000, G loss: 0.5063580870628357, D loss: 0.1520458161830902\n",
            "Iteration 3523/6000, G loss: 0.489635169506073, D loss: 0.0530107282102108\n",
            "Iteration 3524/6000, G loss: 0.45166778564453125, D loss: 0.007705989293754101\n",
            "Iteration 3525/6000, G loss: 0.53705894947052, D loss: 0.023107443004846573\n",
            "Iteration 3526/6000, G loss: 0.5589436888694763, D loss: 0.040273070335388184\n",
            "Iteration 3527/6000, G loss: 0.5415121912956238, D loss: 0.0601084902882576\n",
            "Iteration 3528/6000, G loss: 0.4522162079811096, D loss: 0.029179850593209267\n",
            "Iteration 3529/6000, G loss: 0.6350653767585754, D loss: 0.005815867800265551\n",
            "Iteration 3530/6000, G loss: 0.6519709825515747, D loss: 0.04321844130754471\n",
            "Iteration 3531/6000, G loss: 0.3964047133922577, D loss: 0.002372428774833679\n",
            "Iteration 3532/6000, G loss: 0.5273553729057312, D loss: 0.014926698058843613\n",
            "Iteration 3533/6000, G loss: 0.5680207014083862, D loss: 0.0005850933957844973\n",
            "Iteration 3534/6000, G loss: 0.5728550553321838, D loss: 0.003972075413912535\n",
            "Iteration 3535/6000, G loss: 0.5396909713745117, D loss: 0.0003011923690792173\n",
            "Iteration 3536/6000, G loss: 0.48625603318214417, D loss: 0.0065651205368340015\n",
            "Iteration 3537/6000, G loss: 0.4719049334526062, D loss: 0.010729240253567696\n",
            "Iteration 3538/6000, G loss: 0.5011950135231018, D loss: 0.00717204250395298\n",
            "Iteration 3539/6000, G loss: 0.4483411908149719, D loss: 0.025891784578561783\n",
            "Iteration 3540/6000, G loss: 0.5827822089195251, D loss: 0.014234675094485283\n",
            "Iteration 3541/6000, G loss: 0.48455068469047546, D loss: 0.003020593896508217\n",
            "Iteration 3542/6000, G loss: 0.5812051892280579, D loss: 0.0024209115654230118\n",
            "Iteration 3543/6000, G loss: 0.5302331447601318, D loss: 0.006466572172939777\n",
            "Iteration 3544/6000, G loss: 0.5113618969917297, D loss: 0.0003673216560855508\n",
            "Iteration 3545/6000, G loss: 0.40070828795433044, D loss: 0.0017359234625473619\n",
            "Iteration 3546/6000, G loss: 0.6058410406112671, D loss: 0.0048889536410570145\n",
            "Iteration 3547/6000, G loss: 0.5677407383918762, D loss: 0.009306814521551132\n",
            "Iteration 3548/6000, G loss: 0.6458612680435181, D loss: 0.0074108378030359745\n",
            "Iteration 3549/6000, G loss: 0.5492154955863953, D loss: 0.0027993549592792988\n",
            "Iteration 3550/6000, G loss: 0.5427770018577576, D loss: 3.9845177525421605e-05\n",
            "Iteration 3551/6000, G loss: 0.5005667805671692, D loss: 0.0008726139785721898\n",
            "Iteration 3552/6000, G loss: 0.4449974000453949, D loss: 0.0023918049409985542\n",
            "Iteration 3553/6000, G loss: 0.44196394085884094, D loss: 0.02389071136713028\n",
            "Iteration 3554/6000, G loss: 0.47051864862442017, D loss: 0.0014206590130925179\n",
            "Iteration 3555/6000, G loss: 0.48689767718315125, D loss: 0.27005356550216675\n",
            "Iteration 3556/6000, G loss: 0.45692646503448486, D loss: 0.0014282024931162596\n",
            "Iteration 3557/6000, G loss: 0.4848821759223938, D loss: 0.0004852609126828611\n",
            "Iteration 3558/6000, G loss: 0.5801417827606201, D loss: 0.02588335983455181\n",
            "Iteration 3559/6000, G loss: 0.39261847734451294, D loss: 3.2817930332385004e-05\n",
            "Iteration 3560/6000, G loss: 0.6002378463745117, D loss: 7.224493310786784e-05\n",
            "Iteration 3561/6000, G loss: 0.4722118079662323, D loss: 4.078655183548108e-05\n",
            "Iteration 3562/6000, G loss: 0.5159170031547546, D loss: 0.0002471969637554139\n",
            "Iteration 3563/6000, G loss: 0.5011399984359741, D loss: 0.013002857565879822\n",
            "Iteration 3564/6000, G loss: 0.5877054333686829, D loss: 0.04322690889239311\n",
            "Iteration 3565/6000, G loss: 0.5829162001609802, D loss: 0.20556823909282684\n",
            "Iteration 3566/6000, G loss: 0.4844517111778259, D loss: 0.042966607958078384\n",
            "Iteration 3567/6000, G loss: 0.498666912317276, D loss: 4.728353815153241e-05\n",
            "Iteration 3568/6000, G loss: 0.5545817613601685, D loss: 0.00393321830779314\n",
            "Iteration 3569/6000, G loss: 0.5354241728782654, D loss: 0.0006910562515258789\n",
            "Iteration 3570/6000, G loss: 0.5630239248275757, D loss: 0.0025136307813227177\n",
            "Iteration 3571/6000, G loss: 0.422526091337204, D loss: 0.02339131012558937\n",
            "Iteration 3572/6000, G loss: 0.4674531817436218, D loss: 0.00040589147829450667\n",
            "Iteration 3573/6000, G loss: 0.5018506050109863, D loss: 0.004311955068260431\n",
            "Iteration 3574/6000, G loss: 0.5356562733650208, D loss: 0.0031944538932293653\n",
            "Iteration 3575/6000, G loss: 0.6178430318832397, D loss: 0.11971527338027954\n",
            "Iteration 3576/6000, G loss: 0.4285193383693695, D loss: 0.03082425333559513\n",
            "Iteration 3577/6000, G loss: 0.5337674617767334, D loss: 0.005571859888732433\n",
            "Iteration 3578/6000, G loss: 0.467000812292099, D loss: 0.0036115311086177826\n",
            "Iteration 3579/6000, G loss: 0.5191136598587036, D loss: 0.004109326750040054\n",
            "Iteration 3580/6000, G loss: 0.4579131603240967, D loss: 0.00035852857399731874\n",
            "Iteration 3581/6000, G loss: 0.5689530372619629, D loss: 0.07719843089580536\n",
            "Iteration 3582/6000, G loss: 0.5788158178329468, D loss: 0.011837366037070751\n",
            "Iteration 3583/6000, G loss: 0.4306391179561615, D loss: 0.0809401348233223\n",
            "Iteration 3584/6000, G loss: 0.45053744316101074, D loss: 5.5183844566345215\n",
            "Iteration 3585/6000, G loss: 0.5200695395469666, D loss: 0.06793075799942017\n",
            "Iteration 3586/6000, G loss: 0.48736050724983215, D loss: 0.053456131368875504\n",
            "Iteration 3587/6000, G loss: 0.5036495923995972, D loss: 0.00933036208152771\n",
            "Iteration 3588/6000, G loss: 0.5858210921287537, D loss: 0.015493588522076607\n",
            "Iteration 3589/6000, G loss: 0.5435582399368286, D loss: 0.0019004482310265303\n",
            "Iteration 3590/6000, G loss: 0.4808337688446045, D loss: 0.0015406857710331678\n",
            "Iteration 3591/6000, G loss: 0.4826008677482605, D loss: 0.002148432657122612\n",
            "Iteration 3592/6000, G loss: 0.48257091641426086, D loss: 0.0011053308844566345\n",
            "Iteration 3593/6000, G loss: 0.5059422254562378, D loss: 0.018038153648376465\n",
            "Iteration 3594/6000, G loss: 0.500056803226471, D loss: 0.1306866705417633\n",
            "Iteration 3595/6000, G loss: 0.6794679760932922, D loss: 0.02267974242568016\n",
            "Iteration 3596/6000, G loss: 0.6590862274169922, D loss: 0.011938607320189476\n",
            "Iteration 3597/6000, G loss: 0.5246255993843079, D loss: 0.0029859784990549088\n",
            "Iteration 3598/6000, G loss: 0.5213923454284668, D loss: 0.004180941730737686\n",
            "Iteration 3599/6000, G loss: 0.4895758032798767, D loss: 0.011049206368625164\n",
            "Iteration 3600/6000, G loss: 0.5676145553588867, D loss: 0.012320192530751228\n",
            "Iteration 3601/6000, G loss: 0.5464234352111816, D loss: 0.02041478082537651\n",
            "Iteration 3602/6000, G loss: 0.534516453742981, D loss: 0.02244921587407589\n",
            "Iteration 3603/6000, G loss: 0.5284890532493591, D loss: 0.011805864050984383\n",
            "Iteration 3604/6000, G loss: 0.6340787410736084, D loss: 0.0792047381401062\n",
            "Iteration 3605/6000, G loss: 0.5405628085136414, D loss: 0.002317806240171194\n",
            "Iteration 3606/6000, G loss: 0.44430679082870483, D loss: 0.0034980797208845615\n",
            "Iteration 3607/6000, G loss: 0.5239790081977844, D loss: 0.0029365145601332188\n",
            "Iteration 3608/6000, G loss: 0.452024906873703, D loss: 0.006504614371806383\n",
            "Iteration 3609/6000, G loss: 0.4811931848526001, D loss: 0.002384265884757042\n",
            "Iteration 3610/6000, G loss: 0.4376906454563141, D loss: 0.0058763595297932625\n",
            "Iteration 3611/6000, G loss: 0.4736540615558624, D loss: 0.0017626220360398293\n",
            "Iteration 3612/6000, G loss: 0.4826478660106659, D loss: 0.00031380646396428347\n",
            "Iteration 3613/6000, G loss: 0.4519936740398407, D loss: 0.0034873289987444878\n",
            "Iteration 3614/6000, G loss: 0.5619824528694153, D loss: 0.00029404560336843133\n",
            "Iteration 3615/6000, G loss: 0.5930773019790649, D loss: 0.00017980388656724244\n",
            "Iteration 3616/6000, G loss: 0.5069450736045837, D loss: 0.0024785827845335007\n",
            "Iteration 3617/6000, G loss: 0.48867976665496826, D loss: 0.0033605298958718777\n",
            "Iteration 3618/6000, G loss: 0.572102963924408, D loss: 0.0008979392005130649\n",
            "Iteration 3619/6000, G loss: 0.453581303358078, D loss: 0.027666350826621056\n",
            "Iteration 3620/6000, G loss: 0.5587276220321655, D loss: 0.008913668803870678\n",
            "Iteration 3621/6000, G loss: 0.6424241065979004, D loss: 0.006894312798976898\n",
            "Iteration 3622/6000, G loss: 0.49598073959350586, D loss: 0.0008847852586768568\n",
            "Iteration 3623/6000, G loss: 0.4678976833820343, D loss: 0.0001370669633615762\n",
            "Iteration 3624/6000, G loss: 0.5417901873588562, D loss: 0.00025372172240167856\n",
            "Iteration 3625/6000, G loss: 0.533523440361023, D loss: 0.0009695332264527678\n",
            "Iteration 3626/6000, G loss: 0.46535125374794006, D loss: 0.000897785066626966\n",
            "Iteration 3627/6000, G loss: 0.5274156928062439, D loss: 6.98134463164024e-05\n",
            "Iteration 3628/6000, G loss: 0.5693902373313904, D loss: 8.3593484305311e-05\n",
            "Iteration 3629/6000, G loss: 0.3707696795463562, D loss: 0.016056543216109276\n",
            "Iteration 3630/6000, G loss: 0.4889974892139435, D loss: 0.0016392136458307505\n",
            "Iteration 3631/6000, G loss: 0.6610361337661743, D loss: 0.013204379007220268\n",
            "Iteration 3632/6000, G loss: 0.5083730816841125, D loss: 0.00041163794230669737\n",
            "Iteration 3633/6000, G loss: 0.4682444930076599, D loss: 0.004041970707476139\n",
            "Iteration 3634/6000, G loss: 0.48131564259529114, D loss: 0.007138910703361034\n",
            "Iteration 3635/6000, G loss: 0.527320384979248, D loss: 5.226649955147877e-05\n",
            "Iteration 3636/6000, G loss: 0.46230223774909973, D loss: 0.0007659366237930954\n",
            "Iteration 3637/6000, G loss: 0.5322994589805603, D loss: 0.0018411329947412014\n",
            "Iteration 3638/6000, G loss: 0.5158771872520447, D loss: 0.030357487499713898\n",
            "Iteration 3639/6000, G loss: 0.5523718595504761, D loss: 0.007483498193323612\n",
            "Iteration 3640/6000, G loss: 0.5131015181541443, D loss: 0.0004887957475148141\n",
            "Iteration 3641/6000, G loss: 0.5052245855331421, D loss: 0.0003043205360881984\n",
            "Iteration 3642/6000, G loss: 0.5659630298614502, D loss: 0.0001669097546255216\n",
            "Iteration 3643/6000, G loss: 0.4982377588748932, D loss: 0.08346568048000336\n",
            "Iteration 3644/6000, G loss: 0.6130474209785461, D loss: 0.09338712692260742\n",
            "Iteration 3645/6000, G loss: 0.5355364084243774, D loss: 0.0001434031582903117\n",
            "Iteration 3646/6000, G loss: 0.5567931532859802, D loss: 0.0008614574326202273\n",
            "Iteration 3647/6000, G loss: 0.4936455488204956, D loss: 0.010341253131628036\n",
            "Iteration 3648/6000, G loss: 0.5235145092010498, D loss: 0.0001100326917367056\n",
            "Iteration 3649/6000, G loss: 0.5593059062957764, D loss: 5.763759872934315e-06\n",
            "Iteration 3650/6000, G loss: 0.5938841104507446, D loss: 0.0007334863767027855\n",
            "Iteration 3651/6000, G loss: 0.5041304230690002, D loss: 0.0006257306085899472\n",
            "Iteration 3652/6000, G loss: 0.5461224913597107, D loss: 0.0007603575941175222\n",
            "Iteration 3653/6000, G loss: 0.5506207346916199, D loss: 0.00010528253915254027\n",
            "Iteration 3654/6000, G loss: 0.47815290093421936, D loss: 0.026484739035367966\n",
            "Iteration 3655/6000, G loss: 0.4076142907142639, D loss: 0.0005341431824490428\n",
            "Iteration 3656/6000, G loss: 0.4721507132053375, D loss: 0.0004532315651886165\n",
            "Iteration 3657/6000, G loss: 0.6173385381698608, D loss: 0.16564658284187317\n",
            "Iteration 3658/6000, G loss: 0.5664832592010498, D loss: 0.08078597486019135\n",
            "Iteration 3659/6000, G loss: 0.5033152103424072, D loss: 0.0034102764911949635\n",
            "Iteration 3660/6000, G loss: 0.4888634979724884, D loss: 0.0017924914136528969\n",
            "Iteration 3661/6000, G loss: 0.5371609330177307, D loss: 0.00034667691215872765\n",
            "Iteration 3662/6000, G loss: 0.4692039489746094, D loss: 0.00010929176642093807\n",
            "Iteration 3663/6000, G loss: 0.5883293747901917, D loss: 0.015763984993100166\n",
            "Iteration 3664/6000, G loss: 0.5276055335998535, D loss: 0.022650178521871567\n",
            "Iteration 3665/6000, G loss: 0.5324503183364868, D loss: 0.0029751553665846586\n",
            "Iteration 3666/6000, G loss: 0.49737992882728577, D loss: 0.2557981610298157\n",
            "Iteration 3667/6000, G loss: 0.6621466875076294, D loss: 5.336714744567871\n",
            "Iteration 3668/6000, G loss: 0.4617156684398651, D loss: 0.003242320381104946\n",
            "Iteration 3669/6000, G loss: 0.6211832165718079, D loss: 7.304386235773563e-05\n",
            "Iteration 3670/6000, G loss: 0.5869417786598206, D loss: 0.002244455274194479\n",
            "Iteration 3671/6000, G loss: 0.5483570098876953, D loss: 0.00531879672780633\n",
            "Iteration 3672/6000, G loss: 0.4731079638004303, D loss: 0.022748732939362526\n",
            "Iteration 3673/6000, G loss: 0.4784890115261078, D loss: 0.008913149125874043\n",
            "Iteration 3674/6000, G loss: 0.3805173635482788, D loss: 0.020200934261083603\n",
            "Iteration 3675/6000, G loss: 0.4484241306781769, D loss: 0.05162270739674568\n",
            "Iteration 3676/6000, G loss: 0.5312651991844177, D loss: 0.07816663384437561\n",
            "Iteration 3677/6000, G loss: 0.4128219187259674, D loss: 0.06220404803752899\n",
            "Iteration 3678/6000, G loss: 0.47866499423980713, D loss: 0.017225971445441246\n",
            "Iteration 3679/6000, G loss: 0.4309913218021393, D loss: 0.06211836263537407\n",
            "Iteration 3680/6000, G loss: 0.446786493062973, D loss: 0.059677086770534515\n",
            "Iteration 3681/6000, G loss: 0.6132994294166565, D loss: 0.12874919176101685\n",
            "Iteration 3682/6000, G loss: 0.5124907493591309, D loss: 0.3076170086860657\n",
            "Iteration 3683/6000, G loss: 0.5311939716339111, D loss: 0.08222219347953796\n",
            "Iteration 3684/6000, G loss: 0.4237050712108612, D loss: 0.10689278692007065\n",
            "Iteration 3685/6000, G loss: 0.5113258361816406, D loss: 0.08026739954948425\n",
            "Iteration 3686/6000, G loss: 0.5359575748443604, D loss: 0.02161216363310814\n",
            "Iteration 3687/6000, G loss: 0.5519424080848694, D loss: 0.024500761181116104\n",
            "Iteration 3688/6000, G loss: 0.6482865214347839, D loss: 0.016568036749958992\n",
            "Iteration 3689/6000, G loss: 0.4333847463130951, D loss: 0.007792326621711254\n",
            "Iteration 3690/6000, G loss: 0.47676312923431396, D loss: 0.03714315593242645\n",
            "Iteration 3691/6000, G loss: 0.5991965532302856, D loss: 0.015571263618767262\n",
            "Iteration 3692/6000, G loss: 0.5087186098098755, D loss: 0.013038181699812412\n",
            "Iteration 3693/6000, G loss: 0.507898211479187, D loss: 0.07245951145887375\n",
            "Iteration 3694/6000, G loss: 0.5108336210250854, D loss: 0.001962910406291485\n",
            "Iteration 3695/6000, G loss: 0.510543167591095, D loss: 0.004746372811496258\n",
            "Iteration 3696/6000, G loss: 0.4185999035835266, D loss: 0.024107975885272026\n",
            "Iteration 3697/6000, G loss: 0.5235854983329773, D loss: 0.0039642732590436935\n",
            "Iteration 3698/6000, G loss: 0.5187059640884399, D loss: 0.030477844178676605\n",
            "Iteration 3699/6000, G loss: 0.5020077228546143, D loss: 0.02630021795630455\n",
            "Iteration 3700/6000, G loss: 0.4663262367248535, D loss: 0.026608726009726524\n",
            "Iteration 3701/6000, G loss: 0.5675355195999146, D loss: 0.0019903203938156366\n",
            "Iteration 3702/6000, G loss: 0.5627028346061707, D loss: 0.00480789365246892\n",
            "Iteration 3703/6000, G loss: 0.54461669921875, D loss: 0.008858473971486092\n",
            "Iteration 3704/6000, G loss: 0.5095507502555847, D loss: 0.0037789512425661087\n",
            "Iteration 3705/6000, G loss: 0.5139501690864563, D loss: 0.0003003055171575397\n",
            "Iteration 3706/6000, G loss: 0.45988500118255615, D loss: 0.0008358308114111423\n",
            "Iteration 3707/6000, G loss: 0.5555338263511658, D loss: 0.0035271435044705868\n",
            "Iteration 3708/6000, G loss: 0.5340201258659363, D loss: 0.018626321107149124\n",
            "Iteration 3709/6000, G loss: 0.49009037017822266, D loss: 0.0012098761508241296\n",
            "Iteration 3710/6000, G loss: 0.44798046350479126, D loss: 0.0017344285733997822\n",
            "Iteration 3711/6000, G loss: 0.35823702812194824, D loss: 0.0022381525486707687\n",
            "Iteration 3712/6000, G loss: 0.5413979887962341, D loss: 0.06752447783946991\n",
            "Iteration 3713/6000, G loss: 0.4363907277584076, D loss: 0.0013874346623197198\n",
            "Iteration 3714/6000, G loss: 0.5044273734092712, D loss: 0.0014615351101383567\n",
            "Iteration 3715/6000, G loss: 0.5153707265853882, D loss: 0.0019079954363405704\n",
            "Iteration 3716/6000, G loss: 0.5627632737159729, D loss: 0.0012011396465823054\n",
            "Iteration 3717/6000, G loss: 0.5548626780509949, D loss: 0.008186574093997478\n",
            "Iteration 3718/6000, G loss: 0.5454100966453552, D loss: 0.020601259544491768\n",
            "Iteration 3719/6000, G loss: 0.43816354870796204, D loss: 0.002542856615036726\n",
            "Iteration 3720/6000, G loss: 0.5227384567260742, D loss: 0.0022718682885169983\n",
            "Iteration 3721/6000, G loss: 0.4322517216205597, D loss: 0.009746174328029156\n",
            "Iteration 3722/6000, G loss: 0.4501029849052429, D loss: 0.002747894264757633\n",
            "Iteration 3723/6000, G loss: 0.41891762614250183, D loss: 0.0009801497217267752\n",
            "Iteration 3724/6000, G loss: 0.4732818007469177, D loss: 0.003289208048954606\n",
            "Iteration 3725/6000, G loss: 0.5554367899894714, D loss: 0.00702426303178072\n",
            "Iteration 3726/6000, G loss: 0.42998117208480835, D loss: 0.003918158821761608\n",
            "Iteration 3727/6000, G loss: 0.47324317693710327, D loss: 0.09672968089580536\n",
            "Iteration 3728/6000, G loss: 0.4645628333091736, D loss: 0.19164307415485382\n",
            "Iteration 3729/6000, G loss: 0.6230359077453613, D loss: 0.06580042093992233\n",
            "Iteration 3730/6000, G loss: 0.5007100701332092, D loss: 0.009093831293284893\n",
            "Iteration 3731/6000, G loss: 0.5527514815330505, D loss: 0.0019782232120633125\n",
            "Iteration 3732/6000, G loss: 0.4411054253578186, D loss: 0.00282488577067852\n",
            "Iteration 3733/6000, G loss: 0.5994226932525635, D loss: 0.0002889213210437447\n",
            "Iteration 3734/6000, G loss: 0.5038245320320129, D loss: 2.0056701032444835e-05\n",
            "Iteration 3735/6000, G loss: 0.46639764308929443, D loss: 0.000373234914150089\n",
            "Iteration 3736/6000, G loss: 0.4656972587108612, D loss: 4.690796049544588e-05\n",
            "Iteration 3737/6000, G loss: 0.5170323848724365, D loss: 0.0011117643443867564\n",
            "Iteration 3738/6000, G loss: 0.5277149677276611, D loss: 0.0003438233397901058\n",
            "Iteration 3739/6000, G loss: 0.6414972543716431, D loss: 0.03535710275173187\n",
            "Iteration 3740/6000, G loss: 0.5599717497825623, D loss: 0.01657552644610405\n",
            "Iteration 3741/6000, G loss: 0.48350799083709717, D loss: 0.0026852719020098448\n",
            "Iteration 3742/6000, G loss: 0.540198028087616, D loss: 0.03748858720064163\n",
            "Iteration 3743/6000, G loss: 0.5050543546676636, D loss: 1.4316956367110834e-05\n",
            "Iteration 3744/6000, G loss: 0.48016008734703064, D loss: 7.2359862315352075e-06\n",
            "Iteration 3745/6000, G loss: 0.5397943258285522, D loss: 7.129566802177578e-05\n",
            "Iteration 3746/6000, G loss: 0.5004153847694397, D loss: 0.00011915592767763883\n",
            "Iteration 3747/6000, G loss: 0.5362182855606079, D loss: 0.0698469877243042\n",
            "Iteration 3748/6000, G loss: 0.4682871699333191, D loss: 0.001060029724612832\n",
            "Iteration 3749/6000, G loss: 0.6138757467269897, D loss: 0.30860036611557007\n",
            "Iteration 3750/6000, G loss: 0.5021117925643921, D loss: 0.0010594810592010617\n",
            "Iteration 3751/6000, G loss: 0.5180414319038391, D loss: 0.009575285948812962\n",
            "Iteration 3752/6000, G loss: 0.5105226039886475, D loss: 0.13786813616752625\n",
            "Iteration 3753/6000, G loss: 0.6280450820922852, D loss: 0.023341801017522812\n",
            "Iteration 3754/6000, G loss: 0.5280905961990356, D loss: 0.003227096050977707\n",
            "Iteration 3755/6000, G loss: 0.6148971319198608, D loss: 0.021638205274939537\n",
            "Iteration 3756/6000, G loss: 0.5994839668273926, D loss: 0.00027887121541425586\n",
            "Iteration 3757/6000, G loss: 0.5603932738304138, D loss: 0.08213883638381958\n",
            "Iteration 3758/6000, G loss: 0.4984043836593628, D loss: 0.2559274435043335\n",
            "Iteration 3759/6000, G loss: 0.6031076908111572, D loss: 0.2046377956867218\n",
            "Iteration 3760/6000, G loss: 0.4446098804473877, D loss: 0.002180115319788456\n",
            "Iteration 3761/6000, G loss: 0.4636744558811188, D loss: 2.391169309616089\n",
            "Iteration 3762/6000, G loss: 0.4424513876438141, D loss: 0.023551195859909058\n",
            "Iteration 3763/6000, G loss: 0.5277859568595886, D loss: 0.9447277188301086\n",
            "Iteration 3764/6000, G loss: 0.4873976409435272, D loss: 0.00148329371586442\n",
            "Iteration 3765/6000, G loss: 0.5054686069488525, D loss: 1.3303484593052417e-05\n",
            "Iteration 3766/6000, G loss: 0.443142294883728, D loss: 0.17907151579856873\n",
            "Iteration 3767/6000, G loss: 0.5589024424552917, D loss: 0.03456269949674606\n",
            "Iteration 3768/6000, G loss: 0.5229513645172119, D loss: 0.023016471415758133\n",
            "Iteration 3769/6000, G loss: 0.6140566468238831, D loss: 0.019732102751731873\n",
            "Iteration 3770/6000, G loss: 0.5855795741081238, D loss: 0.32324469089508057\n",
            "Iteration 3771/6000, G loss: 0.6005734801292419, D loss: 4.827938028029166e-06\n",
            "Iteration 3772/6000, G loss: 0.5341483354568481, D loss: 8.344649415903405e-08\n",
            "Iteration 3773/6000, G loss: 0.539877712726593, D loss: 0.00022698257816955447\n",
            "Iteration 3774/6000, G loss: 0.5098698139190674, D loss: 1.6319234418915585e-05\n",
            "Iteration 3775/6000, G loss: 0.5461255311965942, D loss: 0.0070863934233784676\n",
            "Iteration 3776/6000, G loss: 0.37592393159866333, D loss: 0.0036249570548534393\n",
            "Iteration 3777/6000, G loss: 0.6116177439689636, D loss: 0.1884523630142212\n",
            "Iteration 3778/6000, G loss: 0.4836539924144745, D loss: 3.0020502890693024e-05\n",
            "Iteration 3779/6000, G loss: 0.5467789173126221, D loss: 0.0008126005996018648\n",
            "Iteration 3780/6000, G loss: 0.5280508995056152, D loss: 0.03145577758550644\n",
            "Iteration 3781/6000, G loss: 0.4134763181209564, D loss: 0.20225447416305542\n",
            "Iteration 3782/6000, G loss: 0.51381915807724, D loss: 0.010938252322375774\n",
            "Iteration 3783/6000, G loss: 0.43037378787994385, D loss: 0.003988034091889858\n",
            "Iteration 3784/6000, G loss: 0.4339982569217682, D loss: 0.5395802855491638\n",
            "Iteration 3785/6000, G loss: 0.42314431071281433, D loss: 0.03583430498838425\n",
            "Iteration 3786/6000, G loss: 0.5823724269866943, D loss: 1.5018230676651\n",
            "Iteration 3787/6000, G loss: 0.5648270845413208, D loss: 0.007319358643144369\n",
            "Iteration 3788/6000, G loss: 0.46454423666000366, D loss: 0.006344219669699669\n",
            "Iteration 3789/6000, G loss: 0.478636771440506, D loss: 2.451141357421875\n",
            "Iteration 3790/6000, G loss: 0.5870113968849182, D loss: 2.8390893936157227\n",
            "Iteration 3791/6000, G loss: 0.6279009580612183, D loss: 6.661082443315536e-05\n",
            "Iteration 3792/6000, G loss: 0.5243414044380188, D loss: 8.635453559691086e-05\n",
            "Iteration 3793/6000, G loss: 0.5514250993728638, D loss: 0.0727355033159256\n",
            "Iteration 3794/6000, G loss: 0.6405361890792847, D loss: 1.7092087268829346\n",
            "Iteration 3795/6000, G loss: 0.6178508996963501, D loss: 0.00655028410255909\n",
            "Iteration 3796/6000, G loss: 0.5392002463340759, D loss: 0.05286934971809387\n",
            "Iteration 3797/6000, G loss: 0.6139972805976868, D loss: 0.05854348465800285\n",
            "Iteration 3798/6000, G loss: 0.5679911375045776, D loss: 0.00012575059372466058\n",
            "Iteration 3799/6000, G loss: 0.6000407934188843, D loss: 0.00019297808466944844\n",
            "Iteration 3800/6000, G loss: 0.3323218822479248, D loss: 0.41637030243873596\n",
            "Iteration 3801/6000, G loss: 0.5693624019622803, D loss: 0.000398208387196064\n",
            "Iteration 3802/6000, G loss: 0.47390973567962646, D loss: 6.368529284372926e-05\n",
            "Iteration 3803/6000, G loss: 0.6320510506629944, D loss: 0.01131268497556448\n",
            "Iteration 3804/6000, G loss: 0.5170062780380249, D loss: 0.03940533101558685\n",
            "Iteration 3805/6000, G loss: 0.5823235511779785, D loss: 0.9327789545059204\n",
            "Iteration 3806/6000, G loss: 0.45302140712738037, D loss: 4.7668023109436035\n",
            "Iteration 3807/6000, G loss: 0.4861072301864624, D loss: 0.07802213728427887\n",
            "Iteration 3808/6000, G loss: 0.5467401742935181, D loss: 0.00038055135519243777\n",
            "Iteration 3809/6000, G loss: 0.5615175366401672, D loss: 0.002601203043013811\n",
            "Iteration 3810/6000, G loss: 0.6234896183013916, D loss: 0.005704924464225769\n",
            "Iteration 3811/6000, G loss: 0.4822636842727661, D loss: 0.03793252259492874\n",
            "Iteration 3812/6000, G loss: 0.42083439230918884, D loss: 0.004889920819550753\n",
            "Iteration 3813/6000, G loss: 0.5014930963516235, D loss: 0.0022718857508152723\n",
            "Iteration 3814/6000, G loss: 0.5817206501960754, D loss: 0.006932187359780073\n",
            "Iteration 3815/6000, G loss: 0.4827612042427063, D loss: 0.009923161007463932\n",
            "Iteration 3816/6000, G loss: 0.5190595984458923, D loss: 0.1012495905160904\n",
            "Iteration 3817/6000, G loss: 0.47593140602111816, D loss: 0.006742867641150951\n",
            "Iteration 3818/6000, G loss: 0.46428102254867554, D loss: 0.04872938245534897\n",
            "Iteration 3819/6000, G loss: 0.5219262838363647, D loss: 0.03358686715364456\n",
            "Iteration 3820/6000, G loss: 0.5553942322731018, D loss: 0.004746549297124147\n",
            "Iteration 3821/6000, G loss: 0.5136527419090271, D loss: 0.0027924254536628723\n",
            "Iteration 3822/6000, G loss: 0.5966761708259583, D loss: 0.0004218040849082172\n",
            "Iteration 3823/6000, G loss: 0.5045905113220215, D loss: 0.005547729320824146\n",
            "Iteration 3824/6000, G loss: 0.6091701984405518, D loss: 0.003953867591917515\n",
            "Iteration 3825/6000, G loss: 0.5987229347229004, D loss: 0.00045799935469403863\n",
            "Iteration 3826/6000, G loss: 0.4670749604701996, D loss: 0.0008001106907613575\n",
            "Iteration 3827/6000, G loss: 0.5894106030464172, D loss: 0.009160581976175308\n",
            "Iteration 3828/6000, G loss: 0.41330552101135254, D loss: 0.007823158986866474\n",
            "Iteration 3829/6000, G loss: 0.4641109108924866, D loss: 0.0010425883810967207\n",
            "Iteration 3830/6000, G loss: 0.5069978833198547, D loss: 0.0027415526565164328\n",
            "Iteration 3831/6000, G loss: 0.5390570759773254, D loss: 0.010591121390461922\n",
            "Iteration 3832/6000, G loss: 0.5518176555633545, D loss: 0.07122217863798141\n",
            "Iteration 3833/6000, G loss: 0.41167375445365906, D loss: 0.00017569692863617092\n",
            "Iteration 3834/6000, G loss: 0.54459547996521, D loss: 0.22468024492263794\n",
            "Iteration 3835/6000, G loss: 0.4912533164024353, D loss: 0.003284513484686613\n",
            "Iteration 3836/6000, G loss: 0.5646147131919861, D loss: 0.00029218586860224605\n",
            "Iteration 3837/6000, G loss: 0.5322190523147583, D loss: 0.0014637403655797243\n",
            "Iteration 3838/6000, G loss: 0.5215132236480713, D loss: 0.09719959646463394\n",
            "Iteration 3839/6000, G loss: 0.48212987184524536, D loss: 0.01810712367296219\n",
            "Iteration 3840/6000, G loss: 0.4545077979564667, D loss: 0.0017526429146528244\n",
            "Iteration 3841/6000, G loss: 0.5935030579566956, D loss: 0.00039971305523067713\n",
            "Iteration 3842/6000, G loss: 0.47058379650115967, D loss: 0.008413657546043396\n",
            "Iteration 3843/6000, G loss: 0.45476627349853516, D loss: 0.00011702979827532545\n",
            "Iteration 3844/6000, G loss: 0.5077343583106995, D loss: 0.0012642480432987213\n",
            "Iteration 3845/6000, G loss: 0.5620594620704651, D loss: 0.0014155221870169044\n",
            "Iteration 3846/6000, G loss: 0.4445028007030487, D loss: 0.0051033711060881615\n",
            "Iteration 3847/6000, G loss: 0.5270367860794067, D loss: 0.0002301502099726349\n",
            "Iteration 3848/6000, G loss: 0.4422508180141449, D loss: 0.005955865606665611\n",
            "Iteration 3849/6000, G loss: 0.4580330550670624, D loss: 0.00027828439488075674\n",
            "Iteration 3850/6000, G loss: 0.5029200315475464, D loss: 0.44446343183517456\n",
            "Iteration 3851/6000, G loss: 0.45168760418891907, D loss: 0.02574719861149788\n",
            "Iteration 3852/6000, G loss: 0.467766135931015, D loss: 0.004641795996576548\n",
            "Iteration 3853/6000, G loss: 0.4310425817966461, D loss: 0.003980463370680809\n",
            "Iteration 3854/6000, G loss: 0.6398736834526062, D loss: 0.04022129997611046\n",
            "Iteration 3855/6000, G loss: 0.5454339981079102, D loss: 0.10878266394138336\n",
            "Iteration 3856/6000, G loss: 0.5225653648376465, D loss: 0.0006905271438881755\n",
            "Iteration 3857/6000, G loss: 0.46401873230934143, D loss: 0.11507774144411087\n",
            "Iteration 3858/6000, G loss: 0.5515127778053284, D loss: 0.12141692638397217\n",
            "Iteration 3859/6000, G loss: 0.5451232194900513, D loss: 0.011510681360960007\n",
            "Iteration 3860/6000, G loss: 0.45538753271102905, D loss: 0.8592344522476196\n",
            "Iteration 3861/6000, G loss: 0.4777596592903137, D loss: 0.12220966815948486\n",
            "Iteration 3862/6000, G loss: 0.6526274085044861, D loss: 0.15611621737480164\n",
            "Iteration 3863/6000, G loss: 0.5751686096191406, D loss: 0.008082510903477669\n",
            "Iteration 3864/6000, G loss: 0.480765700340271, D loss: 1.262963056564331\n",
            "Iteration 3865/6000, G loss: 0.6469171047210693, D loss: 9.97182542050723e-06\n",
            "Iteration 3866/6000, G loss: 0.5668469667434692, D loss: 0.13638938963413239\n",
            "Iteration 3867/6000, G loss: 0.5239720344543457, D loss: 11.47907543182373\n",
            "Iteration 3868/6000, G loss: 0.4841024875640869, D loss: 0.0019737256225198507\n",
            "Iteration 3869/6000, G loss: 0.619928777217865, D loss: 0.09138379991054535\n",
            "Iteration 3870/6000, G loss: 0.43669140338897705, D loss: 0.029605884104967117\n",
            "Iteration 3871/6000, G loss: 0.6153949499130249, D loss: 0.08349232375621796\n",
            "Iteration 3872/6000, G loss: 0.4556758403778076, D loss: 0.25887638330459595\n",
            "Iteration 3873/6000, G loss: 0.4479360282421112, D loss: 0.12987443804740906\n",
            "Iteration 3874/6000, G loss: 0.5922637581825256, D loss: 0.2813529372215271\n",
            "Iteration 3875/6000, G loss: 0.4890364110469818, D loss: 0.28488802909851074\n",
            "Iteration 3876/6000, G loss: 0.4575226902961731, D loss: 0.33246946334838867\n",
            "Iteration 3877/6000, G loss: 0.4820973575115204, D loss: 0.21211229264736176\n",
            "Iteration 3878/6000, G loss: 0.5192866921424866, D loss: 0.17069263756275177\n",
            "Iteration 3879/6000, G loss: 0.49451950192451477, D loss: 0.09956847131252289\n",
            "Iteration 3880/6000, G loss: 0.4818899631500244, D loss: 0.11526422202587128\n",
            "Iteration 3881/6000, G loss: 0.5583264231681824, D loss: 0.10664260387420654\n",
            "Iteration 3882/6000, G loss: 0.505517303943634, D loss: 0.06334657967090607\n",
            "Iteration 3883/6000, G loss: 0.4629690647125244, D loss: 0.24282413721084595\n",
            "Iteration 3884/6000, G loss: 0.3799821436405182, D loss: 0.08583593368530273\n",
            "Iteration 3885/6000, G loss: 0.5104262232780457, D loss: 0.012411925941705704\n",
            "Iteration 3886/6000, G loss: 0.5908047556877136, D loss: 0.01727312058210373\n",
            "Iteration 3887/6000, G loss: 0.4495980739593506, D loss: 0.024292675778269768\n",
            "Iteration 3888/6000, G loss: 0.5882567167282104, D loss: 0.013670559972524643\n",
            "Iteration 3889/6000, G loss: 0.5374497175216675, D loss: 0.027369167655706406\n",
            "Iteration 3890/6000, G loss: 0.5889062881469727, D loss: 0.015527013689279556\n",
            "Iteration 3891/6000, G loss: 0.4648674726486206, D loss: 0.022044535726308823\n",
            "Iteration 3892/6000, G loss: 0.43483319878578186, D loss: 0.008044486865401268\n",
            "Iteration 3893/6000, G loss: 0.4522486627101898, D loss: 0.00387915363535285\n",
            "Iteration 3894/6000, G loss: 0.5220322012901306, D loss: 0.0005116365500725806\n",
            "Iteration 3895/6000, G loss: 0.5180920958518982, D loss: 0.016418563202023506\n",
            "Iteration 3896/6000, G loss: 0.6239042282104492, D loss: 0.003946235869079828\n",
            "Iteration 3897/6000, G loss: 0.6057201027870178, D loss: 0.01059954147785902\n",
            "Iteration 3898/6000, G loss: 0.4863712787628174, D loss: 0.003708433825522661\n",
            "Iteration 3899/6000, G loss: 0.5024225115776062, D loss: 0.021590333431959152\n",
            "Iteration 3900/6000, G loss: 0.521429181098938, D loss: 0.0019088839180767536\n",
            "Iteration 3901/6000, G loss: 0.3808578848838806, D loss: 0.0035616992972791195\n",
            "Iteration 3902/6000, G loss: 0.44266530871391296, D loss: 0.03030577301979065\n",
            "Iteration 3903/6000, G loss: 0.46049782633781433, D loss: 0.0027615756262093782\n",
            "Iteration 3904/6000, G loss: 0.41523319482803345, D loss: 0.023773374035954475\n",
            "Iteration 3905/6000, G loss: 0.5663325190544128, D loss: 0.2797589898109436\n",
            "Iteration 3906/6000, G loss: 0.5016305446624756, D loss: 0.00443185493350029\n",
            "Iteration 3907/6000, G loss: 0.35685449838638306, D loss: 0.07409197837114334\n",
            "Iteration 3908/6000, G loss: 0.5156980156898499, D loss: 0.0016512684524059296\n",
            "Iteration 3909/6000, G loss: 0.45737510919570923, D loss: 0.002094025257974863\n",
            "Iteration 3910/6000, G loss: 0.5587989687919617, D loss: 0.001171704614534974\n",
            "Iteration 3911/6000, G loss: 0.5849637389183044, D loss: 0.003194676712155342\n",
            "Iteration 3912/6000, G loss: 0.5460323691368103, D loss: 0.02700899913907051\n",
            "Iteration 3913/6000, G loss: 0.5863097906112671, D loss: 0.040222860872745514\n",
            "Iteration 3914/6000, G loss: 0.5003359317779541, D loss: 0.0035423352383077145\n",
            "Iteration 3915/6000, G loss: 0.46151992678642273, D loss: 0.0008855086052790284\n",
            "Iteration 3916/6000, G loss: 0.5440151691436768, D loss: 0.009679628536105156\n",
            "Iteration 3917/6000, G loss: 0.5189545154571533, D loss: 0.0013366718776524067\n",
            "Iteration 3918/6000, G loss: 0.5614433288574219, D loss: 0.01773608662188053\n",
            "Iteration 3919/6000, G loss: 0.5525918006896973, D loss: 0.23909780383110046\n",
            "Iteration 3920/6000, G loss: 0.5675442218780518, D loss: 0.011980903334915638\n",
            "Iteration 3921/6000, G loss: 0.4809318482875824, D loss: 0.01993531733751297\n",
            "Iteration 3922/6000, G loss: 0.5273367166519165, D loss: 0.0020437377970665693\n",
            "Iteration 3923/6000, G loss: 0.54511559009552, D loss: 0.9966942667961121\n",
            "Iteration 3924/6000, G loss: 0.5365750789642334, D loss: 0.0012368899770081043\n",
            "Iteration 3925/6000, G loss: 0.4683372974395752, D loss: 0.0012076296843588352\n",
            "Iteration 3926/6000, G loss: 0.5540551543235779, D loss: 8.004884875845164e-06\n",
            "Iteration 3927/6000, G loss: 0.4692330062389374, D loss: 0.002202253555878997\n",
            "Iteration 3928/6000, G loss: 0.4802890419960022, D loss: 0.011516524478793144\n",
            "Iteration 3929/6000, G loss: 0.5881297588348389, D loss: 0.026934601366519928\n",
            "Iteration 3930/6000, G loss: 0.5172279477119446, D loss: 0.0017980426782742143\n",
            "Iteration 3931/6000, G loss: 0.4941917061805725, D loss: 0.1672566533088684\n",
            "Iteration 3932/6000, G loss: 0.3476669192314148, D loss: 0.739611804485321\n",
            "Iteration 3933/6000, G loss: 0.4851997196674347, D loss: 0.01187099888920784\n",
            "Iteration 3934/6000, G loss: 0.4836794435977936, D loss: 0.016761958599090576\n",
            "Iteration 3935/6000, G loss: 0.5039476752281189, D loss: 0.0007480318890884519\n",
            "Iteration 3936/6000, G loss: 0.6138927936553955, D loss: 0.07063626497983932\n",
            "Iteration 3937/6000, G loss: 0.45892760157585144, D loss: 0.009204452857375145\n",
            "Iteration 3938/6000, G loss: 0.40061047673225403, D loss: 1.7326173782348633\n",
            "Iteration 3939/6000, G loss: 0.6119142770767212, D loss: 0.04286705330014229\n",
            "Iteration 3940/6000, G loss: 0.5238073468208313, D loss: 0.0056432392448186874\n",
            "Iteration 3941/6000, G loss: 0.49267467856407166, D loss: 0.015249506570398808\n",
            "Iteration 3942/6000, G loss: 0.5783715844154358, D loss: 0.004287791438400745\n",
            "Iteration 3943/6000, G loss: 0.48029354214668274, D loss: 0.016617506742477417\n",
            "Iteration 3944/6000, G loss: 0.5994269251823425, D loss: 2.7292633056640625\n",
            "Iteration 3945/6000, G loss: 0.6196444630622864, D loss: 0.007449136581271887\n",
            "Iteration 3946/6000, G loss: 0.44235092401504517, D loss: 1.9043580323341303e-05\n",
            "Iteration 3947/6000, G loss: 0.5405995845794678, D loss: 1.0985099834215362e-05\n",
            "Iteration 3948/6000, G loss: 0.5070936679840088, D loss: 4.5138000132283196e-05\n",
            "Iteration 3949/6000, G loss: 0.586794912815094, D loss: 0.00017521061818115413\n",
            "Iteration 3950/6000, G loss: 0.4616159200668335, D loss: 0.0004181792901363224\n",
            "Iteration 3951/6000, G loss: 0.566184401512146, D loss: 0.00019843435438815504\n",
            "Iteration 3952/6000, G loss: 0.3927423357963562, D loss: 0.0007545406697317958\n",
            "Iteration 3953/6000, G loss: 0.5942333936691284, D loss: 0.030451860278844833\n",
            "Iteration 3954/6000, G loss: 0.4172629714012146, D loss: 0.024664919823408127\n",
            "Iteration 3955/6000, G loss: 0.6040567755699158, D loss: 0.005270611494779587\n",
            "Iteration 3956/6000, G loss: 0.5630362033843994, D loss: 1.6790523659437895e-05\n",
            "Iteration 3957/6000, G loss: 0.41941750049591064, D loss: 0.007203555665910244\n",
            "Iteration 3958/6000, G loss: 0.49039363861083984, D loss: 0.0028957840986549854\n",
            "Iteration 3959/6000, G loss: 0.5763844847679138, D loss: 0.008140386082231998\n",
            "Iteration 3960/6000, G loss: 0.5151183605194092, D loss: 2.9134491342119873e-05\n",
            "Iteration 3961/6000, G loss: 0.492110013961792, D loss: 0.02700047381222248\n",
            "Iteration 3962/6000, G loss: 0.39702820777893066, D loss: 0.0011081789853051305\n",
            "Iteration 3963/6000, G loss: 0.602285623550415, D loss: 0.0069506349973380566\n",
            "Iteration 3964/6000, G loss: 0.46661490201950073, D loss: 0.0053826747462153435\n",
            "Iteration 3965/6000, G loss: 0.4294377565383911, D loss: 0.012208987958729267\n",
            "Iteration 3966/6000, G loss: 0.5562581419944763, D loss: 0.000402106496039778\n",
            "Iteration 3967/6000, G loss: 0.5325430035591125, D loss: 0.0008930101757869124\n",
            "Iteration 3968/6000, G loss: 0.5607439875602722, D loss: 0.009161438792943954\n",
            "Iteration 3969/6000, G loss: 0.47278767824172974, D loss: 0.002636476419866085\n",
            "Iteration 3970/6000, G loss: 0.5184158682823181, D loss: 0.015772555023431778\n",
            "Iteration 3971/6000, G loss: 0.5156955718994141, D loss: 0.009943466633558273\n",
            "Iteration 3972/6000, G loss: 0.5244002342224121, D loss: 0.6114134788513184\n",
            "Iteration 3973/6000, G loss: 0.4526202082633972, D loss: 0.0034417230635881424\n",
            "Iteration 3974/6000, G loss: 0.6041495203971863, D loss: 0.17798268795013428\n",
            "Iteration 3975/6000, G loss: 0.5370949506759644, D loss: 0.0036327624693512917\n",
            "Iteration 3976/6000, G loss: 0.5211906433105469, D loss: 0.0001578431110829115\n",
            "Iteration 3977/6000, G loss: 0.5156245827674866, D loss: 0.0036762708332389593\n",
            "Iteration 3978/6000, G loss: 0.5876441597938538, D loss: 9.466118353884667e-05\n",
            "Iteration 3979/6000, G loss: 0.6113324761390686, D loss: 0.005732803605496883\n",
            "Iteration 3980/6000, G loss: 0.5453141927719116, D loss: 0.0034363060258328915\n",
            "Iteration 3981/6000, G loss: 0.47634246945381165, D loss: 0.011094549670815468\n",
            "Iteration 3982/6000, G loss: 0.5437361598014832, D loss: 0.010080037638545036\n",
            "Iteration 3983/6000, G loss: 0.5153722167015076, D loss: 0.0033280695788562298\n",
            "Iteration 3984/6000, G loss: 0.42380204796791077, D loss: 0.08846955001354218\n",
            "Iteration 3985/6000, G loss: 0.6114102602005005, D loss: 0.20500589907169342\n",
            "Iteration 3986/6000, G loss: 0.6225371956825256, D loss: 2.777501106262207\n",
            "Iteration 3987/6000, G loss: 0.5300874710083008, D loss: 0.00034847354982048273\n",
            "Iteration 3988/6000, G loss: 0.42866113781929016, D loss: 0.001132625387981534\n",
            "Iteration 3989/6000, G loss: 0.5331568121910095, D loss: 0.01679588109254837\n",
            "Iteration 3990/6000, G loss: 0.5390427112579346, D loss: 1.9895895093213767e-05\n",
            "Iteration 3991/6000, G loss: 0.467387318611145, D loss: 0.00041940418304875493\n",
            "Iteration 3992/6000, G loss: 0.4497866630554199, D loss: 0.0003960109897889197\n",
            "Iteration 3993/6000, G loss: 0.5367051959037781, D loss: 0.0023331320844590664\n",
            "Iteration 3994/6000, G loss: 0.5180283188819885, D loss: 0.07019750028848648\n",
            "Iteration 3995/6000, G loss: 0.5025424957275391, D loss: 0.010994261130690575\n",
            "Iteration 3996/6000, G loss: 0.49228644371032715, D loss: 0.00026363186771050096\n",
            "Iteration 3997/6000, G loss: 0.5204091668128967, D loss: 0.0032377312891185284\n",
            "Iteration 3998/6000, G loss: 0.5327134728431702, D loss: 0.00041731022065505385\n",
            "Iteration 3999/6000, G loss: 0.3857181668281555, D loss: 0.002863360568881035\n",
            "Iteration 4000/6000, G loss: 0.6159041523933411, D loss: 0.45039433240890503\n",
            "Iteration 4001/6000, G loss: 0.4546249508857727, D loss: 0.024424463510513306\n",
            "Iteration 4002/6000, G loss: 0.5218300223350525, D loss: 0.001980782486498356\n",
            "Iteration 4003/6000, G loss: 0.5660152435302734, D loss: 0.1589844524860382\n",
            "Iteration 4004/6000, G loss: 0.5546263456344604, D loss: 0.000301020045299083\n",
            "Iteration 4005/6000, G loss: 0.4932076036930084, D loss: 0.002411327324807644\n",
            "Iteration 4006/6000, G loss: 0.44675108790397644, D loss: 0.0029326407238841057\n",
            "Iteration 4007/6000, G loss: 0.4567873179912567, D loss: 0.003367326920852065\n",
            "Iteration 4008/6000, G loss: 0.43604710698127747, D loss: 0.026025936007499695\n",
            "Iteration 4009/6000, G loss: 0.3795957565307617, D loss: 0.011656763032078743\n",
            "Iteration 4010/6000, G loss: 0.5180079936981201, D loss: 0.00543591845780611\n",
            "Iteration 4011/6000, G loss: 0.4570905566215515, D loss: 0.008116023615002632\n",
            "Iteration 4012/6000, G loss: 0.541002094745636, D loss: 0.0026138736866414547\n",
            "Iteration 4013/6000, G loss: 0.486150860786438, D loss: 0.04071120172739029\n",
            "Iteration 4014/6000, G loss: 0.5197339057922363, D loss: 0.006016409490257502\n",
            "Iteration 4015/6000, G loss: 0.5487982034683228, D loss: 0.0002506150340195745\n",
            "Iteration 4016/6000, G loss: 0.39930272102355957, D loss: 0.034262966364622116\n",
            "Iteration 4017/6000, G loss: 0.5807639956474304, D loss: 0.0003690519370138645\n",
            "Iteration 4018/6000, G loss: 0.5704062581062317, D loss: 0.028796592727303505\n",
            "Iteration 4019/6000, G loss: 0.5700125694274902, D loss: 0.0003172960423398763\n",
            "Iteration 4020/6000, G loss: 0.6535727977752686, D loss: 0.00018932315288111567\n",
            "Iteration 4021/6000, G loss: 0.5488420724868774, D loss: 0.005381089635193348\n",
            "Iteration 4022/6000, G loss: 0.4990839958190918, D loss: 3.0415818400797434e-05\n",
            "Iteration 4023/6000, G loss: 0.4699035882949829, D loss: 0.00024590548127889633\n",
            "Iteration 4024/6000, G loss: 0.5007029175758362, D loss: 0.0025572171434760094\n",
            "Iteration 4025/6000, G loss: 0.5127230286598206, D loss: 0.03968023136258125\n",
            "Iteration 4026/6000, G loss: 0.6396976113319397, D loss: 0.0007484850939363241\n",
            "Iteration 4027/6000, G loss: 0.5043036937713623, D loss: 0.0018629743717610836\n",
            "Iteration 4028/6000, G loss: 0.5416179895401001, D loss: 0.0008491325424984097\n",
            "Iteration 4029/6000, G loss: 0.5386224389076233, D loss: 0.001362749608233571\n",
            "Iteration 4030/6000, G loss: 0.4304811358451843, D loss: 0.00039835009374655783\n",
            "Iteration 4031/6000, G loss: 0.5462368726730347, D loss: 0.010061103850603104\n",
            "Iteration 4032/6000, G loss: 0.43399831652641296, D loss: 0.017982477322220802\n",
            "Iteration 4033/6000, G loss: 0.5913784503936768, D loss: 0.006376361008733511\n",
            "Iteration 4034/6000, G loss: 0.5509488582611084, D loss: 0.0038059218786656857\n",
            "Iteration 4035/6000, G loss: 0.4623302221298218, D loss: 0.004329303279519081\n",
            "Iteration 4036/6000, G loss: 0.48064425587654114, D loss: 0.008432062342762947\n",
            "Iteration 4037/6000, G loss: 0.4426170885562897, D loss: 0.00010420914622955024\n",
            "Iteration 4038/6000, G loss: 0.49260997772216797, D loss: 0.012602675706148148\n",
            "Iteration 4039/6000, G loss: 0.5220484733581543, D loss: 2.338601589202881\n",
            "Iteration 4040/6000, G loss: 0.5632200241088867, D loss: 3.625707540777512e-05\n",
            "Iteration 4041/6000, G loss: 0.39623868465423584, D loss: 0.016720296815037727\n",
            "Iteration 4042/6000, G loss: 0.5215162634849548, D loss: 0.012495739385485649\n",
            "Iteration 4043/6000, G loss: 0.5598999261856079, D loss: 0.09649842977523804\n",
            "Iteration 4044/6000, G loss: 0.45904746651649475, D loss: 0.010758591815829277\n",
            "Iteration 4045/6000, G loss: 0.4275839626789093, D loss: 0.00010280667629558593\n",
            "Iteration 4046/6000, G loss: 0.4952918589115143, D loss: 0.1423693299293518\n",
            "Iteration 4047/6000, G loss: 0.5238997936248779, D loss: 0.014683549292385578\n",
            "Iteration 4048/6000, G loss: 0.4460531771183014, D loss: 0.015082288533449173\n",
            "Iteration 4049/6000, G loss: 0.5371296405792236, D loss: 0.02782560884952545\n",
            "Iteration 4050/6000, G loss: 0.5705249905586243, D loss: 0.002108808606863022\n",
            "Iteration 4051/6000, G loss: 0.44441431760787964, D loss: 0.00125270732678473\n",
            "Iteration 4052/6000, G loss: 0.43299928307533264, D loss: 0.004009372554719448\n",
            "Iteration 4053/6000, G loss: 0.48739683628082275, D loss: 0.4486188292503357\n",
            "Iteration 4054/6000, G loss: 0.5105274319648743, D loss: 0.003644825192168355\n",
            "Iteration 4055/6000, G loss: 0.4926304519176483, D loss: 0.006031872238963842\n",
            "Iteration 4056/6000, G loss: 0.4654576778411865, D loss: 0.004730634856969118\n",
            "Iteration 4057/6000, G loss: 0.419353723526001, D loss: 0.24656090140342712\n",
            "Iteration 4058/6000, G loss: 0.471577912569046, D loss: 0.02105625718832016\n",
            "Iteration 4059/6000, G loss: 0.4071057438850403, D loss: 0.3496687710285187\n",
            "Iteration 4060/6000, G loss: 0.459128201007843, D loss: 0.003491119248792529\n",
            "Iteration 4061/6000, G loss: 0.5901711583137512, D loss: 0.11813875287771225\n",
            "Iteration 4062/6000, G loss: 0.5218828320503235, D loss: 0.020908884704113007\n",
            "Iteration 4063/6000, G loss: 0.531772255897522, D loss: 0.0001443255750928074\n",
            "Iteration 4064/6000, G loss: 0.5593926906585693, D loss: 0.03503365442156792\n",
            "Iteration 4065/6000, G loss: 0.5901282429695129, D loss: 0.000418843817897141\n",
            "Iteration 4066/6000, G loss: 0.46910980343818665, D loss: 0.0003648269921541214\n",
            "Iteration 4067/6000, G loss: 0.6192712783813477, D loss: 0.09692317992448807\n",
            "Iteration 4068/6000, G loss: 0.5005005598068237, D loss: 0.0014774820301681757\n",
            "Iteration 4069/6000, G loss: 0.48723816871643066, D loss: 0.004333760589361191\n",
            "Iteration 4070/6000, G loss: 0.6184585094451904, D loss: 0.003412187099456787\n",
            "Iteration 4071/6000, G loss: 0.5204532146453857, D loss: 5.59318796149455e-05\n",
            "Iteration 4072/6000, G loss: 0.5450906753540039, D loss: 0.0029218653216958046\n",
            "Iteration 4073/6000, G loss: 0.5877063274383545, D loss: 1.6951963901519775\n",
            "Iteration 4074/6000, G loss: 0.4926232397556305, D loss: 0.6242156028747559\n",
            "Iteration 4075/6000, G loss: 0.5380740761756897, D loss: 8.952592907007784e-06\n",
            "Iteration 4076/6000, G loss: 0.5835630297660828, D loss: 0.00011410749721108004\n",
            "Iteration 4077/6000, G loss: 0.507143497467041, D loss: 3.142921923426911e-05\n",
            "Iteration 4078/6000, G loss: 0.5354872345924377, D loss: 0.00915654469281435\n",
            "Iteration 4079/6000, G loss: 0.6610134243965149, D loss: 0.10608462989330292\n",
            "Iteration 4080/6000, G loss: 0.6068539023399353, D loss: 0.05844803899526596\n",
            "Iteration 4081/6000, G loss: 0.5827233195304871, D loss: 0.0009396633831784129\n",
            "Iteration 4082/6000, G loss: 0.48926782608032227, D loss: 1.0388793270976748e-05\n",
            "Iteration 4083/6000, G loss: 0.46538129448890686, D loss: 0.0007094130851328373\n",
            "Iteration 4084/6000, G loss: 0.47849947214126587, D loss: 3.4570689422253054e-07\n",
            "Iteration 4085/6000, G loss: 0.383840948343277, D loss: 4.8070200136862695e-05\n",
            "Iteration 4086/6000, G loss: 0.49134111404418945, D loss: 0.00016788378707133234\n",
            "Iteration 4087/6000, G loss: 0.5838978290557861, D loss: 0.027295559644699097\n",
            "Iteration 4088/6000, G loss: 0.5987644195556641, D loss: 0.10446929931640625\n",
            "Iteration 4089/6000, G loss: 0.5117542743682861, D loss: 0.2648634612560272\n",
            "Iteration 4090/6000, G loss: 0.3500315845012665, D loss: 0.000753466971218586\n",
            "Iteration 4091/6000, G loss: 0.5325343608856201, D loss: 4.44105317001231e-05\n",
            "Iteration 4092/6000, G loss: 0.5011997222900391, D loss: 0.007569104433059692\n",
            "Iteration 4093/6000, G loss: 0.5586383938789368, D loss: 0.00025035347789525986\n",
            "Iteration 4094/6000, G loss: 0.5861203670501709, D loss: 0.002328806556761265\n",
            "Iteration 4095/6000, G loss: 0.4511328637599945, D loss: 0.06550807505846024\n",
            "Iteration 4096/6000, G loss: 0.6313554048538208, D loss: 0.6747649908065796\n",
            "Iteration 4097/6000, G loss: 0.4879414141178131, D loss: 0.004975049756467342\n",
            "Iteration 4098/6000, G loss: 0.4571051597595215, D loss: 0.1914648413658142\n",
            "Iteration 4099/6000, G loss: 0.619691789150238, D loss: 0.01589803956449032\n",
            "Iteration 4100/6000, G loss: 0.4711691737174988, D loss: 2.4026337996474467e-05\n",
            "Iteration 4101/6000, G loss: 0.5131462812423706, D loss: 0.0006091272225603461\n",
            "Iteration 4102/6000, G loss: 0.4688720703125, D loss: 0.17787936329841614\n",
            "Iteration 4103/6000, G loss: 0.5813853740692139, D loss: 0.011938007548451424\n",
            "Iteration 4104/6000, G loss: 0.42729276418685913, D loss: 0.28923559188842773\n",
            "Iteration 4105/6000, G loss: 0.5975798964500427, D loss: 0.012506839819252491\n",
            "Iteration 4106/6000, G loss: 0.4255662262439728, D loss: 0.2610359191894531\n",
            "Iteration 4107/6000, G loss: 0.5925053358078003, D loss: 12.59052848815918\n",
            "Iteration 4108/6000, G loss: 0.4886876940727234, D loss: 9.590354238753207e-06\n",
            "Iteration 4109/6000, G loss: 0.550703227519989, D loss: 0.00016834968118928373\n",
            "Iteration 4110/6000, G loss: 0.5283210873603821, D loss: 0.0046784039586782455\n",
            "Iteration 4111/6000, G loss: 0.5766711235046387, D loss: 0.0004394161223899573\n",
            "Iteration 4112/6000, G loss: 0.5408322811126709, D loss: 0.024433467537164688\n",
            "Iteration 4113/6000, G loss: 0.48095381259918213, D loss: 0.09041410684585571\n",
            "Iteration 4114/6000, G loss: 0.49466341733932495, D loss: 0.03373553231358528\n",
            "Iteration 4115/6000, G loss: 0.5007267594337463, D loss: 0.07068206369876862\n",
            "Iteration 4116/6000, G loss: 0.5789897441864014, D loss: 0.0989941954612732\n",
            "Iteration 4117/6000, G loss: 0.39374083280563354, D loss: 0.3067144751548767\n",
            "Iteration 4118/6000, G loss: 0.5299395322799683, D loss: 0.13416734337806702\n",
            "Iteration 4119/6000, G loss: 0.5532538890838623, D loss: 0.24003979563713074\n",
            "Iteration 4120/6000, G loss: 0.6735274791717529, D loss: 0.29476678371429443\n",
            "Iteration 4121/6000, G loss: 0.4440499544143677, D loss: 0.12463803589344025\n",
            "Iteration 4122/6000, G loss: 0.6739033460617065, D loss: 0.20272433757781982\n",
            "Iteration 4123/6000, G loss: 0.44849544763565063, D loss: 0.13537631928920746\n",
            "Iteration 4124/6000, G loss: 0.47455912828445435, D loss: 0.10905246436595917\n",
            "Iteration 4125/6000, G loss: 0.5060669183731079, D loss: 0.1217670813202858\n",
            "Iteration 4126/6000, G loss: 0.5509432554244995, D loss: 0.046537239104509354\n",
            "Iteration 4127/6000, G loss: 0.3870050311088562, D loss: 0.05896177887916565\n",
            "Iteration 4128/6000, G loss: 0.5577895641326904, D loss: 0.0545174703001976\n",
            "Iteration 4129/6000, G loss: 0.4017147719860077, D loss: 0.0065530044957995415\n",
            "Iteration 4130/6000, G loss: 0.4360417425632477, D loss: 0.022838018834590912\n",
            "Iteration 4131/6000, G loss: 0.5838034749031067, D loss: 0.0292393546551466\n",
            "Iteration 4132/6000, G loss: 0.4897899031639099, D loss: 0.01442677527666092\n",
            "Iteration 4133/6000, G loss: 0.4824376702308655, D loss: 0.14156115055084229\n",
            "Iteration 4134/6000, G loss: 0.5656096935272217, D loss: 0.020761366933584213\n",
            "Iteration 4135/6000, G loss: 0.5030390620231628, D loss: 0.003675199346616864\n",
            "Iteration 4136/6000, G loss: 0.5124452114105225, D loss: 0.010724452324211597\n",
            "Iteration 4137/6000, G loss: 0.5130181908607483, D loss: 0.01310540921986103\n",
            "Iteration 4138/6000, G loss: 0.44987961649894714, D loss: 0.008091198280453682\n",
            "Iteration 4139/6000, G loss: 0.40912485122680664, D loss: 0.006061000283807516\n",
            "Iteration 4140/6000, G loss: 0.5626214146614075, D loss: 0.00537665793672204\n",
            "Iteration 4141/6000, G loss: 0.43279141187667847, D loss: 0.003184639848768711\n",
            "Iteration 4142/6000, G loss: 0.5060408115386963, D loss: 0.0035825991071760654\n",
            "Iteration 4143/6000, G loss: 0.4473050534725189, D loss: 0.03128645196557045\n",
            "Iteration 4144/6000, G loss: 0.3721742630004883, D loss: 0.0003178300685249269\n",
            "Iteration 4145/6000, G loss: 0.5214365720748901, D loss: 0.0028797569684684277\n",
            "Iteration 4146/6000, G loss: 0.6124185919761658, D loss: 0.03520212322473526\n",
            "Iteration 4147/6000, G loss: 0.5854277610778809, D loss: 0.0017659335862845182\n",
            "Iteration 4148/6000, G loss: 0.5387828946113586, D loss: 0.00044222536962479353\n",
            "Iteration 4149/6000, G loss: 0.5712379813194275, D loss: 0.01308068260550499\n",
            "Iteration 4150/6000, G loss: 0.4831082820892334, D loss: 0.000696005008649081\n",
            "Iteration 4151/6000, G loss: 0.5944547057151794, D loss: 0.05196952447295189\n",
            "Iteration 4152/6000, G loss: 0.48027271032333374, D loss: 0.0006972568808123469\n",
            "Iteration 4153/6000, G loss: 0.40334707498550415, D loss: 0.0037163752131164074\n",
            "Iteration 4154/6000, G loss: 0.5251519083976746, D loss: 0.0035598797257989645\n",
            "Iteration 4155/6000, G loss: 0.5348522067070007, D loss: 0.005838438868522644\n",
            "Iteration 4156/6000, G loss: 0.5395865440368652, D loss: 0.008006766438484192\n",
            "Iteration 4157/6000, G loss: 0.3674008846282959, D loss: 0.007249212823808193\n",
            "Iteration 4158/6000, G loss: 0.5511977672576904, D loss: 0.01279466599225998\n",
            "Iteration 4159/6000, G loss: 0.4433164894580841, D loss: 0.014857551082968712\n",
            "Iteration 4160/6000, G loss: 0.5732495784759521, D loss: 0.02519207075238228\n",
            "Iteration 4161/6000, G loss: 0.5241560935974121, D loss: 9.666790720075369e-05\n",
            "Iteration 4162/6000, G loss: 0.537547767162323, D loss: 0.002287614159286022\n",
            "Iteration 4163/6000, G loss: 0.5194054841995239, D loss: 0.3477886915206909\n",
            "Iteration 4164/6000, G loss: 0.539685070514679, D loss: 0.00011990644270554185\n",
            "Iteration 4165/6000, G loss: 0.5482040047645569, D loss: 0.001415939535945654\n",
            "Iteration 4166/6000, G loss: 0.46355074644088745, D loss: 0.002061561681330204\n",
            "Iteration 4167/6000, G loss: 0.5584765672683716, D loss: 0.013230101205408573\n",
            "Iteration 4168/6000, G loss: 0.4572901427745819, D loss: 0.01202761847525835\n",
            "Iteration 4169/6000, G loss: 0.49042147397994995, D loss: 1.1290655136108398\n",
            "Iteration 4170/6000, G loss: 0.4673328399658203, D loss: 0.00046500455937348306\n",
            "Iteration 4171/6000, G loss: 0.5546280145645142, D loss: 0.03577830269932747\n",
            "Iteration 4172/6000, G loss: 0.4772340655326843, D loss: 0.008626706898212433\n",
            "Iteration 4173/6000, G loss: 0.4566959738731384, D loss: 0.2347739338874817\n",
            "Iteration 4174/6000, G loss: 0.5881960391998291, D loss: 0.0013381895842030644\n",
            "Iteration 4175/6000, G loss: 0.5843732953071594, D loss: 0.009605255909264088\n",
            "Iteration 4176/6000, G loss: 0.4749119281768799, D loss: 0.00019580185471568257\n",
            "Iteration 4177/6000, G loss: 0.5001450181007385, D loss: 1.2576530934893526e-05\n",
            "Iteration 4178/6000, G loss: 0.4571162164211273, D loss: 0.4681147336959839\n",
            "Iteration 4179/6000, G loss: 0.6517795920372009, D loss: 1.8037254810333252\n",
            "Iteration 4180/6000, G loss: 0.6047261357307434, D loss: 1.4489824025076814e-05\n",
            "Iteration 4181/6000, G loss: 0.42734667658805847, D loss: 0.0001481665822211653\n",
            "Iteration 4182/6000, G loss: 0.6095660924911499, D loss: 0.0005771893775090575\n",
            "Iteration 4183/6000, G loss: 0.5304439067840576, D loss: 0.013680113479495049\n",
            "Iteration 4184/6000, G loss: 0.39222875237464905, D loss: 6.0791215896606445\n",
            "Iteration 4185/6000, G loss: 0.5182778239250183, D loss: 0.0005834624171257019\n",
            "Iteration 4186/6000, G loss: 0.41363707184791565, D loss: 0.0291462205350399\n",
            "Iteration 4187/6000, G loss: 0.4950360953807831, D loss: 0.007130826357752085\n",
            "Iteration 4188/6000, G loss: 0.45368483662605286, D loss: 0.10171088576316833\n",
            "Iteration 4189/6000, G loss: 0.44961610436439514, D loss: 0.055794455111026764\n",
            "Iteration 4190/6000, G loss: 0.3565942645072937, D loss: 0.00011535482190083712\n",
            "Iteration 4191/6000, G loss: 0.49579593539237976, D loss: 0.0005803951062262058\n",
            "Iteration 4192/6000, G loss: 0.4811011850833893, D loss: 0.0003245627449359745\n",
            "Iteration 4193/6000, G loss: 0.42826545238494873, D loss: 0.0015610824339091778\n",
            "Iteration 4194/6000, G loss: 0.5066662430763245, D loss: 0.11080668866634369\n",
            "Iteration 4195/6000, G loss: 0.5177453756332397, D loss: 0.02705339714884758\n",
            "Iteration 4196/6000, G loss: 0.45850181579589844, D loss: 0.08327102661132812\n",
            "Iteration 4197/6000, G loss: 0.6214006543159485, D loss: 0.001996819395571947\n",
            "Iteration 4198/6000, G loss: 0.6016207337379456, D loss: 0.006369045004248619\n",
            "Iteration 4199/6000, G loss: 0.49623197317123413, D loss: 0.002031853888183832\n",
            "Iteration 4200/6000, G loss: 0.4329076409339905, D loss: 0.004392542410641909\n",
            "Iteration 4201/6000, G loss: 0.5439712405204773, D loss: 0.001513845520094037\n",
            "Iteration 4202/6000, G loss: 0.5525987148284912, D loss: 0.005612214561551809\n",
            "Iteration 4203/6000, G loss: 0.5187456607818604, D loss: 0.046262919902801514\n",
            "Iteration 4204/6000, G loss: 0.49877768754959106, D loss: 0.16352176666259766\n",
            "Iteration 4205/6000, G loss: 0.45107027888298035, D loss: 0.0056519052013754845\n",
            "Iteration 4206/6000, G loss: 0.4792482852935791, D loss: 0.013425104320049286\n",
            "Iteration 4207/6000, G loss: 0.5061213970184326, D loss: 0.004906319081783295\n",
            "Iteration 4208/6000, G loss: 0.4604952335357666, D loss: 0.001257413998246193\n",
            "Iteration 4209/6000, G loss: 0.5745888948440552, D loss: 0.007282108999788761\n",
            "Iteration 4210/6000, G loss: 0.5072351694107056, D loss: 0.012984318658709526\n",
            "Iteration 4211/6000, G loss: 0.4342919886112213, D loss: 0.018690478056669235\n",
            "Iteration 4212/6000, G loss: 0.5404980778694153, D loss: 0.03383908420801163\n",
            "Iteration 4213/6000, G loss: 0.5017354488372803, D loss: 0.026288125663995743\n",
            "Iteration 4214/6000, G loss: 0.45346733927726746, D loss: 0.04651886969804764\n",
            "Iteration 4215/6000, G loss: 0.5417090058326721, D loss: 0.0007523850654251873\n",
            "Iteration 4216/6000, G loss: 0.49879398941993713, D loss: 0.05320078879594803\n",
            "Iteration 4217/6000, G loss: 0.4722973108291626, D loss: 0.01178925670683384\n",
            "Iteration 4218/6000, G loss: 0.5246676206588745, D loss: 0.01801048219203949\n",
            "Iteration 4219/6000, G loss: 0.41490697860717773, D loss: 0.0006788048194721341\n",
            "Iteration 4220/6000, G loss: 0.5858947038650513, D loss: 0.005564020946621895\n",
            "Iteration 4221/6000, G loss: 0.47400662302970886, D loss: 0.013547813519835472\n",
            "Iteration 4222/6000, G loss: 0.4663126468658447, D loss: 0.05038885027170181\n",
            "Iteration 4223/6000, G loss: 0.5073217749595642, D loss: 0.016815505921840668\n",
            "Iteration 4224/6000, G loss: 0.5698619484901428, D loss: 0.0024725915864109993\n",
            "Iteration 4225/6000, G loss: 0.5552136898040771, D loss: 0.004497768357396126\n",
            "Iteration 4226/6000, G loss: 0.5175718069076538, D loss: 0.004456584341824055\n",
            "Iteration 4227/6000, G loss: 0.44302016496658325, D loss: 0.10477419197559357\n",
            "Iteration 4228/6000, G loss: 0.4810497462749481, D loss: 0.0005542043945752084\n",
            "Iteration 4229/6000, G loss: 0.4775618314743042, D loss: 0.0005784945678897202\n",
            "Iteration 4230/6000, G loss: 0.5183257460594177, D loss: 0.053516991436481476\n",
            "Iteration 4231/6000, G loss: 0.5142931342124939, D loss: 0.0010053326841443777\n",
            "Iteration 4232/6000, G loss: 0.5380344390869141, D loss: 0.0005447025178000331\n",
            "Iteration 4233/6000, G loss: 0.40317094326019287, D loss: 0.0023738485760986805\n",
            "Iteration 4234/6000, G loss: 0.5430085062980652, D loss: 0.01419554091989994\n",
            "Iteration 4235/6000, G loss: 0.5609312057495117, D loss: 0.0027955896221101284\n",
            "Iteration 4236/6000, G loss: 0.5032446384429932, D loss: 0.033680759370326996\n",
            "Iteration 4237/6000, G loss: 0.4909478724002838, D loss: 0.027333837002515793\n",
            "Iteration 4238/6000, G loss: 0.5070773959159851, D loss: 0.036744117736816406\n",
            "Iteration 4239/6000, G loss: 0.4899880886077881, D loss: 0.0016696329694241285\n",
            "Iteration 4240/6000, G loss: 0.39605748653411865, D loss: 0.0008358419872820377\n",
            "Iteration 4241/6000, G loss: 0.5209128260612488, D loss: 0.00024778489023447037\n",
            "Iteration 4242/6000, G loss: 0.4813351631164551, D loss: 0.003847218817099929\n",
            "Iteration 4243/6000, G loss: 0.5055682063102722, D loss: 0.017119422554969788\n",
            "Iteration 4244/6000, G loss: 0.49624499678611755, D loss: 0.004042662680149078\n",
            "Iteration 4245/6000, G loss: 0.5513707399368286, D loss: 0.01283392682671547\n",
            "Iteration 4246/6000, G loss: 0.4494292140007019, D loss: 0.0004452390130609274\n",
            "Iteration 4247/6000, G loss: 0.41187259554862976, D loss: 0.004715513903647661\n",
            "Iteration 4248/6000, G loss: 0.47209393978118896, D loss: 0.01924186199903488\n",
            "Iteration 4249/6000, G loss: 0.48977774381637573, D loss: 0.021045932546257973\n",
            "Iteration 4250/6000, G loss: 0.6040719747543335, D loss: 0.003764674300327897\n",
            "Iteration 4251/6000, G loss: 0.4884372055530548, D loss: 0.007842373102903366\n",
            "Iteration 4252/6000, G loss: 0.5891768336296082, D loss: 0.014139721170067787\n",
            "Iteration 4253/6000, G loss: 0.593821108341217, D loss: 0.030268467962741852\n",
            "Iteration 4254/6000, G loss: 0.4837420880794525, D loss: 0.0006730651948601007\n",
            "Iteration 4255/6000, G loss: 0.5374415516853333, D loss: 0.0940403938293457\n",
            "Iteration 4256/6000, G loss: 0.46962210536003113, D loss: 0.015015112236142159\n",
            "Iteration 4257/6000, G loss: 0.5014460682868958, D loss: 0.0036804655101150274\n",
            "Iteration 4258/6000, G loss: 0.48897045850753784, D loss: 0.004710976500064135\n",
            "Iteration 4259/6000, G loss: 0.3973981738090515, D loss: 0.02132374420762062\n",
            "Iteration 4260/6000, G loss: 0.6130136251449585, D loss: 0.00906806718558073\n",
            "Iteration 4261/6000, G loss: 0.45702236890792847, D loss: 0.6899374723434448\n",
            "Iteration 4262/6000, G loss: 0.5261226892471313, D loss: 0.00262326467782259\n",
            "Iteration 4263/6000, G loss: 0.47008761763572693, D loss: 0.0029576735105365515\n",
            "Iteration 4264/6000, G loss: 0.47132226824760437, D loss: 0.006768632680177689\n",
            "Iteration 4265/6000, G loss: 0.6313918828964233, D loss: 0.058239977806806564\n",
            "Iteration 4266/6000, G loss: 0.47688573598861694, D loss: 0.07585898786783218\n",
            "Iteration 4267/6000, G loss: 0.5733788013458252, D loss: 0.000173008389538154\n",
            "Iteration 4268/6000, G loss: 0.5742431282997131, D loss: 0.036768652498722076\n",
            "Iteration 4269/6000, G loss: 0.427656888961792, D loss: 0.000662274775095284\n",
            "Iteration 4270/6000, G loss: 0.49007225036621094, D loss: 0.03850594162940979\n",
            "Iteration 4271/6000, G loss: 0.5374565124511719, D loss: 0.014006651937961578\n",
            "Iteration 4272/6000, G loss: 0.5509417057037354, D loss: 0.0054752882570028305\n",
            "Iteration 4273/6000, G loss: 0.46873143315315247, D loss: 0.18351833522319794\n",
            "Iteration 4274/6000, G loss: 0.5511692762374878, D loss: 0.08149050176143646\n",
            "Iteration 4275/6000, G loss: 0.3880041241645813, D loss: 0.8543180227279663\n",
            "Iteration 4276/6000, G loss: 0.4553765654563904, D loss: 4.5556230545043945\n",
            "Iteration 4277/6000, G loss: 0.5469942688941956, D loss: 0.0029218848794698715\n",
            "Iteration 4278/6000, G loss: 0.5284996032714844, D loss: 0.0017221017042174935\n",
            "Iteration 4279/6000, G loss: 0.5863033533096313, D loss: 0.0002374249161221087\n",
            "Iteration 4280/6000, G loss: 0.5563746690750122, D loss: 0.0007783835753798485\n",
            "Iteration 4281/6000, G loss: 0.5832151174545288, D loss: 0.00039389118319377303\n",
            "Iteration 4282/6000, G loss: 0.5283060073852539, D loss: 4.898217594018206e-05\n",
            "Iteration 4283/6000, G loss: 0.4824290871620178, D loss: 0.02064700424671173\n",
            "Iteration 4284/6000, G loss: 0.435605525970459, D loss: 0.001449268776923418\n",
            "Iteration 4285/6000, G loss: 0.6042572855949402, D loss: 0.03732343018054962\n",
            "Iteration 4286/6000, G loss: 0.45613762736320496, D loss: 0.009927932173013687\n",
            "Iteration 4287/6000, G loss: 0.5214579701423645, D loss: 0.0017897190991789103\n",
            "Iteration 4288/6000, G loss: 0.618563711643219, D loss: 0.0007820766186341643\n",
            "Iteration 4289/6000, G loss: 0.4887516498565674, D loss: 0.000256645173067227\n",
            "Iteration 4290/6000, G loss: 0.5848696231842041, D loss: 0.00511527992784977\n",
            "Iteration 4291/6000, G loss: 0.49771401286125183, D loss: 0.0007483311928808689\n",
            "Iteration 4292/6000, G loss: 0.4429798722267151, D loss: 0.0021186028607189655\n",
            "Iteration 4293/6000, G loss: 0.4108297824859619, D loss: 0.001188925700262189\n",
            "Iteration 4294/6000, G loss: 0.5018121600151062, D loss: 0.009576069191098213\n",
            "Iteration 4295/6000, G loss: 0.505611002445221, D loss: 0.02694094181060791\n",
            "Iteration 4296/6000, G loss: 0.5206658840179443, D loss: 0.025389183312654495\n",
            "Iteration 4297/6000, G loss: 0.5224671959877014, D loss: 0.005190125200897455\n",
            "Iteration 4298/6000, G loss: 0.5439712405204773, D loss: 0.013635920360684395\n",
            "Iteration 4299/6000, G loss: 0.4604443907737732, D loss: 0.011493660509586334\n",
            "Iteration 4300/6000, G loss: 0.44715744256973267, D loss: 0.002544279210269451\n",
            "Iteration 4301/6000, G loss: 0.5317466855049133, D loss: 0.0007061200449243188\n",
            "Iteration 4302/6000, G loss: 0.5467352867126465, D loss: 0.004506308119744062\n",
            "Iteration 4303/6000, G loss: 0.4595484435558319, D loss: 0.001944208168424666\n",
            "Iteration 4304/6000, G loss: 0.4215506911277771, D loss: 0.004467306192964315\n",
            "Iteration 4305/6000, G loss: 0.43687504529953003, D loss: 0.001417596242390573\n",
            "Iteration 4306/6000, G loss: 0.45038166642189026, D loss: 0.030551588162779808\n",
            "Iteration 4307/6000, G loss: 0.45037785172462463, D loss: 0.00877490546554327\n",
            "Iteration 4308/6000, G loss: 0.5073215961456299, D loss: 0.04710536450147629\n",
            "Iteration 4309/6000, G loss: 0.47201424837112427, D loss: 0.00907324068248272\n",
            "Iteration 4310/6000, G loss: 0.5558038949966431, D loss: 0.008982570841908455\n",
            "Iteration 4311/6000, G loss: 0.3931947946548462, D loss: 0.07226741313934326\n",
            "Iteration 4312/6000, G loss: 0.5486787557601929, D loss: 0.014931181445717812\n",
            "Iteration 4313/6000, G loss: 0.550324022769928, D loss: 0.008575738407671452\n",
            "Iteration 4314/6000, G loss: 0.5184891819953918, D loss: 0.020940570160746574\n",
            "Iteration 4315/6000, G loss: 0.48571041226387024, D loss: 0.00992502085864544\n",
            "Iteration 4316/6000, G loss: 0.5341632962226868, D loss: 0.013848069123923779\n",
            "Iteration 4317/6000, G loss: 0.5787820219993591, D loss: 0.0014469949528574944\n",
            "Iteration 4318/6000, G loss: 0.5269904732704163, D loss: 0.006402494851499796\n",
            "Iteration 4319/6000, G loss: 0.49033743143081665, D loss: 0.006973305717110634\n",
            "Iteration 4320/6000, G loss: 0.5411110520362854, D loss: 0.018863528966903687\n",
            "Iteration 4321/6000, G loss: 0.39478039741516113, D loss: 0.0076482174918055534\n",
            "Iteration 4322/6000, G loss: 0.4827258884906769, D loss: 0.03365423530340195\n",
            "Iteration 4323/6000, G loss: 0.5878059267997742, D loss: 0.0013949335552752018\n",
            "Iteration 4324/6000, G loss: 0.7584581971168518, D loss: 0.12283878773450851\n",
            "Iteration 4325/6000, G loss: 0.47305169701576233, D loss: 0.04774842411279678\n",
            "Iteration 4326/6000, G loss: 0.46229737997055054, D loss: 0.011432343162596226\n",
            "Iteration 4327/6000, G loss: 0.45283228158950806, D loss: 0.052539046853780746\n",
            "Iteration 4328/6000, G loss: 0.47534018754959106, D loss: 0.0019185661803930998\n",
            "Iteration 4329/6000, G loss: 0.43791747093200684, D loss: 0.003977187443524599\n",
            "Iteration 4330/6000, G loss: 0.4595780372619629, D loss: 6.344206485664472e-05\n",
            "Iteration 4331/6000, G loss: 0.4796299934387207, D loss: 0.010492514818906784\n",
            "Iteration 4332/6000, G loss: 0.5186575651168823, D loss: 0.09462961554527283\n",
            "Iteration 4333/6000, G loss: 0.45020347833633423, D loss: 0.9517291188240051\n",
            "Iteration 4334/6000, G loss: 0.40355610847473145, D loss: 0.020827660337090492\n",
            "Iteration 4335/6000, G loss: 0.4928853213787079, D loss: 0.18518772721290588\n",
            "Iteration 4336/6000, G loss: 0.5703276991844177, D loss: 0.003560330718755722\n",
            "Iteration 4337/6000, G loss: 0.38286644220352173, D loss: 0.14933975040912628\n",
            "Iteration 4338/6000, G loss: 0.4592025876045227, D loss: 2.384566307067871\n",
            "Iteration 4339/6000, G loss: 0.4981571137905121, D loss: 0.016994820907711983\n",
            "Iteration 4340/6000, G loss: 0.5139790177345276, D loss: 0.0006396265234798193\n",
            "Iteration 4341/6000, G loss: 0.5349456071853638, D loss: 0.011190928518772125\n",
            "Iteration 4342/6000, G loss: 0.5563575029373169, D loss: 0.06316328048706055\n",
            "Iteration 4343/6000, G loss: 0.450713574886322, D loss: 0.048717349767684937\n",
            "Iteration 4344/6000, G loss: 0.5349822640419006, D loss: 0.06736704707145691\n",
            "Iteration 4345/6000, G loss: 0.5250649452209473, D loss: 0.45904654264450073\n",
            "Iteration 4346/6000, G loss: 0.5826870203018188, D loss: 0.000333713716827333\n",
            "Iteration 4347/6000, G loss: 0.5776973962783813, D loss: 0.007125411182641983\n",
            "Iteration 4348/6000, G loss: 0.4130915105342865, D loss: 0.13823743164539337\n",
            "Iteration 4349/6000, G loss: 0.5298187732696533, D loss: 0.001151516567915678\n",
            "Iteration 4350/6000, G loss: 0.4711853265762329, D loss: 0.0006372201023623347\n",
            "Iteration 4351/6000, G loss: 0.5325974225997925, D loss: 0.0006528033409267664\n",
            "Iteration 4352/6000, G loss: 0.5399979948997498, D loss: 0.006563506089150906\n",
            "Iteration 4353/6000, G loss: 0.5275219082832336, D loss: 0.004912900738418102\n",
            "Iteration 4354/6000, G loss: 0.526049792766571, D loss: 0.051590222865343094\n",
            "Iteration 4355/6000, G loss: 0.6061393618583679, D loss: 0.01717190444469452\n",
            "Iteration 4356/6000, G loss: 0.4744294583797455, D loss: 0.015660645440220833\n",
            "Iteration 4357/6000, G loss: 0.360919713973999, D loss: 0.051956336945295334\n",
            "Iteration 4358/6000, G loss: 0.574731171131134, D loss: 0.0006856764084659517\n",
            "Iteration 4359/6000, G loss: 0.45695555210113525, D loss: 0.001569985644891858\n",
            "Iteration 4360/6000, G loss: 0.4383241534233093, D loss: 0.0016754998359829187\n",
            "Iteration 4361/6000, G loss: 0.4921970069408417, D loss: 0.3157818615436554\n",
            "Iteration 4362/6000, G loss: 0.3671087920665741, D loss: 0.06393028795719147\n",
            "Iteration 4363/6000, G loss: 0.514029324054718, D loss: 0.03276653587818146\n",
            "Iteration 4364/6000, G loss: 0.47606950998306274, D loss: 0.0052918922156095505\n",
            "Iteration 4365/6000, G loss: 0.5537141561508179, D loss: 0.001115552382543683\n",
            "Iteration 4366/6000, G loss: 0.5044444799423218, D loss: 0.00018766088760457933\n",
            "Iteration 4367/6000, G loss: 0.45775192975997925, D loss: 0.00153985689394176\n",
            "Iteration 4368/6000, G loss: 0.5468388199806213, D loss: 0.05541011318564415\n",
            "Iteration 4369/6000, G loss: 0.6921488046646118, D loss: 0.3693118989467621\n",
            "Iteration 4370/6000, G loss: 0.47737014293670654, D loss: 0.0002930927730631083\n",
            "Iteration 4371/6000, G loss: 0.4863815903663635, D loss: 1.739252547849901e-05\n",
            "Iteration 4372/6000, G loss: 0.5794321298599243, D loss: 0.0006917430437169969\n",
            "Iteration 4373/6000, G loss: 0.44372856616973877, D loss: 7.077856571413577e-05\n",
            "Iteration 4374/6000, G loss: 0.45856818556785583, D loss: 4.035225629195338e-06\n",
            "Iteration 4375/6000, G loss: 0.45993584394454956, D loss: 0.018616914749145508\n",
            "Iteration 4376/6000, G loss: 0.5811980366706848, D loss: 0.877790093421936\n",
            "Iteration 4377/6000, G loss: 0.44436419010162354, D loss: 3.555248975753784\n",
            "Iteration 4378/6000, G loss: 0.5990152955055237, D loss: 0.017720073461532593\n",
            "Iteration 4379/6000, G loss: 0.536737859249115, D loss: 2.8220551030244678e-05\n",
            "Iteration 4380/6000, G loss: 0.40569713711738586, D loss: 0.0019409925444051623\n",
            "Iteration 4381/6000, G loss: 0.5811014771461487, D loss: 0.0005977106047794223\n",
            "Iteration 4382/6000, G loss: 0.5294653177261353, D loss: 9.429326746612787e-05\n",
            "Iteration 4383/6000, G loss: 0.5131396651268005, D loss: 0.0002947233442682773\n",
            "Iteration 4384/6000, G loss: 0.4086993932723999, D loss: 0.006086320150643587\n",
            "Iteration 4385/6000, G loss: 0.5272419452667236, D loss: 0.10585609078407288\n",
            "Iteration 4386/6000, G loss: 0.413913756608963, D loss: 0.0816306322813034\n",
            "Iteration 4387/6000, G loss: 0.5308856964111328, D loss: 0.039393313229084015\n",
            "Iteration 4388/6000, G loss: 0.6614972352981567, D loss: 0.004662198014557362\n",
            "Iteration 4389/6000, G loss: 0.4795592129230499, D loss: 0.0063892630860209465\n",
            "Iteration 4390/6000, G loss: 0.3906882405281067, D loss: 0.1156291663646698\n",
            "Iteration 4391/6000, G loss: 0.5351080894470215, D loss: 0.026914244517683983\n",
            "Iteration 4392/6000, G loss: 0.44489383697509766, D loss: 0.016681263223290443\n",
            "Iteration 4393/6000, G loss: 0.4828340709209442, D loss: 0.00424797460436821\n",
            "Iteration 4394/6000, G loss: 0.49822893738746643, D loss: 0.04759125038981438\n",
            "Iteration 4395/6000, G loss: 0.5744258761405945, D loss: 0.09390080720186234\n",
            "Iteration 4396/6000, G loss: 0.5625281929969788, D loss: 0.08126355707645416\n",
            "Iteration 4397/6000, G loss: 0.5660810470581055, D loss: 0.02068485878407955\n",
            "Iteration 4398/6000, G loss: 0.5934382677078247, D loss: 0.007643489167094231\n",
            "Iteration 4399/6000, G loss: 0.5094974040985107, D loss: 0.005330310203135014\n",
            "Iteration 4400/6000, G loss: 0.5058955550193787, D loss: 0.00041374180000275373\n",
            "Iteration 4401/6000, G loss: 0.5018684267997742, D loss: 0.001331619336269796\n",
            "Iteration 4402/6000, G loss: 0.4764614999294281, D loss: 0.012304481118917465\n",
            "Iteration 4403/6000, G loss: 0.4435729682445526, D loss: 0.0012741637183353305\n",
            "Iteration 4404/6000, G loss: 0.48596134781837463, D loss: 0.03637346997857094\n",
            "Iteration 4405/6000, G loss: 0.509320855140686, D loss: 0.06213253736495972\n",
            "Iteration 4406/6000, G loss: 0.4904075562953949, D loss: 0.01067326869815588\n",
            "Iteration 4407/6000, G loss: 0.4552120268344879, D loss: 0.017938077449798584\n",
            "Iteration 4408/6000, G loss: 0.5563125014305115, D loss: 0.0009386216988787055\n",
            "Iteration 4409/6000, G loss: 0.591702938079834, D loss: 0.000336945871822536\n",
            "Iteration 4410/6000, G loss: 0.561859130859375, D loss: 0.0002973195514641702\n",
            "Iteration 4411/6000, G loss: 0.4988965392112732, D loss: 0.011776655912399292\n",
            "Iteration 4412/6000, G loss: 0.5538773536682129, D loss: 0.003976266831159592\n",
            "Iteration 4413/6000, G loss: 0.514950692653656, D loss: 4.6955821744631976e-05\n",
            "Iteration 4414/6000, G loss: 0.6358123421669006, D loss: 0.002186079043895006\n",
            "Iteration 4415/6000, G loss: 0.5800257921218872, D loss: 5.8167046518065035e-05\n",
            "Iteration 4416/6000, G loss: 0.4276202619075775, D loss: 0.0004958250792697072\n",
            "Iteration 4417/6000, G loss: 0.3977973163127899, D loss: 0.0024301279336214066\n",
            "Iteration 4418/6000, G loss: 0.5743675231933594, D loss: 0.0728202760219574\n",
            "Iteration 4419/6000, G loss: 0.46044307947158813, D loss: 0.0013025328516960144\n",
            "Iteration 4420/6000, G loss: 0.5016355514526367, D loss: 0.0920257717370987\n",
            "Iteration 4421/6000, G loss: 0.6242615580558777, D loss: 0.0008092350326478481\n",
            "Iteration 4422/6000, G loss: 0.5054268836975098, D loss: 0.0009834256488829851\n",
            "Iteration 4423/6000, G loss: 0.4433257579803467, D loss: 0.0020970506593585014\n",
            "Iteration 4424/6000, G loss: 0.5398053526878357, D loss: 0.0010226191952824593\n",
            "Iteration 4425/6000, G loss: 0.3875987231731415, D loss: 0.006418217439204454\n",
            "Iteration 4426/6000, G loss: 0.5405008792877197, D loss: 0.013539230450987816\n",
            "Iteration 4427/6000, G loss: 0.47702860832214355, D loss: 0.4850844740867615\n",
            "Iteration 4428/6000, G loss: 0.6331377625465393, D loss: 0.12471778690814972\n",
            "Iteration 4429/6000, G loss: 0.5174080729484558, D loss: 0.00036553043173626065\n",
            "Iteration 4430/6000, G loss: 0.6579875946044922, D loss: 0.006725100800395012\n",
            "Iteration 4431/6000, G loss: 0.485751211643219, D loss: 0.005403955467045307\n",
            "Iteration 4432/6000, G loss: 0.543652355670929, D loss: 0.05497698485851288\n",
            "Iteration 4433/6000, G loss: 0.5524485111236572, D loss: 0.10131296515464783\n",
            "Iteration 4434/6000, G loss: 0.4509478211402893, D loss: 5.820286241942085e-05\n",
            "Iteration 4435/6000, G loss: 0.46505773067474365, D loss: 0.0009193285368382931\n",
            "Iteration 4436/6000, G loss: 0.4748430848121643, D loss: 0.0012039269786328077\n",
            "Iteration 4437/6000, G loss: 0.5287438035011292, D loss: 0.19913268089294434\n",
            "Iteration 4438/6000, G loss: 0.5283048152923584, D loss: 0.15304958820343018\n",
            "Iteration 4439/6000, G loss: 0.5389857292175293, D loss: 0.5276848077774048\n",
            "Iteration 4440/6000, G loss: 0.4741610586643219, D loss: 0.13794931769371033\n",
            "Iteration 4441/6000, G loss: 0.5107990503311157, D loss: 0.0033859233371913433\n",
            "Iteration 4442/6000, G loss: 0.3837338089942932, D loss: 0.33891767263412476\n",
            "Iteration 4443/6000, G loss: 0.5238091945648193, D loss: 1.4357702732086182\n",
            "Iteration 4444/6000, G loss: 0.5487733483314514, D loss: 0.21378856897354126\n",
            "Iteration 4445/6000, G loss: 0.47786155343055725, D loss: 0.8181851506233215\n",
            "Iteration 4446/6000, G loss: 0.5109013915061951, D loss: 3.421268775127828e-05\n",
            "Iteration 4447/6000, G loss: 0.6778363585472107, D loss: 0.003533783368766308\n",
            "Iteration 4448/6000, G loss: 0.5212239623069763, D loss: 0.0017045902786776423\n",
            "Iteration 4449/6000, G loss: 0.5765796303749084, D loss: 6.020065939082997e-07\n",
            "Iteration 4450/6000, G loss: 0.44225943088531494, D loss: 0.0019395665731281042\n",
            "Iteration 4451/6000, G loss: 0.5093558430671692, D loss: 1.5413426808663644e-05\n",
            "Iteration 4452/6000, G loss: 0.494573712348938, D loss: 0.2539277970790863\n",
            "Iteration 4453/6000, G loss: 0.4127354621887207, D loss: 0.01359681785106659\n",
            "Iteration 4454/6000, G loss: 0.5032199621200562, D loss: 2.310879945755005\n",
            "Iteration 4455/6000, G loss: 0.5924855470657349, D loss: 0.0007278701523318887\n",
            "Iteration 4456/6000, G loss: 0.5422856211662292, D loss: 0.0002480478724464774\n",
            "Iteration 4457/6000, G loss: 0.4324059784412384, D loss: 0.2384343296289444\n",
            "Iteration 4458/6000, G loss: 0.6002143025398254, D loss: 0.029903030022978783\n",
            "Iteration 4459/6000, G loss: 0.3823939263820648, D loss: 0.13201481103897095\n",
            "Iteration 4460/6000, G loss: 0.547590970993042, D loss: 0.0031594173051416874\n",
            "Iteration 4461/6000, G loss: 0.4870453476905823, D loss: 0.00035659701097756624\n",
            "Iteration 4462/6000, G loss: 0.5088434219360352, D loss: 0.05456662178039551\n",
            "Iteration 4463/6000, G loss: 0.5890020728111267, D loss: 0.024418193846940994\n",
            "Iteration 4464/6000, G loss: 0.5905746221542358, D loss: 0.1329299509525299\n",
            "Iteration 4465/6000, G loss: 0.5719096064567566, D loss: 0.03550037741661072\n",
            "Iteration 4466/6000, G loss: 0.4722484052181244, D loss: 0.006161990575492382\n",
            "Iteration 4467/6000, G loss: 0.5173487663269043, D loss: 0.00014704455679748207\n",
            "Iteration 4468/6000, G loss: 0.5697788000106812, D loss: 3.290076710982248e-05\n",
            "Iteration 4469/6000, G loss: 0.4800023138523102, D loss: 0.006306272000074387\n",
            "Iteration 4470/6000, G loss: 0.5359089970588684, D loss: 0.0005097334505990148\n",
            "Iteration 4471/6000, G loss: 0.5430902242660522, D loss: 0.7361078262329102\n",
            "Iteration 4472/6000, G loss: 0.5320988893508911, D loss: 0.0002902178093791008\n",
            "Iteration 4473/6000, G loss: 0.5618243217468262, D loss: 0.04657292366027832\n",
            "Iteration 4474/6000, G loss: 0.5440906286239624, D loss: 0.00034961794153787196\n",
            "Iteration 4475/6000, G loss: 0.47403258085250854, D loss: 0.024631459265947342\n",
            "Iteration 4476/6000, G loss: 0.5578752756118774, D loss: 0.45623496174812317\n",
            "Iteration 4477/6000, G loss: 0.4580557942390442, D loss: 0.14711853861808777\n",
            "Iteration 4478/6000, G loss: 0.6214861273765564, D loss: 0.0012985758949071169\n",
            "Iteration 4479/6000, G loss: 0.4532822370529175, D loss: 0.007686631754040718\n",
            "Iteration 4480/6000, G loss: 0.5357528924942017, D loss: 0.05310659855604172\n",
            "Iteration 4481/6000, G loss: 0.5350863337516785, D loss: 0.03724800422787666\n",
            "Iteration 4482/6000, G loss: 0.456788033246994, D loss: 0.2791000008583069\n",
            "Iteration 4483/6000, G loss: 0.518814206123352, D loss: 0.011389699764549732\n",
            "Iteration 4484/6000, G loss: 0.42411914467811584, D loss: 0.08719341456890106\n",
            "Iteration 4485/6000, G loss: 0.6121093034744263, D loss: 1.8891223669052124\n",
            "Iteration 4486/6000, G loss: 0.5582708716392517, D loss: 0.5009691715240479\n",
            "Iteration 4487/6000, G loss: 0.5375889539718628, D loss: 0.019672304391860962\n",
            "Iteration 4488/6000, G loss: 0.60313880443573, D loss: 7.636870577698573e-05\n",
            "Iteration 4489/6000, G loss: 0.6186338067054749, D loss: 0.004644544329494238\n",
            "Iteration 4490/6000, G loss: 0.4442947208881378, D loss: 0.0015437717083841562\n",
            "Iteration 4491/6000, G loss: 0.4637424945831299, D loss: 0.45928776264190674\n",
            "Iteration 4492/6000, G loss: 0.46345797181129456, D loss: 0.00024872543872334063\n",
            "Iteration 4493/6000, G loss: 0.5437484979629517, D loss: 0.916091799736023\n",
            "Iteration 4494/6000, G loss: 0.6156895160675049, D loss: 0.7059746980667114\n",
            "Iteration 4495/6000, G loss: 0.5765700340270996, D loss: 0.0002997230039909482\n",
            "Iteration 4496/6000, G loss: 0.42990297079086304, D loss: 0.027888549491763115\n",
            "Iteration 4497/6000, G loss: 0.5167187452316284, D loss: 0.036796391010284424\n",
            "Iteration 4498/6000, G loss: 0.43065255880355835, D loss: 0.07049857079982758\n",
            "Iteration 4499/6000, G loss: 0.43689489364624023, D loss: 0.04841786250472069\n",
            "Iteration 4500/6000, G loss: 0.4889482855796814, D loss: 0.13227242231369019\n",
            "Iteration 4501/6000, G loss: 0.5177040696144104, D loss: 0.01683051325380802\n",
            "Iteration 4502/6000, G loss: 0.4326627254486084, D loss: 0.0016486074309796095\n",
            "Iteration 4503/6000, G loss: 0.528299868106842, D loss: 0.012604336254298687\n",
            "Iteration 4504/6000, G loss: 0.49043673276901245, D loss: 0.11078104376792908\n",
            "Iteration 4505/6000, G loss: 0.4241805672645569, D loss: 1.4999544620513916\n",
            "Iteration 4506/6000, G loss: 0.5031194090843201, D loss: 0.10343212634325027\n",
            "Iteration 4507/6000, G loss: 0.47498103976249695, D loss: 0.2589506506919861\n",
            "Iteration 4508/6000, G loss: 0.480008065700531, D loss: 0.05695730447769165\n",
            "Iteration 4509/6000, G loss: 0.38031435012817383, D loss: 1.1012026071548462\n",
            "Iteration 4510/6000, G loss: 0.6080738306045532, D loss: 0.00020047425641678274\n",
            "Iteration 4511/6000, G loss: 0.5587697625160217, D loss: 2.195801971538458e-05\n",
            "Iteration 4512/6000, G loss: 0.6706165075302124, D loss: 0.0008037807419896126\n",
            "Iteration 4513/6000, G loss: 0.45570069551467896, D loss: 0.0001276249240618199\n",
            "Iteration 4514/6000, G loss: 0.5804900527000427, D loss: 0.013750540092587471\n",
            "Iteration 4515/6000, G loss: 0.4377409517765045, D loss: 0.10454320907592773\n",
            "Iteration 4516/6000, G loss: 0.5494414567947388, D loss: 0.0005152945523150265\n",
            "Iteration 4517/6000, G loss: 0.5274179577827454, D loss: 0.014924637041985989\n",
            "Iteration 4518/6000, G loss: 0.43569496273994446, D loss: 6.496849437098717e-06\n",
            "Iteration 4519/6000, G loss: 0.5823634266853333, D loss: 0.002957697492092848\n",
            "Iteration 4520/6000, G loss: 0.4060637652873993, D loss: 0.005866688676178455\n",
            "Iteration 4521/6000, G loss: 0.4585239887237549, D loss: 7.612341141793877e-05\n",
            "Iteration 4522/6000, G loss: 0.5841095447540283, D loss: 0.002361029153689742\n",
            "Iteration 4523/6000, G loss: 0.5849882364273071, D loss: 0.5041930675506592\n",
            "Iteration 4524/6000, G loss: 0.5656365752220154, D loss: 0.08911363780498505\n",
            "Iteration 4525/6000, G loss: 0.5705101490020752, D loss: 0.00011252909462200478\n",
            "Iteration 4526/6000, G loss: 0.39294835925102234, D loss: 0.049502357840538025\n",
            "Iteration 4527/6000, G loss: 0.5757958889007568, D loss: 0.0002621499006636441\n",
            "Iteration 4528/6000, G loss: 0.41319718956947327, D loss: 0.0023737712763249874\n",
            "Iteration 4529/6000, G loss: 0.4993804395198822, D loss: 0.0020326757803559303\n",
            "Iteration 4530/6000, G loss: 0.41064655780792236, D loss: 0.011299381032586098\n",
            "Iteration 4531/6000, G loss: 0.5284948945045471, D loss: 0.16636189818382263\n",
            "Iteration 4532/6000, G loss: 0.5768042206764221, D loss: 0.18588799238204956\n",
            "Iteration 4533/6000, G loss: 0.6426297426223755, D loss: 0.22231966257095337\n",
            "Iteration 4534/6000, G loss: 0.5725281238555908, D loss: 2.48906399065163e-05\n",
            "Iteration 4535/6000, G loss: 0.5350990295410156, D loss: 0.00026514744968153536\n",
            "Iteration 4536/6000, G loss: 0.46107643842697144, D loss: 4.035228812426794e-06\n",
            "Iteration 4537/6000, G loss: 0.5626755356788635, D loss: 0.0004785526543855667\n",
            "Iteration 4538/6000, G loss: 0.5012599229812622, D loss: 0.03506094589829445\n",
            "Iteration 4539/6000, G loss: 0.5370091795921326, D loss: 3.9162306785583496\n",
            "Iteration 4540/6000, G loss: 0.41680610179901123, D loss: 0.10940166562795639\n",
            "Iteration 4541/6000, G loss: 0.41988569498062134, D loss: 5.0441045459592715e-05\n",
            "Iteration 4542/6000, G loss: 0.5696783065795898, D loss: 0.0005645165219902992\n",
            "Iteration 4543/6000, G loss: 0.4810289144515991, D loss: 0.0026267622597515583\n",
            "Iteration 4544/6000, G loss: 0.43574219942092896, D loss: 1.9371491362107918e-06\n",
            "Iteration 4545/6000, G loss: 0.503533661365509, D loss: 0.013085287995636463\n",
            "Iteration 4546/6000, G loss: 0.47843697667121887, D loss: 0.0005335301393643022\n",
            "Iteration 4547/6000, G loss: 0.5974066853523254, D loss: 0.013806246221065521\n",
            "Iteration 4548/6000, G loss: 0.48137131333351135, D loss: 0.0004093472962267697\n",
            "Iteration 4549/6000, G loss: 0.4758831858634949, D loss: 0.15524381399154663\n",
            "Iteration 4550/6000, G loss: 0.565536618232727, D loss: 0.04077897220849991\n",
            "Iteration 4551/6000, G loss: 0.5191974639892578, D loss: 0.07287566363811493\n",
            "Iteration 4552/6000, G loss: 0.5476313829421997, D loss: 0.16484755277633667\n",
            "Iteration 4553/6000, G loss: 0.49403560161590576, D loss: 0.47724026441574097\n",
            "Iteration 4554/6000, G loss: 0.5902567505836487, D loss: 0.07739469408988953\n",
            "Iteration 4555/6000, G loss: 0.4686100482940674, D loss: 0.0001254692324437201\n",
            "Iteration 4556/6000, G loss: 0.4728999733924866, D loss: 0.03873267397284508\n",
            "Iteration 4557/6000, G loss: 0.4042624831199646, D loss: 0.055495016276836395\n",
            "Iteration 4558/6000, G loss: 0.4624946117401123, D loss: 0.005162861198186874\n",
            "Iteration 4559/6000, G loss: 0.5346657633781433, D loss: 0.00257959496229887\n",
            "Iteration 4560/6000, G loss: 0.5341643691062927, D loss: 0.02046811580657959\n",
            "Iteration 4561/6000, G loss: 0.45199859142303467, D loss: 0.0015060107689350843\n",
            "Iteration 4562/6000, G loss: 0.49091142416000366, D loss: 0.0069100018590688705\n",
            "Iteration 4563/6000, G loss: 0.4540834426879883, D loss: 0.0034197282511740923\n",
            "Iteration 4564/6000, G loss: 0.5393993258476257, D loss: 0.008822419680655003\n",
            "Iteration 4565/6000, G loss: 0.38497141003608704, D loss: 0.0003722549881786108\n",
            "Iteration 4566/6000, G loss: 0.46324867010116577, D loss: 0.012380970641970634\n",
            "Iteration 4567/6000, G loss: 0.5328068733215332, D loss: 0.10880818217992783\n",
            "Iteration 4568/6000, G loss: 0.5728506445884705, D loss: 0.002475370652973652\n",
            "Iteration 4569/6000, G loss: 0.4668791592121124, D loss: 0.0009626178070902824\n",
            "Iteration 4570/6000, G loss: 0.45709875226020813, D loss: 0.004171233624219894\n",
            "Iteration 4571/6000, G loss: 0.5239809155464172, D loss: 0.1430462747812271\n",
            "Iteration 4572/6000, G loss: 0.5481696724891663, D loss: 0.01708914339542389\n",
            "Iteration 4573/6000, G loss: 0.4551095962524414, D loss: 0.0943073257803917\n",
            "Iteration 4574/6000, G loss: 0.38846635818481445, D loss: 0.7490176558494568\n",
            "Iteration 4575/6000, G loss: 0.5322676301002502, D loss: 0.0007937734480947256\n",
            "Iteration 4576/6000, G loss: 0.5109593272209167, D loss: 0.0007549806032329798\n",
            "Iteration 4577/6000, G loss: 0.48420339822769165, D loss: 0.000103242346085608\n",
            "Iteration 4578/6000, G loss: 0.5084657669067383, D loss: 0.016035331413149834\n",
            "Iteration 4579/6000, G loss: 0.459585577249527, D loss: 0.3647271394729614\n",
            "Iteration 4580/6000, G loss: 0.5221319198608398, D loss: 3.4438552856445312\n",
            "Iteration 4581/6000, G loss: 0.5619264841079712, D loss: 1.0033378601074219\n",
            "Iteration 4582/6000, G loss: 0.5145654678344727, D loss: 0.005853723734617233\n",
            "Iteration 4583/6000, G loss: 0.6164260506629944, D loss: 0.0016394492704421282\n",
            "Iteration 4584/6000, G loss: 0.4644586741924286, D loss: 1.56164071540843e-06\n",
            "Iteration 4585/6000, G loss: 0.5729476809501648, D loss: 2.753731678240001e-06\n",
            "Iteration 4586/6000, G loss: 0.5834489464759827, D loss: 0.004084021784365177\n",
            "Iteration 4587/6000, G loss: 0.5782950520515442, D loss: 0.005995492450892925\n",
            "Iteration 4588/6000, G loss: 0.43755730986595154, D loss: 0.04755792021751404\n",
            "Iteration 4589/6000, G loss: 0.6687430143356323, D loss: 0.1013932004570961\n",
            "Iteration 4590/6000, G loss: 0.48454466462135315, D loss: 0.10596705973148346\n",
            "Iteration 4591/6000, G loss: 0.5651473999023438, D loss: 0.0040227919816970825\n",
            "Iteration 4592/6000, G loss: 0.40202751755714417, D loss: 0.00016626139404252172\n",
            "Iteration 4593/6000, G loss: 0.5532208681106567, D loss: 1.5795221770531498e-06\n",
            "Iteration 4594/6000, G loss: 0.5009095668792725, D loss: 8.241600880865008e-05\n",
            "Iteration 4595/6000, G loss: 0.5516809225082397, D loss: 0.0001702567533357069\n",
            "Iteration 4596/6000, G loss: 0.48856163024902344, D loss: 0.000984686310403049\n",
            "Iteration 4597/6000, G loss: 0.46882039308547974, D loss: 0.11677660793066025\n",
            "Iteration 4598/6000, G loss: 0.5375534296035767, D loss: 3.224609375\n",
            "Iteration 4599/6000, G loss: 0.491738498210907, D loss: 0.23706099390983582\n",
            "Iteration 4600/6000, G loss: 0.4647500813007355, D loss: 0.269997239112854\n",
            "Iteration 4601/6000, G loss: 0.5824490189552307, D loss: 0.034182507544755936\n",
            "Iteration 4602/6000, G loss: 0.5699160099029541, D loss: 0.0004248806508257985\n",
            "Iteration 4603/6000, G loss: 0.5269998908042908, D loss: 8.421641541644931e-05\n",
            "Iteration 4604/6000, G loss: 0.5775636434555054, D loss: 0.21750900149345398\n",
            "Iteration 4605/6000, G loss: 0.5068211555480957, D loss: 0.0017501718830317259\n",
            "Iteration 4606/6000, G loss: 0.48053812980651855, D loss: 0.02982988953590393\n",
            "Iteration 4607/6000, G loss: 0.41167810559272766, D loss: 0.0006053242832422256\n",
            "Iteration 4608/6000, G loss: 0.559689462184906, D loss: 0.34368830919265747\n",
            "Iteration 4609/6000, G loss: 0.4301385283470154, D loss: 0.7645285129547119\n",
            "Iteration 4610/6000, G loss: 0.46166983246803284, D loss: 0.00012908410280942917\n",
            "Iteration 4611/6000, G loss: 0.47599488496780396, D loss: 0.0068598296493291855\n",
            "Iteration 4612/6000, G loss: 0.5383347272872925, D loss: 0.2880678176879883\n",
            "Iteration 4613/6000, G loss: 0.49254223704338074, D loss: 0.17641177773475647\n",
            "Iteration 4614/6000, G loss: 0.4893661439418793, D loss: 0.004346856381744146\n",
            "Iteration 4615/6000, G loss: 0.4726641774177551, D loss: 0.0005870040040463209\n",
            "Iteration 4616/6000, G loss: 0.4208654463291168, D loss: 0.003190780058503151\n",
            "Iteration 4617/6000, G loss: 0.5444925427436829, D loss: 0.005299746058881283\n",
            "Iteration 4618/6000, G loss: 0.5338103771209717, D loss: 0.026202194392681122\n",
            "Iteration 4619/6000, G loss: 0.5380157232284546, D loss: 0.029643524438142776\n",
            "Iteration 4620/6000, G loss: 0.6341810822486877, D loss: 0.0030223410576581955\n",
            "Iteration 4621/6000, G loss: 0.6116646528244019, D loss: 0.010981600731611252\n",
            "Iteration 4622/6000, G loss: 0.52299565076828, D loss: 0.00028583017410710454\n",
            "Iteration 4623/6000, G loss: 0.5646764636039734, D loss: 0.0027943914756178856\n",
            "Iteration 4624/6000, G loss: 0.38602107763290405, D loss: 0.033841367810964584\n",
            "Iteration 4625/6000, G loss: 0.4111763834953308, D loss: 0.006802268326282501\n",
            "Iteration 4626/6000, G loss: 0.5567187666893005, D loss: 0.0032800910994410515\n",
            "Iteration 4627/6000, G loss: 0.4529930651187897, D loss: 0.01357729360461235\n",
            "Iteration 4628/6000, G loss: 0.46529683470726013, D loss: 0.012352230958640575\n",
            "Iteration 4629/6000, G loss: 0.5427945852279663, D loss: 0.12442605197429657\n",
            "Iteration 4630/6000, G loss: 0.6068366169929504, D loss: 0.18281851708889008\n",
            "Iteration 4631/6000, G loss: 0.32739385962486267, D loss: 0.12159410119056702\n",
            "Iteration 4632/6000, G loss: 0.4855405390262604, D loss: 0.02237694151699543\n",
            "Iteration 4633/6000, G loss: 0.4510699510574341, D loss: 0.009529492817819118\n",
            "Iteration 4634/6000, G loss: 0.4921054244041443, D loss: 0.031611159443855286\n",
            "Iteration 4635/6000, G loss: 0.5573263764381409, D loss: 0.01622808910906315\n",
            "Iteration 4636/6000, G loss: 0.5818519592285156, D loss: 0.6566766500473022\n",
            "Iteration 4637/6000, G loss: 0.5106179118156433, D loss: 0.003410068806260824\n",
            "Iteration 4638/6000, G loss: 0.505548357963562, D loss: 0.0007063624216243625\n",
            "Iteration 4639/6000, G loss: 0.4154524803161621, D loss: 0.016727264970541\n",
            "Iteration 4640/6000, G loss: 0.5294695496559143, D loss: 3.042320728302002\n",
            "Iteration 4641/6000, G loss: 0.556103527545929, D loss: 0.7205724716186523\n",
            "Iteration 4642/6000, G loss: 0.4054228365421295, D loss: 0.1611526757478714\n",
            "Iteration 4643/6000, G loss: 0.4935954511165619, D loss: 0.030119113624095917\n",
            "Iteration 4644/6000, G loss: 0.5395646691322327, D loss: 0.0007630755426362157\n",
            "Iteration 4645/6000, G loss: 0.4780488610267639, D loss: 1.342243285762379e-05\n",
            "Iteration 4646/6000, G loss: 0.5311912298202515, D loss: 0.0006178995245136321\n",
            "Iteration 4647/6000, G loss: 0.655790388584137, D loss: 0.014556447975337505\n",
            "Iteration 4648/6000, G loss: 0.5616537928581238, D loss: 0.00015768241428304464\n",
            "Iteration 4649/6000, G loss: 0.45335814356803894, D loss: 0.0004460262425709516\n",
            "Iteration 4650/6000, G loss: 0.47760283946990967, D loss: 0.06512825936079025\n",
            "Iteration 4651/6000, G loss: 0.5829344987869263, D loss: 0.0530657023191452\n",
            "Iteration 4652/6000, G loss: 0.32802507281303406, D loss: 0.07188577950000763\n",
            "Iteration 4653/6000, G loss: 0.5546166300773621, D loss: 0.0027207203675061464\n",
            "Iteration 4654/6000, G loss: 0.39868849515914917, D loss: 0.006617898121476173\n",
            "Iteration 4655/6000, G loss: 0.5360910296440125, D loss: 0.001778086181730032\n",
            "Iteration 4656/6000, G loss: 0.4553987681865692, D loss: 0.001583632780238986\n",
            "Iteration 4657/6000, G loss: 0.452340692281723, D loss: 0.010422449558973312\n",
            "Iteration 4658/6000, G loss: 0.40331417322158813, D loss: 0.008148535154759884\n",
            "Iteration 4659/6000, G loss: 0.5407746434211731, D loss: 0.10700514167547226\n",
            "Iteration 4660/6000, G loss: 0.5977804064750671, D loss: 0.17181068658828735\n",
            "Iteration 4661/6000, G loss: 0.5093077421188354, D loss: 0.05499841645359993\n",
            "Iteration 4662/6000, G loss: 0.5886737108230591, D loss: 0.0011487347073853016\n",
            "Iteration 4663/6000, G loss: 0.5011855959892273, D loss: 0.013223288580775261\n",
            "Iteration 4664/6000, G loss: 0.44300663471221924, D loss: 0.004226612858474255\n",
            "Iteration 4665/6000, G loss: 0.4376239478588104, D loss: 0.14529787003993988\n",
            "Iteration 4666/6000, G loss: 0.5967494249343872, D loss: 0.35349053144454956\n",
            "Iteration 4667/6000, G loss: 0.5523243546485901, D loss: 0.024604488164186478\n",
            "Iteration 4668/6000, G loss: 0.6906661987304688, D loss: 0.005084049887955189\n",
            "Iteration 4669/6000, G loss: 0.40852805972099304, D loss: 0.0008499425603076816\n",
            "Iteration 4670/6000, G loss: 0.5213621258735657, D loss: 0.002681189449504018\n",
            "Iteration 4671/6000, G loss: 0.4862881898880005, D loss: 0.0003067664219997823\n",
            "Iteration 4672/6000, G loss: 0.4161197543144226, D loss: 0.0008407467976212502\n",
            "Iteration 4673/6000, G loss: 0.5432931780815125, D loss: 0.08596086502075195\n",
            "Iteration 4674/6000, G loss: 0.579874575138092, D loss: 0.013227025046944618\n",
            "Iteration 4675/6000, G loss: 0.44705215096473694, D loss: 2.198648452758789\n",
            "Iteration 4676/6000, G loss: 0.5196207761764526, D loss: 1.160325527191162\n",
            "Iteration 4677/6000, G loss: 0.5741398930549622, D loss: 0.0005513126961886883\n",
            "Iteration 4678/6000, G loss: 0.5688072443008423, D loss: 0.000991607317700982\n",
            "Iteration 4679/6000, G loss: 0.4692419767379761, D loss: 0.0067072222009301186\n",
            "Iteration 4680/6000, G loss: 0.5338683724403381, D loss: 8.234745473600924e-05\n",
            "Iteration 4681/6000, G loss: 0.4274108111858368, D loss: 0.09880676865577698\n",
            "Iteration 4682/6000, G loss: 0.4742465615272522, D loss: 0.01689394935965538\n",
            "Iteration 4683/6000, G loss: 0.48347359895706177, D loss: 0.017464423552155495\n",
            "Iteration 4684/6000, G loss: 0.5651811957359314, D loss: 5.0155776989413425e-05\n",
            "Iteration 4685/6000, G loss: 0.41024070978164673, D loss: 0.3964385688304901\n",
            "Iteration 4686/6000, G loss: 0.5276260375976562, D loss: 0.008188838139176369\n",
            "Iteration 4687/6000, G loss: 0.4970899820327759, D loss: 0.012463494203984737\n",
            "Iteration 4688/6000, G loss: 0.5817801356315613, D loss: 0.0011012698523700237\n",
            "Iteration 4689/6000, G loss: 0.4380727708339691, D loss: 0.03173992037773132\n",
            "Iteration 4690/6000, G loss: 0.5857624411582947, D loss: 0.002334366785362363\n",
            "Iteration 4691/6000, G loss: 0.4754125773906708, D loss: 0.001742189284414053\n",
            "Iteration 4692/6000, G loss: 0.5166666507720947, D loss: 0.0003917492867913097\n",
            "Iteration 4693/6000, G loss: 0.49314969778060913, D loss: 0.004399699158966541\n",
            "Iteration 4694/6000, G loss: 0.41826894879341125, D loss: 0.016575999557971954\n",
            "Iteration 4695/6000, G loss: 0.6391333341598511, D loss: 0.0010290882783010602\n",
            "Iteration 4696/6000, G loss: 0.5585049390792847, D loss: 1.131155252456665\n",
            "Iteration 4697/6000, G loss: 0.47663113474845886, D loss: 0.010685848072171211\n",
            "Iteration 4698/6000, G loss: 0.505482017993927, D loss: 0.003109897021204233\n",
            "Iteration 4699/6000, G loss: 0.6065732836723328, D loss: 0.000437072099884972\n",
            "Iteration 4700/6000, G loss: 0.584105908870697, D loss: 0.0001435799931641668\n",
            "Iteration 4701/6000, G loss: 0.518028199672699, D loss: 0.003255479969084263\n",
            "Iteration 4702/6000, G loss: 0.5259063243865967, D loss: 0.08089994639158249\n",
            "Iteration 4703/6000, G loss: 0.5698181986808777, D loss: 0.023975485935807228\n",
            "Iteration 4704/6000, G loss: 0.574800431728363, D loss: 0.08835801482200623\n",
            "Iteration 4705/6000, G loss: 0.4850238859653473, D loss: 0.0027207506354898214\n",
            "Iteration 4706/6000, G loss: 0.5116370916366577, D loss: 0.03787529468536377\n",
            "Iteration 4707/6000, G loss: 0.4622163772583008, D loss: 0.02031852677464485\n",
            "Iteration 4708/6000, G loss: 0.5014122128486633, D loss: 0.0005528826732188463\n",
            "Iteration 4709/6000, G loss: 0.44100433588027954, D loss: 0.016714969649910927\n",
            "Iteration 4710/6000, G loss: 0.4296678900718689, D loss: 3.4065308570861816\n",
            "Iteration 4711/6000, G loss: 0.5339260697364807, D loss: 1.5503006579820067e-05\n",
            "Iteration 4712/6000, G loss: 0.5556594133377075, D loss: 0.0017062975093722343\n",
            "Iteration 4713/6000, G loss: 0.428605318069458, D loss: 0.0608409121632576\n",
            "Iteration 4714/6000, G loss: 0.5156955718994141, D loss: 0.04823330044746399\n",
            "Iteration 4715/6000, G loss: 0.46793854236602783, D loss: 0.07288680970668793\n",
            "Iteration 4716/6000, G loss: 0.5543226599693298, D loss: 0.030698277056217194\n",
            "Iteration 4717/6000, G loss: 0.4533316493034363, D loss: 2.0058469772338867\n",
            "Iteration 4718/6000, G loss: 0.597370982170105, D loss: 0.22672882676124573\n",
            "Iteration 4719/6000, G loss: 0.5751760005950928, D loss: 0.005474450066685677\n",
            "Iteration 4720/6000, G loss: 0.538777768611908, D loss: 0.22311335802078247\n",
            "Iteration 4721/6000, G loss: 0.5895125865936279, D loss: 0.018406320363283157\n",
            "Iteration 4722/6000, G loss: 0.4300249218940735, D loss: 0.5539207458496094\n",
            "Iteration 4723/6000, G loss: 0.5235525965690613, D loss: 0.007145874202251434\n",
            "Iteration 4724/6000, G loss: 0.5201274752616882, D loss: 0.11713193356990814\n",
            "Iteration 4725/6000, G loss: 0.5327779054641724, D loss: 0.009757651016116142\n",
            "Iteration 4726/6000, G loss: 0.4754560589790344, D loss: 0.4437945783138275\n",
            "Iteration 4727/6000, G loss: 0.4058524966239929, D loss: 0.22089359164237976\n",
            "Iteration 4728/6000, G loss: 0.5233021378517151, D loss: 0.0017461705720052123\n",
            "Iteration 4729/6000, G loss: 0.5019643902778625, D loss: 0.045183926820755005\n",
            "Iteration 4730/6000, G loss: 0.3892267346382141, D loss: 0.0017988712061196566\n",
            "Iteration 4731/6000, G loss: 0.48858922719955444, D loss: 0.0015596927842125297\n",
            "Iteration 4732/6000, G loss: 0.5255702137947083, D loss: 0.0701611116528511\n",
            "Iteration 4733/6000, G loss: 0.5556204319000244, D loss: 0.0018830819753929973\n",
            "Iteration 4734/6000, G loss: 0.4587579369544983, D loss: 0.4109228253364563\n",
            "Iteration 4735/6000, G loss: 0.48043859004974365, D loss: 0.08137539029121399\n",
            "Iteration 4736/6000, G loss: 0.521373450756073, D loss: 0.022200094535946846\n",
            "Iteration 4737/6000, G loss: 0.4267195761203766, D loss: 0.8879635334014893\n",
            "Iteration 4738/6000, G loss: 0.466630756855011, D loss: 0.23729005455970764\n",
            "Iteration 4739/6000, G loss: 0.47849932312965393, D loss: 0.6999450325965881\n",
            "Iteration 4740/6000, G loss: 0.5280083417892456, D loss: 0.02162027359008789\n",
            "Iteration 4741/6000, G loss: 0.4259560704231262, D loss: 15.210958480834961\n",
            "Iteration 4742/6000, G loss: 0.5037594437599182, D loss: 0.002030196599662304\n",
            "Iteration 4743/6000, G loss: 0.448927640914917, D loss: 0.16595079004764557\n",
            "Iteration 4744/6000, G loss: 0.4912218451499939, D loss: 0.003944847267121077\n",
            "Iteration 4745/6000, G loss: 0.6352221369743347, D loss: 0.00419475045055151\n",
            "Iteration 4746/6000, G loss: 0.5476090908050537, D loss: 0.03183405101299286\n",
            "Iteration 4747/6000, G loss: 0.574009895324707, D loss: 0.026136545464396477\n",
            "Iteration 4748/6000, G loss: 0.479486346244812, D loss: 0.09766928106546402\n",
            "Iteration 4749/6000, G loss: 0.6199209094047546, D loss: 0.09538480639457703\n",
            "Iteration 4750/6000, G loss: 0.5701555609703064, D loss: 0.11030465364456177\n",
            "Iteration 4751/6000, G loss: 0.5948785543441772, D loss: 0.006606110371649265\n",
            "Iteration 4752/6000, G loss: 0.43999481201171875, D loss: 0.20962059497833252\n",
            "Iteration 4753/6000, G loss: 0.5805932283401489, D loss: 0.020964063704013824\n",
            "Iteration 4754/6000, G loss: 0.43616077303886414, D loss: 0.12259426712989807\n",
            "Iteration 4755/6000, G loss: 0.5053176879882812, D loss: 0.12746405601501465\n",
            "Iteration 4756/6000, G loss: 0.48044589161872864, D loss: 0.05973002314567566\n",
            "Iteration 4757/6000, G loss: 0.5165375471115112, D loss: 0.1912229359149933\n",
            "Iteration 4758/6000, G loss: 0.4916376769542694, D loss: 0.1354500949382782\n",
            "Iteration 4759/6000, G loss: 0.4272812604904175, D loss: 0.06963716447353363\n",
            "Iteration 4760/6000, G loss: 0.39813220500946045, D loss: 0.06682951003313065\n",
            "Iteration 4761/6000, G loss: 0.48738184571266174, D loss: 0.3852381110191345\n",
            "Iteration 4762/6000, G loss: 0.43160611391067505, D loss: 0.05782058835029602\n",
            "Iteration 4763/6000, G loss: 0.5346168875694275, D loss: 0.04941577464342117\n",
            "Iteration 4764/6000, G loss: 0.42738431692123413, D loss: 0.668398380279541\n",
            "Iteration 4765/6000, G loss: 0.47177261114120483, D loss: 0.45168840885162354\n",
            "Iteration 4766/6000, G loss: 0.6595571637153625, D loss: 0.7151221632957458\n",
            "Iteration 4767/6000, G loss: 0.4627721905708313, D loss: 0.024814503267407417\n",
            "Iteration 4768/6000, G loss: 0.459490567445755, D loss: 0.12806189060211182\n",
            "Iteration 4769/6000, G loss: 0.5037201046943665, D loss: 0.338971346616745\n",
            "Iteration 4770/6000, G loss: 0.6431829333305359, D loss: 2.1322665214538574\n",
            "Iteration 4771/6000, G loss: 0.5759701132774353, D loss: 1.0511672496795654\n",
            "Iteration 4772/6000, G loss: 0.4871412515640259, D loss: 0.7641592621803284\n",
            "Iteration 4773/6000, G loss: 0.47391679883003235, D loss: 0.022902067750692368\n",
            "Iteration 4774/6000, G loss: 0.453302264213562, D loss: 0.015912611037492752\n",
            "Iteration 4775/6000, G loss: 0.49603816866874695, D loss: 0.013964624144136906\n",
            "Iteration 4776/6000, G loss: 0.5263123512268066, D loss: 1.7393009662628174\n",
            "Iteration 4777/6000, G loss: 0.6046875715255737, D loss: 0.12479358911514282\n",
            "Iteration 4778/6000, G loss: 0.5373867750167847, D loss: 0.103142149746418\n",
            "Iteration 4779/6000, G loss: 0.4206688404083252, D loss: 0.010588571429252625\n",
            "Iteration 4780/6000, G loss: 0.4900103509426117, D loss: 0.006047780625522137\n",
            "Iteration 4781/6000, G loss: 0.5127567052841187, D loss: 0.030945513397455215\n",
            "Iteration 4782/6000, G loss: 0.4283915162086487, D loss: 0.686208963394165\n",
            "Iteration 4783/6000, G loss: 0.451497882604599, D loss: 0.11351598054170609\n",
            "Iteration 4784/6000, G loss: 0.42575496435165405, D loss: 0.6305469274520874\n",
            "Iteration 4785/6000, G loss: 0.4585072696208954, D loss: 0.8768048286437988\n",
            "Iteration 4786/6000, G loss: 0.5256935358047485, D loss: 0.009667006321251392\n",
            "Iteration 4787/6000, G loss: 0.36896812915802, D loss: 2.3905820846557617\n",
            "Iteration 4788/6000, G loss: 0.49126070737838745, D loss: 0.0007166280411183834\n",
            "Iteration 4789/6000, G loss: 0.5808835625648499, D loss: 0.0024078493006527424\n",
            "Iteration 4790/6000, G loss: 0.3925008177757263, D loss: 0.12168098986148834\n",
            "Iteration 4791/6000, G loss: 0.607760488986969, D loss: 0.0018139878520742059\n",
            "Iteration 4792/6000, G loss: 0.5431800484657288, D loss: 1.5171051025390625\n",
            "Iteration 4793/6000, G loss: 0.5176759362220764, D loss: 0.006819481495767832\n",
            "Iteration 4794/6000, G loss: 0.5716334581375122, D loss: 0.02971239760518074\n",
            "Iteration 4795/6000, G loss: 0.46721982955932617, D loss: 0.7696724534034729\n",
            "Iteration 4796/6000, G loss: 0.5105099081993103, D loss: 0.06844983994960785\n",
            "Iteration 4797/6000, G loss: 0.4655913710594177, D loss: 1.3773384094238281\n",
            "Iteration 4798/6000, G loss: 0.5226373672485352, D loss: 0.07189188152551651\n",
            "Iteration 4799/6000, G loss: 0.4701082706451416, D loss: 0.0446811318397522\n",
            "Iteration 4800/6000, G loss: 0.520901620388031, D loss: 0.9484294056892395\n",
            "Iteration 4801/6000, G loss: 0.5770682096481323, D loss: 0.00017121562268584967\n",
            "Iteration 4802/6000, G loss: 0.4473056495189667, D loss: 0.0008941497653722763\n",
            "Iteration 4803/6000, G loss: 0.4752963185310364, D loss: 6.973589188419282e-05\n",
            "Iteration 4804/6000, G loss: 0.505831241607666, D loss: 0.05664718151092529\n",
            "Iteration 4805/6000, G loss: 0.6028777956962585, D loss: 0.3518415093421936\n",
            "Iteration 4806/6000, G loss: 0.5730693340301514, D loss: 0.07822378724813461\n",
            "Iteration 4807/6000, G loss: 0.41266781091690063, D loss: 0.22657251358032227\n",
            "Iteration 4808/6000, G loss: 0.4505400061607361, D loss: 0.03364831209182739\n",
            "Iteration 4809/6000, G loss: 0.4929388165473938, D loss: 0.001443985616788268\n",
            "Iteration 4810/6000, G loss: 0.5470032095909119, D loss: 4.155108451843262\n",
            "Iteration 4811/6000, G loss: 0.4030219614505768, D loss: 0.27940633893013\n",
            "Iteration 4812/6000, G loss: 0.3741568922996521, D loss: 0.22148479521274567\n",
            "Iteration 4813/6000, G loss: 0.474148154258728, D loss: 0.033611953258514404\n",
            "Iteration 4814/6000, G loss: 0.4817420244216919, D loss: 0.021548954769968987\n",
            "Iteration 4815/6000, G loss: 0.3723456859588623, D loss: 0.4230712652206421\n",
            "Iteration 4816/6000, G loss: 0.5354407429695129, D loss: 0.046059004962444305\n",
            "Iteration 4817/6000, G loss: 0.4533805847167969, D loss: 0.010315046645700932\n",
            "Iteration 4818/6000, G loss: 0.5314884185791016, D loss: 0.0033049238845705986\n",
            "Iteration 4819/6000, G loss: 0.5109638571739197, D loss: 2.2961502075195312\n",
            "Iteration 4820/6000, G loss: 0.47449302673339844, D loss: 0.00020460286759771407\n",
            "Iteration 4821/6000, G loss: 0.4693267047405243, D loss: 0.004840427078306675\n",
            "Iteration 4822/6000, G loss: 0.5134596228599548, D loss: 0.006442279554903507\n",
            "Iteration 4823/6000, G loss: 0.5566383600234985, D loss: 0.10356558859348297\n",
            "Iteration 4824/6000, G loss: 0.5341722369194031, D loss: 0.05568542331457138\n",
            "Iteration 4825/6000, G loss: 0.5351911783218384, D loss: 0.24194884300231934\n",
            "Iteration 4826/6000, G loss: 0.4505915343761444, D loss: 0.9024450778961182\n",
            "Iteration 4827/6000, G loss: 0.5413296818733215, D loss: 0.04886264726519585\n",
            "Iteration 4828/6000, G loss: 0.4882129728794098, D loss: 0.10024838894605637\n",
            "Iteration 4829/6000, G loss: 0.4003414809703827, D loss: 0.234051913022995\n",
            "Iteration 4830/6000, G loss: 0.5101293325424194, D loss: 0.27684473991394043\n",
            "Iteration 4831/6000, G loss: 0.6095671653747559, D loss: 8.203727722167969\n",
            "Iteration 4832/6000, G loss: 0.5183861255645752, D loss: 0.11990722268819809\n",
            "Iteration 4833/6000, G loss: 0.5338494181632996, D loss: 0.0009813752258196473\n",
            "Iteration 4834/6000, G loss: 0.44610053300857544, D loss: 0.003869658336043358\n",
            "Iteration 4835/6000, G loss: 0.5069708228111267, D loss: 0.00012397131649777293\n",
            "Iteration 4836/6000, G loss: 0.6147987246513367, D loss: 0.0008084949804469943\n",
            "Iteration 4837/6000, G loss: 0.596056342124939, D loss: 0.10946550965309143\n",
            "Iteration 4838/6000, G loss: 0.5045293569564819, D loss: 0.022603582590818405\n",
            "Iteration 4839/6000, G loss: 0.5748181343078613, D loss: 0.0005023139528930187\n",
            "Iteration 4840/6000, G loss: 0.44087842106819153, D loss: 0.004411452915519476\n",
            "Iteration 4841/6000, G loss: 0.39369088411331177, D loss: 0.012915292754769325\n",
            "Iteration 4842/6000, G loss: 0.5574961304664612, D loss: 0.007865291088819504\n",
            "Iteration 4843/6000, G loss: 0.4960140287876129, D loss: 0.023824157193303108\n",
            "Iteration 4844/6000, G loss: 0.5124399065971375, D loss: 0.31069931387901306\n",
            "Iteration 4845/6000, G loss: 0.4737694263458252, D loss: 0.04168545454740524\n",
            "Iteration 4846/6000, G loss: 0.5011765956878662, D loss: 0.19144116342067719\n",
            "Iteration 4847/6000, G loss: 0.475053071975708, D loss: 0.38198795914649963\n",
            "Iteration 4848/6000, G loss: 0.4664055109024048, D loss: 0.634697675704956\n",
            "Iteration 4849/6000, G loss: 0.5175973176956177, D loss: 0.06965818256139755\n",
            "Iteration 4850/6000, G loss: 0.43420344591140747, D loss: 0.13567212224006653\n",
            "Iteration 4851/6000, G loss: 0.46603360772132874, D loss: 1.2070436477661133\n",
            "Iteration 4852/6000, G loss: 0.5399553179740906, D loss: 2.67893123626709\n",
            "Iteration 4853/6000, G loss: 0.45056504011154175, D loss: 0.01955510303378105\n",
            "Iteration 4854/6000, G loss: 0.630346417427063, D loss: 0.00666397251188755\n",
            "Iteration 4855/6000, G loss: 0.41104960441589355, D loss: 0.016458500176668167\n",
            "Iteration 4856/6000, G loss: 0.4481118321418762, D loss: 0.050479695200920105\n",
            "Iteration 4857/6000, G loss: 0.5306839942932129, D loss: 0.3087961673736572\n",
            "Iteration 4858/6000, G loss: 0.28493940830230713, D loss: 0.49672847986221313\n",
            "Iteration 4859/6000, G loss: 0.5228390097618103, D loss: 0.1238144040107727\n",
            "Iteration 4860/6000, G loss: 0.574479877948761, D loss: 0.005642091389745474\n",
            "Iteration 4861/6000, G loss: 0.5324665307998657, D loss: 0.007516019977629185\n",
            "Iteration 4862/6000, G loss: 0.5305352807044983, D loss: 0.0016093679005280137\n",
            "Iteration 4863/6000, G loss: 0.4115181863307953, D loss: 0.04742101579904556\n",
            "Iteration 4864/6000, G loss: 0.4881931245326996, D loss: 1.0429495573043823\n",
            "Iteration 4865/6000, G loss: 0.5397526621818542, D loss: 0.0031766449101269245\n",
            "Iteration 4866/6000, G loss: 0.49141019582748413, D loss: 0.0154412304982543\n",
            "Iteration 4867/6000, G loss: 0.4645492136478424, D loss: 0.016835030168294907\n",
            "Iteration 4868/6000, G loss: 0.4281132221221924, D loss: 0.014382215216755867\n",
            "Iteration 4869/6000, G loss: 0.5661526918411255, D loss: 0.03178282827138901\n",
            "Iteration 4870/6000, G loss: 0.49415323138237, D loss: 0.006187651306390762\n",
            "Iteration 4871/6000, G loss: 0.46643054485321045, D loss: 0.11261117458343506\n",
            "Iteration 4872/6000, G loss: 0.5348661541938782, D loss: 0.0592404305934906\n",
            "Iteration 4873/6000, G loss: 0.6298286318778992, D loss: 0.08461683988571167\n",
            "Iteration 4874/6000, G loss: 0.55814528465271, D loss: 0.003369450569152832\n",
            "Iteration 4875/6000, G loss: 0.45240163803100586, D loss: 0.0017883190885186195\n",
            "Iteration 4876/6000, G loss: 0.5482850074768066, D loss: 0.0008484441787004471\n",
            "Iteration 4877/6000, G loss: 0.512259840965271, D loss: 0.13763263821601868\n",
            "Iteration 4878/6000, G loss: 0.4179328978061676, D loss: 0.09974803030490875\n",
            "Iteration 4879/6000, G loss: 0.58577561378479, D loss: 4.205278396606445\n",
            "Iteration 4880/6000, G loss: 0.4033741354942322, D loss: 0.11701837182044983\n",
            "Iteration 4881/6000, G loss: 0.392585426568985, D loss: 0.008724706247448921\n",
            "Iteration 4882/6000, G loss: 0.5253600478172302, D loss: 0.00724414736032486\n",
            "Iteration 4883/6000, G loss: 0.44800373911857605, D loss: 0.003236349206417799\n",
            "Iteration 4884/6000, G loss: 0.5088343024253845, D loss: 0.0007941062794998288\n",
            "Iteration 4885/6000, G loss: 0.5361238718032837, D loss: 0.01417803205549717\n",
            "Iteration 4886/6000, G loss: 0.57834392786026, D loss: 0.030476707965135574\n",
            "Iteration 4887/6000, G loss: 0.5663210153579712, D loss: 0.026349056512117386\n",
            "Iteration 4888/6000, G loss: 0.5061153173446655, D loss: 0.18728430569171906\n",
            "Iteration 4889/6000, G loss: 0.5388940572738647, D loss: 0.0372980460524559\n",
            "Iteration 4890/6000, G loss: 0.4927816390991211, D loss: 0.0036778319627046585\n",
            "Iteration 4891/6000, G loss: 0.4260603189468384, D loss: 0.0074217310175299644\n",
            "Iteration 4892/6000, G loss: 0.47039467096328735, D loss: 0.009144742041826248\n",
            "Iteration 4893/6000, G loss: 0.4517844021320343, D loss: 0.004253206308931112\n",
            "Iteration 4894/6000, G loss: 0.46289485692977905, D loss: 0.09961184114217758\n",
            "Iteration 4895/6000, G loss: 0.4137781262397766, D loss: 0.11827924847602844\n",
            "Iteration 4896/6000, G loss: 0.6715477705001831, D loss: 0.011731286533176899\n",
            "Iteration 4897/6000, G loss: 0.49691200256347656, D loss: 0.007219979539513588\n",
            "Iteration 4898/6000, G loss: 0.4601624608039856, D loss: 0.016124699264764786\n",
            "Iteration 4899/6000, G loss: 0.4128272235393524, D loss: 0.0021561661269515753\n",
            "Iteration 4900/6000, G loss: 0.4868675768375397, D loss: 5.563257217407227\n",
            "Iteration 4901/6000, G loss: 0.5107762813568115, D loss: 0.023072905838489532\n",
            "Iteration 4902/6000, G loss: 0.5032743811607361, D loss: 0.009451674297451973\n",
            "Iteration 4903/6000, G loss: 0.3799210786819458, D loss: 0.0059840623289346695\n",
            "Iteration 4904/6000, G loss: 0.5091333985328674, D loss: 0.0023002373054623604\n",
            "Iteration 4905/6000, G loss: 0.4778842628002167, D loss: 0.0006439018761739135\n",
            "Iteration 4906/6000, G loss: 0.4659021198749542, D loss: 0.007206514943391085\n",
            "Iteration 4907/6000, G loss: 0.4718326926231384, D loss: 0.14145708084106445\n",
            "Iteration 4908/6000, G loss: 0.43137267231941223, D loss: 0.29236242175102234\n",
            "Iteration 4909/6000, G loss: 0.5066394805908203, D loss: 0.5668664574623108\n",
            "Iteration 4910/6000, G loss: 0.5327258706092834, D loss: 0.051295116543769836\n",
            "Iteration 4911/6000, G loss: 0.42441481351852417, D loss: 0.010086901485919952\n",
            "Iteration 4912/6000, G loss: 0.48163682222366333, D loss: 0.003817601129412651\n",
            "Iteration 4913/6000, G loss: 0.5889838337898254, D loss: 0.0017321305349469185\n",
            "Iteration 4914/6000, G loss: 0.5505450367927551, D loss: 0.013616370968520641\n",
            "Iteration 4915/6000, G loss: 0.5999653935432434, D loss: 0.03676058351993561\n",
            "Iteration 4916/6000, G loss: 0.5123344659805298, D loss: 0.0752515196800232\n",
            "Iteration 4917/6000, G loss: 0.564206600189209, D loss: 0.18717433512210846\n",
            "Iteration 4918/6000, G loss: 0.4412247836589813, D loss: 0.015212081372737885\n",
            "Iteration 4919/6000, G loss: 0.5466943383216858, D loss: 0.015174014493823051\n",
            "Iteration 4920/6000, G loss: 0.5087753534317017, D loss: 0.03948948532342911\n",
            "Iteration 4921/6000, G loss: 0.49001339077949524, D loss: 0.0332154780626297\n",
            "Iteration 4922/6000, G loss: 0.4054132401943207, D loss: 0.01479277852922678\n",
            "Iteration 4923/6000, G loss: 0.5817692875862122, D loss: 0.004095972515642643\n",
            "Iteration 4924/6000, G loss: 0.4562709927558899, D loss: 0.006148097570985556\n",
            "Iteration 4925/6000, G loss: 0.5665847063064575, D loss: 0.03822685778141022\n",
            "Iteration 4926/6000, G loss: 0.34879904985427856, D loss: 0.0037140166386961937\n",
            "Iteration 4927/6000, G loss: 0.6199939846992493, D loss: 0.0041509028524160385\n",
            "Iteration 4928/6000, G loss: 0.41749119758605957, D loss: 0.01581236906349659\n",
            "Iteration 4929/6000, G loss: 0.5201087594032288, D loss: 0.010876154527068138\n",
            "Iteration 4930/6000, G loss: 0.46625569462776184, D loss: 0.002475214656442404\n",
            "Iteration 4931/6000, G loss: 0.565611720085144, D loss: 0.05189823359251022\n",
            "Iteration 4932/6000, G loss: 0.47343456745147705, D loss: 0.14476248621940613\n",
            "Iteration 4933/6000, G loss: 0.4723571836948395, D loss: 0.004643801599740982\n",
            "Iteration 4934/6000, G loss: 0.4737531542778015, D loss: 0.002046430017799139\n",
            "Iteration 4935/6000, G loss: 0.46250683069229126, D loss: 0.006660605315119028\n",
            "Iteration 4936/6000, G loss: 0.476570725440979, D loss: 0.0032299295999109745\n",
            "Iteration 4937/6000, G loss: 0.521503210067749, D loss: 0.0020901868119835854\n",
            "Iteration 4938/6000, G loss: 0.44824689626693726, D loss: 0.006161307916045189\n",
            "Iteration 4939/6000, G loss: 0.46455520391464233, D loss: 0.092507503926754\n",
            "Iteration 4940/6000, G loss: 0.5625830292701721, D loss: 0.002776702167466283\n",
            "Iteration 4941/6000, G loss: 0.5054329037666321, D loss: 0.026716122403740883\n",
            "Iteration 4942/6000, G loss: 0.4763695001602173, D loss: 0.005266645457595587\n",
            "Iteration 4943/6000, G loss: 0.4926629364490509, D loss: 0.014917954802513123\n",
            "Iteration 4944/6000, G loss: 0.4806622266769409, D loss: 0.004726390354335308\n",
            "Iteration 4945/6000, G loss: 0.5263534188270569, D loss: 0.10602542757987976\n",
            "Iteration 4946/6000, G loss: 0.4547573924064636, D loss: 0.014573991298675537\n",
            "Iteration 4947/6000, G loss: 0.5884169936180115, D loss: 0.0003996320301666856\n",
            "Iteration 4948/6000, G loss: 0.515399694442749, D loss: 0.02443614974617958\n",
            "Iteration 4949/6000, G loss: 0.5426365733146667, D loss: 0.003164459951221943\n",
            "Iteration 4950/6000, G loss: 0.40457969903945923, D loss: 0.08497311919927597\n",
            "Iteration 4951/6000, G loss: 0.41188597679138184, D loss: 0.0511397123336792\n",
            "Iteration 4952/6000, G loss: 0.5162795186042786, D loss: 0.015614859759807587\n",
            "Iteration 4953/6000, G loss: 0.4629994332790375, D loss: 0.09697938710451126\n",
            "Iteration 4954/6000, G loss: 0.4634459316730499, D loss: 0.25784337520599365\n",
            "Iteration 4955/6000, G loss: 0.4591453969478607, D loss: 0.2045106589794159\n",
            "Iteration 4956/6000, G loss: 0.6164225339889526, D loss: 0.12274482846260071\n",
            "Iteration 4957/6000, G loss: 0.5017754435539246, D loss: 0.12665371596813202\n",
            "Iteration 4958/6000, G loss: 0.49619534611701965, D loss: 0.015265189111232758\n",
            "Iteration 4959/6000, G loss: 0.5152606964111328, D loss: 0.0031844740733504295\n",
            "Iteration 4960/6000, G loss: 0.5073568224906921, D loss: 0.003775299759581685\n",
            "Iteration 4961/6000, G loss: 0.4813705384731293, D loss: 0.045628923922777176\n",
            "Iteration 4962/6000, G loss: 0.4959734082221985, D loss: 2.3672313690185547\n",
            "Iteration 4963/6000, G loss: 0.5896596312522888, D loss: 0.1993880271911621\n",
            "Iteration 4964/6000, G loss: 0.4064299762248993, D loss: 0.03393632546067238\n",
            "Iteration 4965/6000, G loss: 0.5552190542221069, D loss: 0.0011685278732329607\n",
            "Iteration 4966/6000, G loss: 0.4812593162059784, D loss: 7.2479101618228015e-06\n",
            "Iteration 4967/6000, G loss: 0.5134011507034302, D loss: 0.00017375635798089206\n",
            "Iteration 4968/6000, G loss: 0.48118945956230164, D loss: 0.024096649140119553\n",
            "Iteration 4969/6000, G loss: 0.46154695749282837, D loss: 0.25677672028541565\n",
            "Iteration 4970/6000, G loss: 0.5775910019874573, D loss: 0.0027027989272028208\n",
            "Iteration 4971/6000, G loss: 0.49579721689224243, D loss: 0.04612700641155243\n",
            "Iteration 4972/6000, G loss: 0.578540563583374, D loss: 0.0006680639926344156\n",
            "Iteration 4973/6000, G loss: 0.5454050302505493, D loss: 0.014325650408864021\n",
            "Iteration 4974/6000, G loss: 0.48040658235549927, D loss: 6.38364417682169e-06\n",
            "Iteration 4975/6000, G loss: 0.49617666006088257, D loss: 0.0016238563694059849\n",
            "Iteration 4976/6000, G loss: 0.516083836555481, D loss: 0.0003290855383966118\n",
            "Iteration 4977/6000, G loss: 0.538635790348053, D loss: 6.198769551701844e-05\n",
            "Iteration 4978/6000, G loss: 0.514238715171814, D loss: 0.0055137500166893005\n",
            "Iteration 4979/6000, G loss: 0.395256906747818, D loss: 0.001085134455934167\n",
            "Iteration 4980/6000, G loss: 0.6430362462997437, D loss: 0.003602431621402502\n",
            "Iteration 4981/6000, G loss: 0.45018309354782104, D loss: 0.026493092998862267\n",
            "Iteration 4982/6000, G loss: 0.4906492233276367, D loss: 0.15118281543254852\n",
            "Iteration 4983/6000, G loss: 0.4482141435146332, D loss: 0.0005280252662487328\n",
            "Iteration 4984/6000, G loss: 0.47800546884536743, D loss: 0.00015274118050001562\n",
            "Iteration 4985/6000, G loss: 0.5066372156143188, D loss: 0.020551037043333054\n",
            "Iteration 4986/6000, G loss: 0.38038286566734314, D loss: 0.2554880678653717\n",
            "Iteration 4987/6000, G loss: 0.5274786353111267, D loss: 0.2844603657722473\n",
            "Iteration 4988/6000, G loss: 0.4756624400615692, D loss: 0.06689337641000748\n",
            "Iteration 4989/6000, G loss: 0.5042366981506348, D loss: 0.01707756146788597\n",
            "Iteration 4990/6000, G loss: 0.5025956630706787, D loss: 8.928544411901385e-05\n",
            "Iteration 4991/6000, G loss: 0.5592138171195984, D loss: 0.00012992705160286278\n",
            "Iteration 4992/6000, G loss: 0.46804079413414, D loss: 0.007688348181545734\n",
            "Iteration 4993/6000, G loss: 0.43089815974235535, D loss: 0.2767021656036377\n",
            "Iteration 4994/6000, G loss: 0.613847017288208, D loss: 2.7651801109313965\n",
            "Iteration 4995/6000, G loss: 0.5055549144744873, D loss: 0.0952654704451561\n",
            "Iteration 4996/6000, G loss: 0.5361449718475342, D loss: 1.5616406017215922e-06\n",
            "Iteration 4997/6000, G loss: 0.47901445627212524, D loss: 1.1759952030843124e-05\n",
            "Iteration 4998/6000, G loss: 0.5108506679534912, D loss: 9.157955355476588e-05\n",
            "Iteration 4999/6000, G loss: 0.5100551247596741, D loss: 0.09513604640960693\n",
            "Iteration 5000/6000, G loss: 0.4462243616580963, D loss: 0.2806113362312317\n",
            "Iteration 5001/6000, G loss: 0.4599001407623291, D loss: 0.6885867714881897\n",
            "Iteration 5002/6000, G loss: 0.48860007524490356, D loss: 0.0001287591876462102\n",
            "Iteration 5003/6000, G loss: 0.4014767110347748, D loss: 0.0011246759677305818\n",
            "Iteration 5004/6000, G loss: 0.48220303654670715, D loss: 0.1939772367477417\n",
            "Iteration 5005/6000, G loss: 0.4924192726612091, D loss: 0.0026230677030980587\n",
            "Iteration 5006/6000, G loss: 0.5948607325553894, D loss: 0.08471141755580902\n",
            "Iteration 5007/6000, G loss: 0.4410794973373413, D loss: 0.150154709815979\n",
            "Iteration 5008/6000, G loss: 0.5916491746902466, D loss: 0.0010931696742773056\n",
            "Iteration 5009/6000, G loss: 0.45402878522872925, D loss: 0.00011236901627853513\n",
            "Iteration 5010/6000, G loss: 0.5439497232437134, D loss: 2.3961054012033856e-06\n",
            "Iteration 5011/6000, G loss: 0.5511489510536194, D loss: 2.2357548004947603e-05\n",
            "Iteration 5012/6000, G loss: 0.5158215761184692, D loss: 4.108482971787453e-05\n",
            "Iteration 5013/6000, G loss: 0.593197762966156, D loss: 0.1654501110315323\n",
            "Iteration 5014/6000, G loss: 0.46468356251716614, D loss: 0.0528436079621315\n",
            "Iteration 5015/6000, G loss: 0.5951570272445679, D loss: 0.007277258206158876\n",
            "Iteration 5016/6000, G loss: 0.5203368663787842, D loss: 0.0016241382109001279\n",
            "Iteration 5017/6000, G loss: 0.507293701171875, D loss: 1.105662522604689e-05\n",
            "Iteration 5018/6000, G loss: 0.46159666776657104, D loss: 2.1546933567151427e-05\n",
            "Iteration 5019/6000, G loss: 0.451389878988266, D loss: 0.009257595986127853\n",
            "Iteration 5020/6000, G loss: 0.35537636280059814, D loss: 3.0301430225372314\n",
            "Iteration 5021/6000, G loss: 0.5973072648048401, D loss: 0.1616811752319336\n",
            "Iteration 5022/6000, G loss: 0.4379071891307831, D loss: 0.0405442975461483\n",
            "Iteration 5023/6000, G loss: 0.45603904128074646, D loss: 0.0010851192055270076\n",
            "Iteration 5024/6000, G loss: 0.5444382429122925, D loss: 0.0003290273016318679\n",
            "Iteration 5025/6000, G loss: 0.5285549163818359, D loss: 5.596866685664281e-06\n",
            "Iteration 5026/6000, G loss: 0.6272947788238525, D loss: 5.158691783435643e-05\n",
            "Iteration 5027/6000, G loss: 0.5089301466941833, D loss: 0.00017829051648732275\n",
            "Iteration 5028/6000, G loss: 0.5038710236549377, D loss: 0.02294277213513851\n",
            "Iteration 5029/6000, G loss: 0.564128041267395, D loss: 0.00999977346509695\n",
            "Iteration 5030/6000, G loss: 0.5523450374603271, D loss: 0.08859558403491974\n",
            "Iteration 5031/6000, G loss: 0.5220404863357544, D loss: 0.07324230670928955\n",
            "Iteration 5032/6000, G loss: 0.49421459436416626, D loss: 0.0015989593230187893\n",
            "Iteration 5033/6000, G loss: 0.4311184585094452, D loss: 0.002802568254992366\n",
            "Iteration 5034/6000, G loss: 0.4529098570346832, D loss: 0.4362984001636505\n",
            "Iteration 5035/6000, G loss: 0.5002629160881042, D loss: 0.14570856094360352\n",
            "Iteration 5036/6000, G loss: 0.5299708843231201, D loss: 0.007157427258789539\n",
            "Iteration 5037/6000, G loss: 0.5464486479759216, D loss: 0.0004401875485200435\n",
            "Iteration 5038/6000, G loss: 0.5564576387405396, D loss: 0.004611644893884659\n",
            "Iteration 5039/6000, G loss: 0.5695501565933228, D loss: 1.0225392580032349\n",
            "Iteration 5040/6000, G loss: 0.4992672801017761, D loss: 0.012738469056785107\n",
            "Iteration 5041/6000, G loss: 0.44479113817214966, D loss: 0.002606423106044531\n",
            "Iteration 5042/6000, G loss: 0.4952400326728821, D loss: 0.0005800523795187473\n",
            "Iteration 5043/6000, G loss: 0.39633601903915405, D loss: 0.026238523423671722\n",
            "Iteration 5044/6000, G loss: 0.5655768513679504, D loss: 0.005439269356429577\n",
            "Iteration 5045/6000, G loss: 0.5322669744491577, D loss: 0.0025889985263347626\n",
            "Iteration 5046/6000, G loss: 0.38282448053359985, D loss: 0.332344651222229\n",
            "Iteration 5047/6000, G loss: 0.3492075800895691, D loss: 0.0631420910358429\n",
            "Iteration 5048/6000, G loss: 0.4696252644062042, D loss: 0.19361209869384766\n",
            "Iteration 5049/6000, G loss: 0.42376047372817993, D loss: 0.0074091097339987755\n",
            "Iteration 5050/6000, G loss: 0.5117141604423523, D loss: 0.074498750269413\n",
            "Iteration 5051/6000, G loss: 0.5365826487541199, D loss: 0.0006345397559925914\n",
            "Iteration 5052/6000, G loss: 0.5066372156143188, D loss: 0.0007518216734752059\n",
            "Iteration 5053/6000, G loss: 0.6112325191497803, D loss: 0.026654748246073723\n",
            "Iteration 5054/6000, G loss: 0.5588698387145996, D loss: 0.006990720052272081\n",
            "Iteration 5055/6000, G loss: 0.5331658124923706, D loss: 0.0008593060774728656\n",
            "Iteration 5056/6000, G loss: 0.5528900027275085, D loss: 0.08946501463651657\n",
            "Iteration 5057/6000, G loss: 0.43577513098716736, D loss: 0.0002000083914026618\n",
            "Iteration 5058/6000, G loss: 0.5236506462097168, D loss: 0.02456478402018547\n",
            "Iteration 5059/6000, G loss: 0.5346813797950745, D loss: 0.00037718075327575207\n",
            "Iteration 5060/6000, G loss: 0.5196149349212646, D loss: 0.15735213458538055\n",
            "Iteration 5061/6000, G loss: 0.4145272374153137, D loss: 7.737307548522949\n",
            "Iteration 5062/6000, G loss: 0.526017963886261, D loss: 0.021321021020412445\n",
            "Iteration 5063/6000, G loss: 0.4971370995044708, D loss: 0.002103958511725068\n",
            "Iteration 5064/6000, G loss: 0.6847589015960693, D loss: 0.01690767891705036\n",
            "Iteration 5065/6000, G loss: 0.5658524036407471, D loss: 0.0036641107872128487\n",
            "Iteration 5066/6000, G loss: 0.5527779459953308, D loss: 0.00803419854491949\n",
            "Iteration 5067/6000, G loss: 0.48454761505126953, D loss: 0.34407147765159607\n",
            "Iteration 5068/6000, G loss: 0.5942765474319458, D loss: 0.0032321857288479805\n",
            "Iteration 5069/6000, G loss: 0.5047383904457092, D loss: 0.001269874395802617\n",
            "Iteration 5070/6000, G loss: 0.5428011417388916, D loss: 0.019738204777240753\n",
            "Iteration 5071/6000, G loss: 0.4196050763130188, D loss: 0.062423620373010635\n",
            "Iteration 5072/6000, G loss: 0.4999675750732422, D loss: 0.09321516752243042\n",
            "Iteration 5073/6000, G loss: 0.5493713021278381, D loss: 0.4506974220275879\n",
            "Iteration 5074/6000, G loss: 0.5038034319877625, D loss: 0.11985675990581512\n",
            "Iteration 5075/6000, G loss: 0.4259822368621826, D loss: 0.20992065966129303\n",
            "Iteration 5076/6000, G loss: 0.5283310413360596, D loss: 0.004066802095621824\n",
            "Iteration 5077/6000, G loss: 0.4327741265296936, D loss: 0.005847406107932329\n",
            "Iteration 5078/6000, G loss: 0.4559812545776367, D loss: 0.0725058764219284\n",
            "Iteration 5079/6000, G loss: 0.37264978885650635, D loss: 0.018018517643213272\n",
            "Iteration 5080/6000, G loss: 0.4767647087574005, D loss: 0.026446932926774025\n",
            "Iteration 5081/6000, G loss: 0.4595526456832886, D loss: 0.008753813803195953\n",
            "Iteration 5082/6000, G loss: 0.42739132046699524, D loss: 0.000833965721540153\n",
            "Iteration 5083/6000, G loss: 0.5907620787620544, D loss: 0.0020399021450430155\n",
            "Iteration 5084/6000, G loss: 0.5300059914588928, D loss: 0.0026193205267190933\n",
            "Iteration 5085/6000, G loss: 0.5082215070724487, D loss: 0.005012967623770237\n",
            "Iteration 5086/6000, G loss: 0.5187317728996277, D loss: 0.0007883160724304616\n",
            "Iteration 5087/6000, G loss: 0.45256441831588745, D loss: 0.0022774022072553635\n",
            "Iteration 5088/6000, G loss: 0.5021368265151978, D loss: 0.0032907514832913876\n",
            "Iteration 5089/6000, G loss: 0.5058621168136597, D loss: 0.00032933405600488186\n",
            "Iteration 5090/6000, G loss: 0.5530444979667664, D loss: 0.00044002587674185634\n",
            "Iteration 5091/6000, G loss: 0.5964418649673462, D loss: 0.000585841597057879\n",
            "Iteration 5092/6000, G loss: 0.6254472732543945, D loss: 0.0022292810026556253\n",
            "Iteration 5093/6000, G loss: 0.5369937419891357, D loss: 0.0005968059413135052\n",
            "Iteration 5094/6000, G loss: 0.4846114218235016, D loss: 0.007342808414250612\n",
            "Iteration 5095/6000, G loss: 0.40071216225624084, D loss: 7.584440027130768e-05\n",
            "Iteration 5096/6000, G loss: 0.4873136878013611, D loss: 0.038504429161548615\n",
            "Iteration 5097/6000, G loss: 0.37835776805877686, D loss: 0.004996303468942642\n",
            "Iteration 5098/6000, G loss: 0.3482378423213959, D loss: 0.002259167144075036\n",
            "Iteration 5099/6000, G loss: 0.4506301283836365, D loss: 0.010138068348169327\n",
            "Iteration 5100/6000, G loss: 0.4758058190345764, D loss: 0.00034291186602786183\n",
            "Iteration 5101/6000, G loss: 0.4254434108734131, D loss: 0.0044703055173158646\n",
            "Iteration 5102/6000, G loss: 0.5157727599143982, D loss: 0.004754813387989998\n",
            "Iteration 5103/6000, G loss: 0.5279380083084106, D loss: 0.003455148544162512\n",
            "Iteration 5104/6000, G loss: 0.45456555485725403, D loss: 0.0034032342955470085\n",
            "Iteration 5105/6000, G loss: 0.5443205833435059, D loss: 0.02138374373316765\n",
            "Iteration 5106/6000, G loss: 0.5344958901405334, D loss: 0.0035023423843085766\n",
            "Iteration 5107/6000, G loss: 0.5734298229217529, D loss: 0.006186508573591709\n",
            "Iteration 5108/6000, G loss: 0.47659802436828613, D loss: 0.0012309248559176922\n",
            "Iteration 5109/6000, G loss: 0.4770655333995819, D loss: 0.03355380892753601\n",
            "Iteration 5110/6000, G loss: 0.5725788474082947, D loss: 0.005847319960594177\n",
            "Iteration 5111/6000, G loss: 0.4554941654205322, D loss: 0.0008081054547801614\n",
            "Iteration 5112/6000, G loss: 0.367075115442276, D loss: 0.008850615471601486\n",
            "Iteration 5113/6000, G loss: 0.4583008885383606, D loss: 0.00722200283780694\n",
            "Iteration 5114/6000, G loss: 0.45309123396873474, D loss: 0.0018792052287608385\n",
            "Iteration 5115/6000, G loss: 0.5851113796234131, D loss: 0.0044757951982319355\n",
            "Iteration 5116/6000, G loss: 0.5685102343559265, D loss: 0.002864392939954996\n",
            "Iteration 5117/6000, G loss: 0.3946089744567871, D loss: 0.07345094531774521\n",
            "Iteration 5118/6000, G loss: 0.4746035933494568, D loss: 0.0001683972222963348\n",
            "Iteration 5119/6000, G loss: 0.4291696846485138, D loss: 0.002694380935281515\n",
            "Iteration 5120/6000, G loss: 0.46309053897857666, D loss: 6.531350663863122e-05\n",
            "Iteration 5121/6000, G loss: 0.4842996597290039, D loss: 0.0028840918093919754\n",
            "Iteration 5122/6000, G loss: 0.5547404289245605, D loss: 0.02557946741580963\n",
            "Iteration 5123/6000, G loss: 0.4490669071674347, D loss: 0.003429081756621599\n",
            "Iteration 5124/6000, G loss: 0.5704553723335266, D loss: 0.004415206611156464\n",
            "Iteration 5125/6000, G loss: 0.49163827300071716, D loss: 0.0007222198182716966\n",
            "Iteration 5126/6000, G loss: 0.5676344633102417, D loss: 0.00041379209142178297\n",
            "Iteration 5127/6000, G loss: 0.5564501881599426, D loss: 0.0018723937682807446\n",
            "Iteration 5128/6000, G loss: 0.48277747631073, D loss: 0.009226186200976372\n",
            "Iteration 5129/6000, G loss: 0.4010574221611023, D loss: 0.0009038402931764722\n",
            "Iteration 5130/6000, G loss: 0.4877561628818512, D loss: 0.017913147807121277\n",
            "Iteration 5131/6000, G loss: 0.5322420597076416, D loss: 0.025494778528809547\n",
            "Iteration 5132/6000, G loss: 0.500916600227356, D loss: 0.0002728874096646905\n",
            "Iteration 5133/6000, G loss: 0.48545199632644653, D loss: 0.0018080391455441713\n",
            "Iteration 5134/6000, G loss: 0.46528151631355286, D loss: 0.001077595166862011\n",
            "Iteration 5135/6000, G loss: 0.44002193212509155, D loss: 0.03431817144155502\n",
            "Iteration 5136/6000, G loss: 0.5404359102249146, D loss: 0.009164298884570599\n",
            "Iteration 5137/6000, G loss: 0.41142117977142334, D loss: 0.0035874892491847277\n",
            "Iteration 5138/6000, G loss: 0.5071311593055725, D loss: 0.00042811199091374874\n",
            "Iteration 5139/6000, G loss: 0.5955812931060791, D loss: 0.005693444982171059\n",
            "Iteration 5140/6000, G loss: 0.44362717866897583, D loss: 0.005715182051062584\n",
            "Iteration 5141/6000, G loss: 0.5583987832069397, D loss: 0.0001221193524543196\n",
            "Iteration 5142/6000, G loss: 0.5683598518371582, D loss: 0.00010890586418099701\n",
            "Iteration 5143/6000, G loss: 0.5080434679985046, D loss: 0.004359391052275896\n",
            "Iteration 5144/6000, G loss: 0.4174818694591522, D loss: 0.005675550550222397\n",
            "Iteration 5145/6000, G loss: 0.5301803946495056, D loss: 0.01713716983795166\n",
            "Iteration 5146/6000, G loss: 0.5668542385101318, D loss: 0.019260836765170097\n",
            "Iteration 5147/6000, G loss: 0.4388186037540436, D loss: 0.03662809729576111\n",
            "Iteration 5148/6000, G loss: 0.553073525428772, D loss: 0.010076907463371754\n",
            "Iteration 5149/6000, G loss: 0.3995269238948822, D loss: 0.00022172153694555163\n",
            "Iteration 5150/6000, G loss: 0.462589830160141, D loss: 0.00028146826662123203\n",
            "Iteration 5151/6000, G loss: 0.5284214615821838, D loss: 0.0016422704793512821\n",
            "Iteration 5152/6000, G loss: 0.5176657438278198, D loss: 0.0005650893435813487\n",
            "Iteration 5153/6000, G loss: 0.46628308296203613, D loss: 0.04415173828601837\n",
            "Iteration 5154/6000, G loss: 0.4762312173843384, D loss: 0.05085156857967377\n",
            "Iteration 5155/6000, G loss: 0.41913098096847534, D loss: 0.013653566129505634\n",
            "Iteration 5156/6000, G loss: 0.427807480096817, D loss: 0.001586676575243473\n",
            "Iteration 5157/6000, G loss: 0.4817858934402466, D loss: 0.020255062729120255\n",
            "Iteration 5158/6000, G loss: 0.4796752631664276, D loss: 0.00013293074152898043\n",
            "Iteration 5159/6000, G loss: 0.3827943801879883, D loss: 0.0009358050301671028\n",
            "Iteration 5160/6000, G loss: 0.5103532671928406, D loss: 0.014705821871757507\n",
            "Iteration 5161/6000, G loss: 0.4158663749694824, D loss: 0.03375754505395889\n",
            "Iteration 5162/6000, G loss: 0.5052130818367004, D loss: 0.00789479911327362\n",
            "Iteration 5163/6000, G loss: 0.4442521035671234, D loss: 0.02496054396033287\n",
            "Iteration 5164/6000, G loss: 0.4490838944911957, D loss: 0.015398528426885605\n",
            "Iteration 5165/6000, G loss: 0.4831836521625519, D loss: 0.046622470021247864\n",
            "Iteration 5166/6000, G loss: 0.45383137464523315, D loss: 0.005444934591650963\n",
            "Iteration 5167/6000, G loss: 0.4957321286201477, D loss: 0.012543853372335434\n",
            "Iteration 5168/6000, G loss: 0.5567221641540527, D loss: 0.05486632138490677\n",
            "Iteration 5169/6000, G loss: 0.39174753427505493, D loss: 0.012295020744204521\n",
            "Iteration 5170/6000, G loss: 0.5228006839752197, D loss: 0.00870007835328579\n",
            "Iteration 5171/6000, G loss: 0.5381210446357727, D loss: 0.0016505904495716095\n",
            "Iteration 5172/6000, G loss: 0.5520332455635071, D loss: 0.003385901916772127\n",
            "Iteration 5173/6000, G loss: 0.47822192311286926, D loss: 0.0022211854811757803\n",
            "Iteration 5174/6000, G loss: 0.5982503890991211, D loss: 0.04017750546336174\n",
            "Iteration 5175/6000, G loss: 0.5285443663597107, D loss: 0.0005934772780165076\n",
            "Iteration 5176/6000, G loss: 0.46124717593193054, D loss: 0.03134002536535263\n",
            "Iteration 5177/6000, G loss: 0.4404788017272949, D loss: 0.015416586771607399\n",
            "Iteration 5178/6000, G loss: 0.5449103713035583, D loss: 0.00017045976710505784\n",
            "Iteration 5179/6000, G loss: 0.4934689700603485, D loss: 0.009905248880386353\n",
            "Iteration 5180/6000, G loss: 0.46732190251350403, D loss: 0.05361843854188919\n",
            "Iteration 5181/6000, G loss: 0.5339550375938416, D loss: 0.008339733816683292\n",
            "Iteration 5182/6000, G loss: 0.541121780872345, D loss: 0.0019176783971488476\n",
            "Iteration 5183/6000, G loss: 0.5003817081451416, D loss: 0.008718897588551044\n",
            "Iteration 5184/6000, G loss: 0.4722086191177368, D loss: 0.2047581970691681\n",
            "Iteration 5185/6000, G loss: 0.4159367084503174, D loss: 0.5033820867538452\n",
            "Iteration 5186/6000, G loss: 0.5225104093551636, D loss: 0.007091273553669453\n",
            "Iteration 5187/6000, G loss: 0.41687798500061035, D loss: 0.002016786951571703\n",
            "Iteration 5188/6000, G loss: 0.4972306489944458, D loss: 0.007664828561246395\n",
            "Iteration 5189/6000, G loss: 0.5324838757514954, D loss: 0.01049018744379282\n",
            "Iteration 5190/6000, G loss: 0.5872505903244019, D loss: 0.011281594634056091\n",
            "Iteration 5191/6000, G loss: 0.42310187220573425, D loss: 0.1008349061012268\n",
            "Iteration 5192/6000, G loss: 0.49016129970550537, D loss: 0.1573602259159088\n",
            "Iteration 5193/6000, G loss: 0.4882230758666992, D loss: 1.458136796951294\n",
            "Iteration 5194/6000, G loss: 0.5969185829162598, D loss: 0.050952255725860596\n",
            "Iteration 5195/6000, G loss: 0.4889719486236572, D loss: 0.00015823017747607082\n",
            "Iteration 5196/6000, G loss: 0.49417585134506226, D loss: 5.5674638133496046e-05\n",
            "Iteration 5197/6000, G loss: 0.46729934215545654, D loss: 0.13586220145225525\n",
            "Iteration 5198/6000, G loss: 0.45879286527633667, D loss: 1.4443321228027344\n",
            "Iteration 5199/6000, G loss: 0.48542243242263794, D loss: 0.18537455797195435\n",
            "Iteration 5200/6000, G loss: 0.4447718858718872, D loss: 0.6141531467437744\n",
            "Iteration 5201/6000, G loss: 0.454540491104126, D loss: 0.0070656100288033485\n",
            "Iteration 5202/6000, G loss: 0.5409927368164062, D loss: 3.1554195880889893\n",
            "Iteration 5203/6000, G loss: 0.5078219175338745, D loss: 0.004423161502927542\n",
            "Iteration 5204/6000, G loss: 0.5490611791610718, D loss: 2.771607796603348e-06\n",
            "Iteration 5205/6000, G loss: 0.5199319124221802, D loss: 2.640483899085666e-06\n",
            "Iteration 5206/6000, G loss: 0.5354053378105164, D loss: 0.00016919882909860462\n",
            "Iteration 5207/6000, G loss: 0.4190383851528168, D loss: 0.0013352400856092572\n",
            "Iteration 5208/6000, G loss: 0.4491812586784363, D loss: 0.0024280711077153683\n",
            "Iteration 5209/6000, G loss: 0.45977258682250977, D loss: 0.024348830804228783\n",
            "Iteration 5210/6000, G loss: 0.6472864151000977, D loss: 0.01816457323729992\n",
            "Iteration 5211/6000, G loss: 0.4302294850349426, D loss: 0.04138666391372681\n",
            "Iteration 5212/6000, G loss: 0.4611392319202423, D loss: 0.0056497082114219666\n",
            "Iteration 5213/6000, G loss: 0.46259135007858276, D loss: 0.03713507950305939\n",
            "Iteration 5214/6000, G loss: 0.4406532049179077, D loss: 0.11146339029073715\n",
            "Iteration 5215/6000, G loss: 0.5287480354309082, D loss: 0.0018523698672652245\n",
            "Iteration 5216/6000, G loss: 0.4801459014415741, D loss: 0.00787363015115261\n",
            "Iteration 5217/6000, G loss: 0.5271139740943909, D loss: 0.04251982271671295\n",
            "Iteration 5218/6000, G loss: 0.48699837923049927, D loss: 0.08344981074333191\n",
            "Iteration 5219/6000, G loss: 0.4714418649673462, D loss: 0.25893333554267883\n",
            "Iteration 5220/6000, G loss: 0.42856067419052124, D loss: 0.0019204396521672606\n",
            "Iteration 5221/6000, G loss: 0.45457854866981506, D loss: 0.0076013850048184395\n",
            "Iteration 5222/6000, G loss: 0.5557584166526794, D loss: 0.0011845279950648546\n",
            "Iteration 5223/6000, G loss: 0.5538033843040466, D loss: 0.057488299906253815\n",
            "Iteration 5224/6000, G loss: 0.4481404423713684, D loss: 0.0011746229138225317\n",
            "Iteration 5225/6000, G loss: 0.5644614696502686, D loss: 0.0030798970256000757\n",
            "Iteration 5226/6000, G loss: 0.5345591306686401, D loss: 0.008318847045302391\n",
            "Iteration 5227/6000, G loss: 0.5530325174331665, D loss: 0.0529216006398201\n",
            "Iteration 5228/6000, G loss: 0.5175249576568604, D loss: 0.003240834455937147\n",
            "Iteration 5229/6000, G loss: 0.5189816951751709, D loss: 0.011122385039925575\n",
            "Iteration 5230/6000, G loss: 0.4700179696083069, D loss: 0.009921642020344734\n",
            "Iteration 5231/6000, G loss: 0.5051116943359375, D loss: 0.009075330570340157\n",
            "Iteration 5232/6000, G loss: 0.5309824347496033, D loss: 0.1809343546628952\n",
            "Iteration 5233/6000, G loss: 0.5672001838684082, D loss: 0.012320800684392452\n",
            "Iteration 5234/6000, G loss: 0.5601487159729004, D loss: 0.0011473451741039753\n",
            "Iteration 5235/6000, G loss: 0.5007473230361938, D loss: 0.04849996045231819\n",
            "Iteration 5236/6000, G loss: 0.4532102644443512, D loss: 0.13628685474395752\n",
            "Iteration 5237/6000, G loss: 0.5143023729324341, D loss: 0.016822509467601776\n",
            "Iteration 5238/6000, G loss: 0.511337399482727, D loss: 0.31593430042266846\n",
            "Iteration 5239/6000, G loss: 0.4553855359554291, D loss: 0.003650144673883915\n",
            "Iteration 5240/6000, G loss: 0.48994722962379456, D loss: 0.0018908369820564985\n",
            "Iteration 5241/6000, G loss: 0.5964482426643372, D loss: 0.2985907793045044\n",
            "Iteration 5242/6000, G loss: 0.4221787452697754, D loss: 0.22226330637931824\n",
            "Iteration 5243/6000, G loss: 0.5035770535469055, D loss: 0.00127817050088197\n",
            "Iteration 5244/6000, G loss: 0.4941854178905487, D loss: 0.0004963316023349762\n",
            "Iteration 5245/6000, G loss: 0.5444883704185486, D loss: 0.00013723313168156892\n",
            "Iteration 5246/6000, G loss: 0.5589806437492371, D loss: 0.0043096719309687614\n",
            "Iteration 5247/6000, G loss: 0.4004213213920593, D loss: 0.4621645510196686\n",
            "Iteration 5248/6000, G loss: 0.6433297991752625, D loss: 0.000587530666962266\n",
            "Iteration 5249/6000, G loss: 0.5734714269638062, D loss: 0.12333452701568604\n",
            "Iteration 5250/6000, G loss: 0.5182396769523621, D loss: 0.5337826609611511\n",
            "Iteration 5251/6000, G loss: 0.5011423826217651, D loss: 2.8740100860595703\n",
            "Iteration 5252/6000, G loss: 0.6906642913818359, D loss: 1.4358676708070561e-05\n",
            "Iteration 5253/6000, G loss: 0.5424946546554565, D loss: 0.0029990416951477528\n",
            "Iteration 5254/6000, G loss: 0.6276120543479919, D loss: 0.0015711162704974413\n",
            "Iteration 5255/6000, G loss: 0.54937744140625, D loss: 0.019520491361618042\n",
            "Iteration 5256/6000, G loss: 0.493788480758667, D loss: 0.01437660213559866\n",
            "Iteration 5257/6000, G loss: 0.6243138313293457, D loss: 0.19331538677215576\n",
            "Iteration 5258/6000, G loss: 0.479077011346817, D loss: 0.7332562804222107\n",
            "Iteration 5259/6000, G loss: 0.5204088687896729, D loss: 0.00013502365618478507\n",
            "Iteration 5260/6000, G loss: 0.40465325117111206, D loss: 7.88103134254925e-05\n",
            "Iteration 5261/6000, G loss: 0.5614948272705078, D loss: 0.00041519029764458537\n",
            "Iteration 5262/6000, G loss: 0.43979671597480774, D loss: 1.6751701831817627\n",
            "Iteration 5263/6000, G loss: 0.4821133613586426, D loss: 0.0025754941161721945\n",
            "Iteration 5264/6000, G loss: 0.3611188232898712, D loss: 0.0027755689807236195\n",
            "Iteration 5265/6000, G loss: 0.4007265567779541, D loss: 0.10769200325012207\n",
            "Iteration 5266/6000, G loss: 0.5033394694328308, D loss: 0.2817055583000183\n",
            "Iteration 5267/6000, G loss: 0.389462947845459, D loss: 0.01318463683128357\n",
            "Iteration 5268/6000, G loss: 0.409811407327652, D loss: 0.22388429939746857\n",
            "Iteration 5269/6000, G loss: 0.5603959560394287, D loss: 0.0010261449497193098\n",
            "Iteration 5270/6000, G loss: 0.5271741151809692, D loss: 0.0022789386566728354\n",
            "Iteration 5271/6000, G loss: 0.46939802169799805, D loss: 0.006434152368456125\n",
            "Iteration 5272/6000, G loss: 0.4520884156227112, D loss: 0.37915676832199097\n",
            "Iteration 5273/6000, G loss: 0.48974546790122986, D loss: 0.00798990298062563\n",
            "Iteration 5274/6000, G loss: 0.5070086121559143, D loss: 0.005139001179486513\n",
            "Iteration 5275/6000, G loss: 0.43533703684806824, D loss: 0.07142373919487\n",
            "Iteration 5276/6000, G loss: 0.4934591054916382, D loss: 0.0001904038363136351\n",
            "Iteration 5277/6000, G loss: 0.4785955846309662, D loss: 0.00010197615483775735\n",
            "Iteration 5278/6000, G loss: 0.514719545841217, D loss: 0.00048092720680870116\n",
            "Iteration 5279/6000, G loss: 0.5366740822792053, D loss: 0.045781202614307404\n",
            "Iteration 5280/6000, G loss: 0.5845195055007935, D loss: 0.002863023430109024\n",
            "Iteration 5281/6000, G loss: 0.44586893916130066, D loss: 0.002147689461708069\n",
            "Iteration 5282/6000, G loss: 0.6087547540664673, D loss: 0.0028547639958560467\n",
            "Iteration 5283/6000, G loss: 0.5538968443870544, D loss: 0.04032307118177414\n",
            "Iteration 5284/6000, G loss: 0.5612920522689819, D loss: 0.24707700312137604\n",
            "Iteration 5285/6000, G loss: 0.435217022895813, D loss: 0.0016090867575258017\n",
            "Iteration 5286/6000, G loss: 0.5282464623451233, D loss: 0.005741019733250141\n",
            "Iteration 5287/6000, G loss: 0.44551873207092285, D loss: 0.007562858983874321\n",
            "Iteration 5288/6000, G loss: 0.5389959216117859, D loss: 0.004343999549746513\n",
            "Iteration 5289/6000, G loss: 0.5668178796768188, D loss: 0.0046466803178191185\n",
            "Iteration 5290/6000, G loss: 0.3776097595691681, D loss: 0.0682002604007721\n",
            "Iteration 5291/6000, G loss: 0.3230777680873871, D loss: 8.345248222351074\n",
            "Iteration 5292/6000, G loss: 0.5508017539978027, D loss: 0.004926004447042942\n",
            "Iteration 5293/6000, G loss: 0.5113337635993958, D loss: 0.03515417128801346\n",
            "Iteration 5294/6000, G loss: 0.4200153350830078, D loss: 0.0030469128396362066\n",
            "Iteration 5295/6000, G loss: 0.4992264211177826, D loss: 0.0028503870125859976\n",
            "Iteration 5296/6000, G loss: 0.5432655215263367, D loss: 0.003488963469862938\n",
            "Iteration 5297/6000, G loss: 0.4139325022697449, D loss: 0.00421305513009429\n",
            "Iteration 5298/6000, G loss: 0.6368182897567749, D loss: 0.11335235834121704\n",
            "Iteration 5299/6000, G loss: 0.4720333218574524, D loss: 0.03688473254442215\n",
            "Iteration 5300/6000, G loss: 0.40883931517601013, D loss: 0.010779926553368568\n",
            "Iteration 5301/6000, G loss: 0.4089832305908203, D loss: 0.005108910147100687\n",
            "Iteration 5302/6000, G loss: 0.53102707862854, D loss: 0.002145159523934126\n",
            "Iteration 5303/6000, G loss: 0.5889701843261719, D loss: 0.015255307778716087\n",
            "Iteration 5304/6000, G loss: 0.5133289694786072, D loss: 0.01313414704054594\n",
            "Iteration 5305/6000, G loss: 0.4185377359390259, D loss: 0.007321883924305439\n",
            "Iteration 5306/6000, G loss: 0.5837922692298889, D loss: 0.006579061038792133\n",
            "Iteration 5307/6000, G loss: 0.40322819352149963, D loss: 0.12970489263534546\n",
            "Iteration 5308/6000, G loss: 0.45262473821640015, D loss: 0.004099157638847828\n",
            "Iteration 5309/6000, G loss: 0.5168127417564392, D loss: 0.006492914166301489\n",
            "Iteration 5310/6000, G loss: 0.46132197976112366, D loss: 0.009245825000107288\n",
            "Iteration 5311/6000, G loss: 0.46964895725250244, D loss: 0.014190539717674255\n",
            "Iteration 5312/6000, G loss: 0.5009359121322632, D loss: 0.02000289596617222\n",
            "Iteration 5313/6000, G loss: 0.4903194308280945, D loss: 0.0004233051440678537\n",
            "Iteration 5314/6000, G loss: 0.49627187848091125, D loss: 0.019704366102814674\n",
            "Iteration 5315/6000, G loss: 0.5321066379547119, D loss: 0.054235685616731644\n",
            "Iteration 5316/6000, G loss: 0.506056010723114, D loss: 0.04744935780763626\n",
            "Iteration 5317/6000, G loss: 0.44293656945228577, D loss: 0.07939950376749039\n",
            "Iteration 5318/6000, G loss: 0.5236760377883911, D loss: 0.000215899694012478\n",
            "Iteration 5319/6000, G loss: 0.48578277230262756, D loss: 0.005642763804644346\n",
            "Iteration 5320/6000, G loss: 0.4548373222351074, D loss: 0.008423706516623497\n",
            "Iteration 5321/6000, G loss: 0.43394574522972107, D loss: 0.005244359374046326\n",
            "Iteration 5322/6000, G loss: 0.48900851607322693, D loss: 0.028604885563254356\n",
            "Iteration 5323/6000, G loss: 0.4889270067214966, D loss: 0.023988181725144386\n",
            "Iteration 5324/6000, G loss: 0.4189038872718811, D loss: 0.03644808381795883\n",
            "Iteration 5325/6000, G loss: 0.42256784439086914, D loss: 0.005218719132244587\n",
            "Iteration 5326/6000, G loss: 0.4565044939517975, D loss: 0.00037995100137777627\n",
            "Iteration 5327/6000, G loss: 0.4286835789680481, D loss: 0.010905035771429539\n",
            "Iteration 5328/6000, G loss: 0.4114396274089813, D loss: 0.0006077426951378584\n",
            "Iteration 5329/6000, G loss: 0.5168296694755554, D loss: 0.012905027717351913\n",
            "Iteration 5330/6000, G loss: 0.6268605589866638, D loss: 0.0014471985632553697\n",
            "Iteration 5331/6000, G loss: 0.48681923747062683, D loss: 0.009873567149043083\n",
            "Iteration 5332/6000, G loss: 0.519078254699707, D loss: 0.022936586290597916\n",
            "Iteration 5333/6000, G loss: 0.5774396657943726, D loss: 0.001131795928813517\n",
            "Iteration 5334/6000, G loss: 0.5112419724464417, D loss: 0.002268334850668907\n",
            "Iteration 5335/6000, G loss: 0.3920324742794037, D loss: 0.12364621460437775\n",
            "Iteration 5336/6000, G loss: 0.39640671014785767, D loss: 0.006855233572423458\n",
            "Iteration 5337/6000, G loss: 0.47841694951057434, D loss: 0.0018565692007541656\n",
            "Iteration 5338/6000, G loss: 0.5594756007194519, D loss: 0.04619968682527542\n",
            "Iteration 5339/6000, G loss: 0.42826607823371887, D loss: 0.0027294845785945654\n",
            "Iteration 5340/6000, G loss: 0.4170595109462738, D loss: 0.019683126360177994\n",
            "Iteration 5341/6000, G loss: 0.5123134851455688, D loss: 0.008328165858983994\n",
            "Iteration 5342/6000, G loss: 0.4580157995223999, D loss: 0.0008728484390303493\n",
            "Iteration 5343/6000, G loss: 0.5442095398902893, D loss: 0.9286606311798096\n",
            "Iteration 5344/6000, G loss: 0.4426942765712738, D loss: 0.04988482594490051\n",
            "Iteration 5345/6000, G loss: 0.5389164686203003, D loss: 0.003294840920716524\n",
            "Iteration 5346/6000, G loss: 0.5460225939750671, D loss: 0.018152888864278793\n",
            "Iteration 5347/6000, G loss: 0.4621492326259613, D loss: 0.002892975229769945\n",
            "Iteration 5348/6000, G loss: 0.5467896461486816, D loss: 0.015265697613358498\n",
            "Iteration 5349/6000, G loss: 0.5618691444396973, D loss: 0.018753089010715485\n",
            "Iteration 5350/6000, G loss: 0.4334869682788849, D loss: 0.026763249188661575\n",
            "Iteration 5351/6000, G loss: 0.4777624309062958, D loss: 0.0013128322316333652\n",
            "Iteration 5352/6000, G loss: 0.5129814147949219, D loss: 0.053007908165454865\n",
            "Iteration 5353/6000, G loss: 0.5673534274101257, D loss: 0.000262238085269928\n",
            "Iteration 5354/6000, G loss: 0.5321434736251831, D loss: 0.003821061458438635\n",
            "Iteration 5355/6000, G loss: 0.502275824546814, D loss: 0.12590624392032623\n",
            "Iteration 5356/6000, G loss: 0.4265841543674469, D loss: 0.5195425748825073\n",
            "Iteration 5357/6000, G loss: 0.5393833518028259, D loss: 0.038130272179841995\n",
            "Iteration 5358/6000, G loss: 0.48141130805015564, D loss: 0.0014694496057927608\n",
            "Iteration 5359/6000, G loss: 0.6008415222167969, D loss: 0.008866839110851288\n",
            "Iteration 5360/6000, G loss: 0.5632801055908203, D loss: 0.01591849885880947\n",
            "Iteration 5361/6000, G loss: 0.50226891040802, D loss: 7.09453010559082\n",
            "Iteration 5362/6000, G loss: 0.5342060923576355, D loss: 0.00048049757606349885\n",
            "Iteration 5363/6000, G loss: 0.4380832612514496, D loss: 0.02367883175611496\n",
            "Iteration 5364/6000, G loss: 0.4705311059951782, D loss: 0.001633612671867013\n",
            "Iteration 5365/6000, G loss: 0.4393358826637268, D loss: 0.0028567127883434296\n",
            "Iteration 5366/6000, G loss: 0.4891488254070282, D loss: 0.007827729918062687\n",
            "Iteration 5367/6000, G loss: 0.606368362903595, D loss: 0.023362867534160614\n",
            "Iteration 5368/6000, G loss: 0.43389734625816345, D loss: 2.5772067601792514e-05\n",
            "Iteration 5369/6000, G loss: 0.48883292078971863, D loss: 0.00021652024588547647\n",
            "Iteration 5370/6000, G loss: 0.48987945914268494, D loss: 0.0039528654888272285\n",
            "Iteration 5371/6000, G loss: 0.4124419093132019, D loss: 0.0024682285729795694\n",
            "Iteration 5372/6000, G loss: 0.5061706900596619, D loss: 0.22134125232696533\n",
            "Iteration 5373/6000, G loss: 0.433686763048172, D loss: 0.011165562085807323\n",
            "Iteration 5374/6000, G loss: 0.5300508141517639, D loss: 0.003491148818284273\n",
            "Iteration 5375/6000, G loss: 0.5025836825370789, D loss: 0.0027009504847228527\n",
            "Iteration 5376/6000, G loss: 0.540148913860321, D loss: 0.000537003914359957\n",
            "Iteration 5377/6000, G loss: 0.5286368727684021, D loss: 0.0036412994377315044\n",
            "Iteration 5378/6000, G loss: 0.5535649657249451, D loss: 0.011509200558066368\n",
            "Iteration 5379/6000, G loss: 0.4776194393634796, D loss: 0.07508797943592072\n",
            "Iteration 5380/6000, G loss: 0.5325460433959961, D loss: 0.07859297096729279\n",
            "Iteration 5381/6000, G loss: 0.4884908199310303, D loss: 0.040541134774684906\n",
            "Iteration 5382/6000, G loss: 0.4962678551673889, D loss: 0.10726020485162735\n",
            "Iteration 5383/6000, G loss: 0.45145246386528015, D loss: 0.0036170436069369316\n",
            "Iteration 5384/6000, G loss: 0.4151575565338135, D loss: 0.0025295366067439318\n",
            "Iteration 5385/6000, G loss: 0.38577204942703247, D loss: 0.08219297230243683\n",
            "Iteration 5386/6000, G loss: 0.4022141993045807, D loss: 0.7790396213531494\n",
            "Iteration 5387/6000, G loss: 0.5980529189109802, D loss: 0.2808759808540344\n",
            "Iteration 5388/6000, G loss: 0.504295825958252, D loss: 0.00035382702481001616\n",
            "Iteration 5389/6000, G loss: 0.538718044757843, D loss: 0.010722295381128788\n",
            "Iteration 5390/6000, G loss: 0.4061412215232849, D loss: 0.027822742238640785\n",
            "Iteration 5391/6000, G loss: 0.46777626872062683, D loss: 0.2280201017856598\n",
            "Iteration 5392/6000, G loss: 0.6030918955802917, D loss: 0.0017761779017746449\n",
            "Iteration 5393/6000, G loss: 0.590603768825531, D loss: 0.0014493410708382726\n",
            "Iteration 5394/6000, G loss: 0.3944408893585205, D loss: 0.06268554925918579\n",
            "Iteration 5395/6000, G loss: 0.4297858476638794, D loss: 0.0037859769072383642\n",
            "Iteration 5396/6000, G loss: 0.5965630412101746, D loss: 0.000801838468760252\n",
            "Iteration 5397/6000, G loss: 0.520243763923645, D loss: 0.008711889386177063\n",
            "Iteration 5398/6000, G loss: 0.49772605299949646, D loss: 0.2593289613723755\n",
            "Iteration 5399/6000, G loss: 0.43080374598503113, D loss: 0.5841749906539917\n",
            "Iteration 5400/6000, G loss: 0.5471711158752441, D loss: 0.014038123190402985\n",
            "Iteration 5401/6000, G loss: 0.479193776845932, D loss: 0.010685862973332405\n",
            "Iteration 5402/6000, G loss: 0.5171338319778442, D loss: 0.001466970075853169\n",
            "Iteration 5403/6000, G loss: 0.5049930810928345, D loss: 0.0008399175712838769\n",
            "Iteration 5404/6000, G loss: 0.49139276146888733, D loss: 10.879087448120117\n",
            "Iteration 5405/6000, G loss: 0.4713192582130432, D loss: 1.193228006362915\n",
            "Iteration 5406/6000, G loss: 0.5496006011962891, D loss: 0.0032134437933564186\n",
            "Iteration 5407/6000, G loss: 0.5065300464630127, D loss: 0.0029126321896910667\n",
            "Iteration 5408/6000, G loss: 0.4743378758430481, D loss: 0.010989002883434296\n",
            "Iteration 5409/6000, G loss: 0.43804073333740234, D loss: 0.013965204358100891\n",
            "Iteration 5410/6000, G loss: 0.5422806739807129, D loss: 0.0537940189242363\n",
            "Iteration 5411/6000, G loss: 0.3684057891368866, D loss: 0.19607630372047424\n",
            "Iteration 5412/6000, G loss: 0.3790759742259979, D loss: 0.46029406785964966\n",
            "Iteration 5413/6000, G loss: 0.4853934347629547, D loss: 0.12021329998970032\n",
            "Iteration 5414/6000, G loss: 0.4587711691856384, D loss: 0.10486049950122833\n",
            "Iteration 5415/6000, G loss: 0.5733494162559509, D loss: 0.21753966808319092\n",
            "Iteration 5416/6000, G loss: 0.35237443447113037, D loss: 0.1988568902015686\n",
            "Iteration 5417/6000, G loss: 0.5185536742210388, D loss: 0.21859678626060486\n",
            "Iteration 5418/6000, G loss: 0.5609811544418335, D loss: 0.03240189701318741\n",
            "Iteration 5419/6000, G loss: 0.40940213203430176, D loss: 0.09381103515625\n",
            "Iteration 5420/6000, G loss: 0.5505074858665466, D loss: 0.0520971342921257\n",
            "Iteration 5421/6000, G loss: 0.5002090930938721, D loss: 0.1041824072599411\n",
            "Iteration 5422/6000, G loss: 0.4377281963825226, D loss: 0.10823588073253632\n",
            "Iteration 5423/6000, G loss: 0.4404875636100769, D loss: 0.016833361238241196\n",
            "Iteration 5424/6000, G loss: 0.5132536888122559, D loss: 0.07206414639949799\n",
            "Iteration 5425/6000, G loss: 0.4413856267929077, D loss: 0.08111035823822021\n",
            "Iteration 5426/6000, G loss: 0.5460745096206665, D loss: 0.026478007435798645\n",
            "Iteration 5427/6000, G loss: 0.4432952105998993, D loss: 0.015340609475970268\n",
            "Iteration 5428/6000, G loss: 0.47823989391326904, D loss: 0.02060667611658573\n",
            "Iteration 5429/6000, G loss: 0.49928635358810425, D loss: 0.016099901869893074\n",
            "Iteration 5430/6000, G loss: 0.4202049970626831, D loss: 0.030834898352622986\n",
            "Iteration 5431/6000, G loss: 0.513087272644043, D loss: 0.022342611104249954\n",
            "Iteration 5432/6000, G loss: 0.4418375492095947, D loss: 0.009411253035068512\n",
            "Iteration 5433/6000, G loss: 0.4472186863422394, D loss: 0.04801170900464058\n",
            "Iteration 5434/6000, G loss: 0.47983652353286743, D loss: 0.013481290079653263\n",
            "Iteration 5435/6000, G loss: 0.5654060244560242, D loss: 0.00222922395914793\n",
            "Iteration 5436/6000, G loss: 0.4964624047279358, D loss: 0.0069182557053864\n",
            "Iteration 5437/6000, G loss: 0.37196433544158936, D loss: 0.026323847472667694\n",
            "Iteration 5438/6000, G loss: 0.5559812188148499, D loss: 0.02209324575960636\n",
            "Iteration 5439/6000, G loss: 0.51163649559021, D loss: 0.03233395144343376\n",
            "Iteration 5440/6000, G loss: 0.45147281885147095, D loss: 0.06762523204088211\n",
            "Iteration 5441/6000, G loss: 0.512159526348114, D loss: 0.003347051329910755\n",
            "Iteration 5442/6000, G loss: 0.461387038230896, D loss: 0.002552588004618883\n",
            "Iteration 5443/6000, G loss: 0.5808315873146057, D loss: 0.002965535968542099\n",
            "Iteration 5444/6000, G loss: 0.49554315209388733, D loss: 0.0016317556146532297\n",
            "Iteration 5445/6000, G loss: 0.6378639936447144, D loss: 0.004297681152820587\n",
            "Iteration 5446/6000, G loss: 0.47209352254867554, D loss: 0.02631242573261261\n",
            "Iteration 5447/6000, G loss: 0.5120338797569275, D loss: 0.0035461934749037027\n",
            "Iteration 5448/6000, G loss: 0.6178915500640869, D loss: 0.003929138649255037\n",
            "Iteration 5449/6000, G loss: 0.4413370192050934, D loss: 0.007876374758780003\n",
            "Iteration 5450/6000, G loss: 0.3816765546798706, D loss: 0.0360240675508976\n",
            "Iteration 5451/6000, G loss: 0.5194529891014099, D loss: 0.0009043037425726652\n",
            "Iteration 5452/6000, G loss: 0.4436161518096924, D loss: 0.010197033174335957\n",
            "Iteration 5453/6000, G loss: 0.5304709076881409, D loss: 0.0005512007046490908\n",
            "Iteration 5454/6000, G loss: 0.5166007280349731, D loss: 0.0025093546137213707\n",
            "Iteration 5455/6000, G loss: 0.6417187452316284, D loss: 0.0014319515321403742\n",
            "Iteration 5456/6000, G loss: 0.40642112493515015, D loss: 0.018188269808888435\n",
            "Iteration 5457/6000, G loss: 0.4318613111972809, D loss: 0.0039015423972159624\n",
            "Iteration 5458/6000, G loss: 0.46561363339424133, D loss: 0.009460854344069958\n",
            "Iteration 5459/6000, G loss: 0.47122108936309814, D loss: 0.022186661139130592\n",
            "Iteration 5460/6000, G loss: 0.41810905933380127, D loss: 0.012603800743818283\n",
            "Iteration 5461/6000, G loss: 0.42625388503074646, D loss: 0.038078971207141876\n",
            "Iteration 5462/6000, G loss: 0.4936797320842743, D loss: 0.008143529295921326\n",
            "Iteration 5463/6000, G loss: 0.4168214201927185, D loss: 0.009137016721069813\n",
            "Iteration 5464/6000, G loss: 0.5829333066940308, D loss: 0.0013010106049478054\n",
            "Iteration 5465/6000, G loss: 0.3945699632167816, D loss: 0.002863106317818165\n",
            "Iteration 5466/6000, G loss: 0.43351078033447266, D loss: 0.00955461710691452\n",
            "Iteration 5467/6000, G loss: 0.4315977394580841, D loss: 0.19466397166252136\n",
            "Iteration 5468/6000, G loss: 0.5329347848892212, D loss: 0.006460675969719887\n",
            "Iteration 5469/6000, G loss: 0.4765315055847168, D loss: 0.10033036768436432\n",
            "Iteration 5470/6000, G loss: 0.37699857354164124, D loss: 0.004747167229652405\n",
            "Iteration 5471/6000, G loss: 0.48516350984573364, D loss: 0.23233291506767273\n",
            "Iteration 5472/6000, G loss: 0.3637314438819885, D loss: 0.013927881605923176\n",
            "Iteration 5473/6000, G loss: 0.4230603575706482, D loss: 0.0021220683120191097\n",
            "Iteration 5474/6000, G loss: 0.44487857818603516, D loss: 0.010955080389976501\n",
            "Iteration 5475/6000, G loss: 0.5239875912666321, D loss: 0.017400303855538368\n",
            "Iteration 5476/6000, G loss: 0.45849859714508057, D loss: 0.10157056152820587\n",
            "Iteration 5477/6000, G loss: 0.4314988851547241, D loss: 0.0031477357260882854\n",
            "Iteration 5478/6000, G loss: 0.5570937395095825, D loss: 0.021233532577753067\n",
            "Iteration 5479/6000, G loss: 0.5059158205986023, D loss: 0.0017621784936636686\n",
            "Iteration 5480/6000, G loss: 0.5590113997459412, D loss: 0.0016826505307108164\n",
            "Iteration 5481/6000, G loss: 0.4447025656700134, D loss: 0.0038068722933530807\n",
            "Iteration 5482/6000, G loss: 0.4833821952342987, D loss: 0.0025672712363302708\n",
            "Iteration 5483/6000, G loss: 0.5726338624954224, D loss: 0.00035083747934550047\n",
            "Iteration 5484/6000, G loss: 0.4941500723361969, D loss: 0.05944216251373291\n",
            "Iteration 5485/6000, G loss: 0.4988049566745758, D loss: 0.014210259541869164\n",
            "Iteration 5486/6000, G loss: 0.4533470571041107, D loss: 0.1712149679660797\n",
            "Iteration 5487/6000, G loss: 0.5254393815994263, D loss: 0.004859889857470989\n",
            "Iteration 5488/6000, G loss: 0.5151485800743103, D loss: 0.009078936651349068\n",
            "Iteration 5489/6000, G loss: 0.538122832775116, D loss: 0.00292758597061038\n",
            "Iteration 5490/6000, G loss: 0.37134575843811035, D loss: 0.005873788148164749\n",
            "Iteration 5491/6000, G loss: 0.4083559215068817, D loss: 0.008974024094641209\n",
            "Iteration 5492/6000, G loss: 0.6013039946556091, D loss: 0.08787823468446732\n",
            "Iteration 5493/6000, G loss: 0.6176853775978088, D loss: 0.09996785968542099\n",
            "Iteration 5494/6000, G loss: 0.5544759035110474, D loss: 0.28631484508514404\n",
            "Iteration 5495/6000, G loss: 0.38212403655052185, D loss: 0.013547835871577263\n",
            "Iteration 5496/6000, G loss: 0.5643376708030701, D loss: 0.006392099894583225\n",
            "Iteration 5497/6000, G loss: 0.413432776927948, D loss: 0.07100136578083038\n",
            "Iteration 5498/6000, G loss: 0.40942347049713135, D loss: 0.008878989145159721\n",
            "Iteration 5499/6000, G loss: 0.5347009897232056, D loss: 0.43757158517837524\n",
            "Iteration 5500/6000, G loss: 0.5216183066368103, D loss: 0.10808412730693817\n",
            "Iteration 5501/6000, G loss: 0.4709896743297577, D loss: 0.18198378384113312\n",
            "Iteration 5502/6000, G loss: 0.365304172039032, D loss: 9.418363571166992\n",
            "Iteration 5503/6000, G loss: 0.6121833324432373, D loss: 0.00010053784353658557\n",
            "Iteration 5504/6000, G loss: 0.40195906162261963, D loss: 0.011140469461679459\n",
            "Iteration 5505/6000, G loss: 0.6022935509681702, D loss: 3.079726957366802e-05\n",
            "Iteration 5506/6000, G loss: 0.5167209506034851, D loss: 0.0016722172731533647\n",
            "Iteration 5507/6000, G loss: 0.39911016821861267, D loss: 0.026253104209899902\n",
            "Iteration 5508/6000, G loss: 0.508050262928009, D loss: 0.10053056478500366\n",
            "Iteration 5509/6000, G loss: 0.4627598524093628, D loss: 0.021009087562561035\n",
            "Iteration 5510/6000, G loss: 0.40312403440475464, D loss: 0.1052599623799324\n",
            "Iteration 5511/6000, G loss: 0.43231475353240967, D loss: 0.1680302619934082\n",
            "Iteration 5512/6000, G loss: 0.4708876609802246, D loss: 0.004220569971948862\n",
            "Iteration 5513/6000, G loss: 0.588093638420105, D loss: 0.0015342164551839232\n",
            "Iteration 5514/6000, G loss: 0.4054087996482849, D loss: 0.013623276725411415\n",
            "Iteration 5515/6000, G loss: 0.4045584201812744, D loss: 0.021932626143097878\n",
            "Iteration 5516/6000, G loss: 0.4143441617488861, D loss: 0.025183282792568207\n",
            "Iteration 5517/6000, G loss: 0.35805001854896545, D loss: 0.028696373105049133\n",
            "Iteration 5518/6000, G loss: 0.5190727710723877, D loss: 0.03687542676925659\n",
            "Iteration 5519/6000, G loss: 0.5042246580123901, D loss: 0.027424732223153114\n",
            "Iteration 5520/6000, G loss: 0.5768779516220093, D loss: 0.0006962785846553743\n",
            "Iteration 5521/6000, G loss: 0.5274178385734558, D loss: 0.0017783298389986157\n",
            "Iteration 5522/6000, G loss: 0.5666882395744324, D loss: 0.002605059649795294\n",
            "Iteration 5523/6000, G loss: 0.43711259961128235, D loss: 0.0188782699406147\n",
            "Iteration 5524/6000, G loss: 0.4881739318370819, D loss: 0.006677641533315182\n",
            "Iteration 5525/6000, G loss: 0.4097886085510254, D loss: 0.053217560052871704\n",
            "Iteration 5526/6000, G loss: 0.3862215578556061, D loss: 0.018354181200265884\n",
            "Iteration 5527/6000, G loss: 0.4662550687789917, D loss: 0.007730371318757534\n",
            "Iteration 5528/6000, G loss: 0.4130402207374573, D loss: 0.01707065850496292\n",
            "Iteration 5529/6000, G loss: 0.4692171514034271, D loss: 0.0009447699412703514\n",
            "Iteration 5530/6000, G loss: 0.573868989944458, D loss: 0.0006188757251948118\n",
            "Iteration 5531/6000, G loss: 0.5009074211120605, D loss: 0.0012120578903704882\n",
            "Iteration 5532/6000, G loss: 0.47334226965904236, D loss: 0.005663049407303333\n",
            "Iteration 5533/6000, G loss: 0.5430871844291687, D loss: 0.0016559874638915062\n",
            "Iteration 5534/6000, G loss: 0.5504992604255676, D loss: 0.001139891566708684\n",
            "Iteration 5535/6000, G loss: 0.44166597723960876, D loss: 0.004110126290470362\n",
            "Iteration 5536/6000, G loss: 0.5588169693946838, D loss: 0.003044658340513706\n",
            "Iteration 5537/6000, G loss: 0.47043415904045105, D loss: 0.004413130693137646\n",
            "Iteration 5538/6000, G loss: 0.5042576193809509, D loss: 0.0004588377196341753\n",
            "Iteration 5539/6000, G loss: 0.48798590898513794, D loss: 0.019363950937986374\n",
            "Iteration 5540/6000, G loss: 0.49514245986938477, D loss: 0.00023968482855707407\n",
            "Iteration 5541/6000, G loss: 0.45912057161331177, D loss: 0.009763790294528008\n",
            "Iteration 5542/6000, G loss: 0.4444177746772766, D loss: 0.004316846374422312\n",
            "Iteration 5543/6000, G loss: 0.4990118443965912, D loss: 0.012300228700041771\n",
            "Iteration 5544/6000, G loss: 0.5047711730003357, D loss: 0.00021410943008959293\n",
            "Iteration 5545/6000, G loss: 0.5970988273620605, D loss: 0.002245083451271057\n",
            "Iteration 5546/6000, G loss: 0.47752290964126587, D loss: 0.011412369087338448\n",
            "Iteration 5547/6000, G loss: 0.5065553188323975, D loss: 0.0024266820400953293\n",
            "Iteration 5548/6000, G loss: 0.46394795179367065, D loss: 0.007613564375787973\n",
            "Iteration 5549/6000, G loss: 0.4214783310890198, D loss: 0.019910778850317\n",
            "Iteration 5550/6000, G loss: 0.5885499715805054, D loss: 0.0003425409668125212\n",
            "Iteration 5551/6000, G loss: 0.3356579840183258, D loss: 0.024123569950461388\n",
            "Iteration 5552/6000, G loss: 0.3928581774234772, D loss: 0.018727310001850128\n",
            "Iteration 5553/6000, G loss: 0.4086475074291229, D loss: 0.0018258140189573169\n",
            "Iteration 5554/6000, G loss: 0.43403029441833496, D loss: 0.00030683737713843584\n",
            "Iteration 5555/6000, G loss: 0.5845729112625122, D loss: 0.0021268902346491814\n",
            "Iteration 5556/6000, G loss: 0.5278759002685547, D loss: 0.0005058749811723828\n",
            "Iteration 5557/6000, G loss: 0.32299432158470154, D loss: 0.021610602736473083\n",
            "Iteration 5558/6000, G loss: 0.35144227743148804, D loss: 0.35154905915260315\n",
            "Iteration 5559/6000, G loss: 0.49742671847343445, D loss: 0.0003472892567515373\n",
            "Iteration 5560/6000, G loss: 0.47700363397598267, D loss: 0.033071983605623245\n",
            "Iteration 5561/6000, G loss: 0.40058618783950806, D loss: 0.007938774302601814\n",
            "Iteration 5562/6000, G loss: 0.4877687096595764, D loss: 0.002803290029987693\n",
            "Iteration 5563/6000, G loss: 0.5465173125267029, D loss: 0.0012812400236725807\n",
            "Iteration 5564/6000, G loss: 0.5309274196624756, D loss: 0.00040218106005340815\n",
            "Iteration 5565/6000, G loss: 0.5158706307411194, D loss: 0.0011583891464397311\n",
            "Iteration 5566/6000, G loss: 0.3870749771595001, D loss: 0.04853926599025726\n",
            "Iteration 5567/6000, G loss: 0.4399976134300232, D loss: 0.0024933379609137774\n",
            "Iteration 5568/6000, G loss: 0.4617201089859009, D loss: 0.0128712709993124\n",
            "Iteration 5569/6000, G loss: 0.5573807954788208, D loss: 0.00021757047215942293\n",
            "Iteration 5570/6000, G loss: 0.519417405128479, D loss: 5.304803380568046e-06\n",
            "Iteration 5571/6000, G loss: 0.5148128271102905, D loss: 5.615245027001947e-05\n",
            "Iteration 5572/6000, G loss: 0.44261792302131653, D loss: 0.03393253684043884\n",
            "Iteration 5573/6000, G loss: 0.5220116972923279, D loss: 0.007339720614254475\n",
            "Iteration 5574/6000, G loss: 0.4204089045524597, D loss: 0.0009632525616325438\n",
            "Iteration 5575/6000, G loss: 0.5667862892150879, D loss: 0.014528710395097733\n",
            "Iteration 5576/6000, G loss: 0.4825117886066437, D loss: 0.0006704638944938779\n",
            "Iteration 5577/6000, G loss: 0.4968695640563965, D loss: 6.38364417682169e-06\n",
            "Iteration 5578/6000, G loss: 0.4064916670322418, D loss: 0.0014005494304001331\n",
            "Iteration 5579/6000, G loss: 0.5916199684143066, D loss: 0.00011447176802903414\n",
            "Iteration 5580/6000, G loss: 0.5875784158706665, D loss: 0.0008767331019043922\n",
            "Iteration 5581/6000, G loss: 0.4654010832309723, D loss: 0.010872971266508102\n",
            "Iteration 5582/6000, G loss: 0.5375455021858215, D loss: 0.0001420111075276509\n",
            "Iteration 5583/6000, G loss: 0.6016296744346619, D loss: 0.0001300461299251765\n",
            "Iteration 5584/6000, G loss: 0.45626625418663025, D loss: 0.02163686975836754\n",
            "Iteration 5585/6000, G loss: 0.41651129722595215, D loss: 0.006084054708480835\n",
            "Iteration 5586/6000, G loss: 0.6045771241188049, D loss: 0.00013418166781775653\n",
            "Iteration 5587/6000, G loss: 0.5467050671577454, D loss: 1.5032219380373135e-05\n",
            "Iteration 5588/6000, G loss: 0.4524407386779785, D loss: 0.0020919181406497955\n",
            "Iteration 5589/6000, G loss: 0.3940443694591522, D loss: 0.008430639281868935\n",
            "Iteration 5590/6000, G loss: 0.46689552068710327, D loss: 0.009887107647955418\n",
            "Iteration 5591/6000, G loss: 0.45413169264793396, D loss: 0.007633489556610584\n",
            "Iteration 5592/6000, G loss: 0.4298788011074066, D loss: 0.0028585020918399096\n",
            "Iteration 5593/6000, G loss: 0.4649166762828827, D loss: 0.004686586558818817\n",
            "Iteration 5594/6000, G loss: 0.4267377257347107, D loss: 0.0001332580577582121\n",
            "Iteration 5595/6000, G loss: 0.37114056944847107, D loss: 0.15337756276130676\n",
            "Iteration 5596/6000, G loss: 0.36406761407852173, D loss: 0.007503180298954248\n",
            "Iteration 5597/6000, G loss: 0.4701533317565918, D loss: 0.0030309672001749277\n",
            "Iteration 5598/6000, G loss: 0.5735579133033752, D loss: 0.0002901777916122228\n",
            "Iteration 5599/6000, G loss: 0.4972536861896515, D loss: 0.00046318949898704886\n",
            "Iteration 5600/6000, G loss: 0.46379655599594116, D loss: 0.0148000568151474\n",
            "Iteration 5601/6000, G loss: 0.5735977292060852, D loss: 0.0006597030442208052\n",
            "Iteration 5602/6000, G loss: 0.48187685012817383, D loss: 0.01473650336265564\n",
            "Iteration 5603/6000, G loss: 0.426327645778656, D loss: 0.005232141353189945\n",
            "Iteration 5604/6000, G loss: 0.5165122151374817, D loss: 0.0018973879050463438\n",
            "Iteration 5605/6000, G loss: 0.41268667578697205, D loss: 0.0010689276969060302\n",
            "Iteration 5606/6000, G loss: 0.4558562636375427, D loss: 0.01001773402094841\n",
            "Iteration 5607/6000, G loss: 0.5511868596076965, D loss: 0.01028323546051979\n",
            "Iteration 5608/6000, G loss: 0.44240760803222656, D loss: 0.06998555362224579\n",
            "Iteration 5609/6000, G loss: 0.5161874890327454, D loss: 0.016049381345510483\n",
            "Iteration 5610/6000, G loss: 0.37535691261291504, D loss: 0.004546592943370342\n",
            "Iteration 5611/6000, G loss: 0.4925884008407593, D loss: 0.0009254997130483389\n",
            "Iteration 5612/6000, G loss: 0.5456669926643372, D loss: 0.0013394479174166918\n",
            "Iteration 5613/6000, G loss: 0.4611717164516449, D loss: 1.6450616385554895e-05\n",
            "Iteration 5614/6000, G loss: 0.5212445259094238, D loss: 0.013292908668518066\n",
            "Iteration 5615/6000, G loss: 0.5542079210281372, D loss: 0.7012320756912231\n",
            "Iteration 5616/6000, G loss: 0.495478093624115, D loss: 0.1238645613193512\n",
            "Iteration 5617/6000, G loss: 0.5615809559822083, D loss: 0.003972351085394621\n",
            "Iteration 5618/6000, G loss: 0.362072616815567, D loss: 0.0011949019972234964\n",
            "Iteration 5619/6000, G loss: 0.4072108566761017, D loss: 0.007750293239951134\n",
            "Iteration 5620/6000, G loss: 0.4165802299976349, D loss: 0.0017206879565492272\n",
            "Iteration 5621/6000, G loss: 0.6045271158218384, D loss: 0.004193213302642107\n",
            "Iteration 5622/6000, G loss: 0.3766632378101349, D loss: 0.018431689590215683\n",
            "Iteration 5623/6000, G loss: 0.5183526873588562, D loss: 0.0016184807755053043\n",
            "Iteration 5624/6000, G loss: 0.5298810005187988, D loss: 0.1265062391757965\n",
            "Iteration 5625/6000, G loss: 0.58363938331604, D loss: 2.4008455511648208e-05\n",
            "Iteration 5626/6000, G loss: 0.6009746789932251, D loss: 7.674988592043519e-05\n",
            "Iteration 5627/6000, G loss: 0.4350527822971344, D loss: 0.0015984931960701942\n",
            "Iteration 5628/6000, G loss: 0.5448827743530273, D loss: 0.0004012153367511928\n",
            "Iteration 5629/6000, G loss: 0.534934401512146, D loss: 0.00017359020421281457\n",
            "Iteration 5630/6000, G loss: 0.41951391100883484, D loss: 0.001734356745146215\n",
            "Iteration 5631/6000, G loss: 0.6300215721130371, D loss: 0.042835086584091187\n",
            "Iteration 5632/6000, G loss: 0.5331376791000366, D loss: 0.763860821723938\n",
            "Iteration 5633/6000, G loss: 0.37651675939559937, D loss: 0.05510874465107918\n",
            "Iteration 5634/6000, G loss: 0.41944339871406555, D loss: 0.00010876658780034631\n",
            "Iteration 5635/6000, G loss: 0.5296985507011414, D loss: 0.0\n",
            "Iteration 5636/6000, G loss: 0.513195276260376, D loss: 1.7881392011531716e-07\n",
            "Iteration 5637/6000, G loss: 0.5239584445953369, D loss: 0.0003690945159178227\n",
            "Iteration 5638/6000, G loss: 0.4789138734340668, D loss: 0.00012561609037220478\n",
            "Iteration 5639/6000, G loss: 0.4976131319999695, D loss: 1.798258662223816\n",
            "Iteration 5640/6000, G loss: 0.41606566309928894, D loss: 2.810516834259033\n",
            "Iteration 5641/6000, G loss: 0.35488614439964294, D loss: 0.018927738070487976\n",
            "Iteration 5642/6000, G loss: 0.4321080446243286, D loss: 0.5797144174575806\n",
            "Iteration 5643/6000, G loss: 0.5008878707885742, D loss: 0.0002524281444493681\n",
            "Iteration 5644/6000, G loss: 0.4653612971305847, D loss: 0.0006020820001140237\n",
            "Iteration 5645/6000, G loss: 0.5034492611885071, D loss: 0.0009325628634542227\n",
            "Iteration 5646/6000, G loss: 0.5673622488975525, D loss: 0.0020921630784869194\n",
            "Iteration 5647/6000, G loss: 0.4499996602535248, D loss: 0.00042609882075339556\n",
            "Iteration 5648/6000, G loss: 0.44676655530929565, D loss: 0.004360769875347614\n",
            "Iteration 5649/6000, G loss: 0.5891292691230774, D loss: 0.03356363624334335\n",
            "Iteration 5650/6000, G loss: 0.48056167364120483, D loss: 0.011497782543301582\n",
            "Iteration 5651/6000, G loss: 0.49364912509918213, D loss: 0.00537010096013546\n",
            "Iteration 5652/6000, G loss: 0.5250167846679688, D loss: 0.01045689731836319\n",
            "Iteration 5653/6000, G loss: 0.5189598202705383, D loss: 0.0005787805421277881\n",
            "Iteration 5654/6000, G loss: 0.5715000033378601, D loss: 0.025135988369584084\n",
            "Iteration 5655/6000, G loss: 0.42583760619163513, D loss: 0.0391925573348999\n",
            "Iteration 5656/6000, G loss: 0.46530142426490784, D loss: 0.029371265321969986\n",
            "Iteration 5657/6000, G loss: 0.5668531656265259, D loss: 0.0045031411573290825\n",
            "Iteration 5658/6000, G loss: 0.47710978984832764, D loss: 0.05973755568265915\n",
            "Iteration 5659/6000, G loss: 0.5236905813217163, D loss: 0.005096178036183119\n",
            "Iteration 5660/6000, G loss: 0.4794902801513672, D loss: 0.0009807143360376358\n",
            "Iteration 5661/6000, G loss: 0.5264874696731567, D loss: 0.0027275467291474342\n",
            "Iteration 5662/6000, G loss: 0.36481553316116333, D loss: 0.0020950143225491047\n",
            "Iteration 5663/6000, G loss: 0.520878791809082, D loss: 0.002055118791759014\n",
            "Iteration 5664/6000, G loss: 0.4961184561252594, D loss: 0.006212877109646797\n",
            "Iteration 5665/6000, G loss: 0.40501487255096436, D loss: 0.029581382870674133\n",
            "Iteration 5666/6000, G loss: 0.48592445254325867, D loss: 0.03600156307220459\n",
            "Iteration 5667/6000, G loss: 0.4059232175350189, D loss: 0.06816413253545761\n",
            "Iteration 5668/6000, G loss: 0.44207337498664856, D loss: 0.0007233676733449101\n",
            "Iteration 5669/6000, G loss: 0.5048334002494812, D loss: 0.0047687930054962635\n",
            "Iteration 5670/6000, G loss: 0.5439545512199402, D loss: 0.08072567731142044\n",
            "Iteration 5671/6000, G loss: 0.4542030692100525, D loss: 0.016085904091596603\n",
            "Iteration 5672/6000, G loss: 0.5002292990684509, D loss: 0.00015206709213089198\n",
            "Iteration 5673/6000, G loss: 0.537877082824707, D loss: 0.017276175320148468\n",
            "Iteration 5674/6000, G loss: 0.5253639817237854, D loss: 0.004924622364342213\n",
            "Iteration 5675/6000, G loss: 0.5174508094787598, D loss: 0.0005113458610139787\n",
            "Iteration 5676/6000, G loss: 0.38751134276390076, D loss: 0.013577774167060852\n",
            "Iteration 5677/6000, G loss: 0.5761206746101379, D loss: 0.004840350244194269\n",
            "Iteration 5678/6000, G loss: 0.42930445075035095, D loss: 0.07174207270145416\n",
            "Iteration 5679/6000, G loss: 0.597026526927948, D loss: 0.005518531426787376\n",
            "Iteration 5680/6000, G loss: 0.4369870722293854, D loss: 0.004113051109015942\n",
            "Iteration 5681/6000, G loss: 0.40614429116249084, D loss: 0.0014574455562978983\n",
            "Iteration 5682/6000, G loss: 0.48792850971221924, D loss: 0.013882150873541832\n",
            "Iteration 5683/6000, G loss: 0.47195422649383545, D loss: 0.0004158527299296111\n",
            "Iteration 5684/6000, G loss: 0.6054368615150452, D loss: 0.003843646962195635\n",
            "Iteration 5685/6000, G loss: 0.4026033878326416, D loss: 0.06015074998140335\n",
            "Iteration 5686/6000, G loss: 0.458478718996048, D loss: 0.03303016722202301\n",
            "Iteration 5687/6000, G loss: 0.42906951904296875, D loss: 0.38225260376930237\n",
            "Iteration 5688/6000, G loss: 0.4309491217136383, D loss: 0.006519101094454527\n",
            "Iteration 5689/6000, G loss: 0.4254279136657715, D loss: 0.007420649752020836\n",
            "Iteration 5690/6000, G loss: 0.46321624517440796, D loss: 0.5265525579452515\n",
            "Iteration 5691/6000, G loss: 0.45220160484313965, D loss: 2.6072230339050293\n",
            "Iteration 5692/6000, G loss: 0.38561469316482544, D loss: 0.07116256654262543\n",
            "Iteration 5693/6000, G loss: 0.5101650953292847, D loss: 0.0015323921106755733\n",
            "Iteration 5694/6000, G loss: 0.49928826093673706, D loss: 0.046545930206775665\n",
            "Iteration 5695/6000, G loss: 0.565224289894104, D loss: 0.0012038322165608406\n",
            "Iteration 5696/6000, G loss: 0.5704398155212402, D loss: 0.00014575467503163964\n",
            "Iteration 5697/6000, G loss: 0.48845142126083374, D loss: 3.886216290993616e-06\n",
            "Iteration 5698/6000, G loss: 0.577957808971405, D loss: 0.007152152713388205\n",
            "Iteration 5699/6000, G loss: 0.5128536224365234, D loss: 4.736616392619908e-05\n",
            "Iteration 5700/6000, G loss: 0.5351590514183044, D loss: 0.00043743744026869535\n",
            "Iteration 5701/6000, G loss: 0.6156612634658813, D loss: 0.0004110541776753962\n",
            "Iteration 5702/6000, G loss: 0.45655810832977295, D loss: 1.3655362636200152e-05\n",
            "Iteration 5703/6000, G loss: 0.4931032061576843, D loss: 0.007783782668411732\n",
            "Iteration 5704/6000, G loss: 0.34814247488975525, D loss: 0.02090214192867279\n",
            "Iteration 5705/6000, G loss: 0.46610039472579956, D loss: 0.008683837950229645\n",
            "Iteration 5706/6000, G loss: 0.42524784803390503, D loss: 0.2726273536682129\n",
            "Iteration 5707/6000, G loss: 0.508752167224884, D loss: 0.00015564891509711742\n",
            "Iteration 5708/6000, G loss: 0.4796876907348633, D loss: 0.002008359180763364\n",
            "Iteration 5709/6000, G loss: 0.6638460755348206, D loss: 0.2852976322174072\n",
            "Iteration 5710/6000, G loss: 0.4855409562587738, D loss: 0.00013195929932408035\n",
            "Iteration 5711/6000, G loss: 0.5101746916770935, D loss: 0.003545141778886318\n",
            "Iteration 5712/6000, G loss: 0.5071764588356018, D loss: 0.01607353612780571\n",
            "Iteration 5713/6000, G loss: 0.5790566802024841, D loss: 0.0023568435572087765\n",
            "Iteration 5714/6000, G loss: 0.561355471611023, D loss: 0.0001233649963978678\n",
            "Iteration 5715/6000, G loss: 0.43421268463134766, D loss: 0.027568276971578598\n",
            "Iteration 5716/6000, G loss: 0.4136481285095215, D loss: 1.6384518146514893\n",
            "Iteration 5717/6000, G loss: 0.5561562776565552, D loss: 1.3931094408035278\n",
            "Iteration 5718/6000, G loss: 0.527429461479187, D loss: 0.018085982650518417\n",
            "Iteration 5719/6000, G loss: 0.4686447083950043, D loss: 0.0035769448149949312\n",
            "Iteration 5720/6000, G loss: 0.524735152721405, D loss: 1.299375253438484e-05\n",
            "Iteration 5721/6000, G loss: 0.5019170641899109, D loss: 1.1664582416415215e-05\n",
            "Iteration 5722/6000, G loss: 0.5055845975875854, D loss: 0.00034450326347723603\n",
            "Iteration 5723/6000, G loss: 0.46208930015563965, D loss: 0.014475514180958271\n",
            "Iteration 5724/6000, G loss: 0.51567542552948, D loss: 0.01907171681523323\n",
            "Iteration 5725/6000, G loss: 0.5403327345848083, D loss: 0.007419999688863754\n",
            "Iteration 5726/6000, G loss: 0.44229263067245483, D loss: 2.7537025744095445e-05\n",
            "Iteration 5727/6000, G loss: 0.3898243010044098, D loss: 0.0010331228841096163\n",
            "Iteration 5728/6000, G loss: 0.5836918354034424, D loss: 2.0360806956887245e-05\n",
            "Iteration 5729/6000, G loss: 0.4282446801662445, D loss: 0.0008702923078089952\n",
            "Iteration 5730/6000, G loss: 0.3607890009880066, D loss: 0.6756851673126221\n",
            "Iteration 5731/6000, G loss: 0.5226259827613831, D loss: 0.0008578860433772206\n",
            "Iteration 5732/6000, G loss: 0.4686328172683716, D loss: 0.0022260916884988546\n",
            "Iteration 5733/6000, G loss: 0.5346047282218933, D loss: 0.0302460715174675\n",
            "Iteration 5734/6000, G loss: 0.5795462131500244, D loss: 0.04873637855052948\n",
            "Iteration 5735/6000, G loss: 0.621286153793335, D loss: 0.002731568645685911\n",
            "Iteration 5736/6000, G loss: 0.4635588228702545, D loss: 0.0016596360364928842\n",
            "Iteration 5737/6000, G loss: 0.5091813802719116, D loss: 0.00021873757941648364\n",
            "Iteration 5738/6000, G loss: 0.4624357223510742, D loss: 2.708384272409603e-05\n",
            "Iteration 5739/6000, G loss: 0.5255804657936096, D loss: 0.009801109321415424\n",
            "Iteration 5740/6000, G loss: 0.5947964787483215, D loss: 0.009628568775951862\n",
            "Iteration 5741/6000, G loss: 0.47670120000839233, D loss: 0.006655476056039333\n",
            "Iteration 5742/6000, G loss: 0.5159013271331787, D loss: 0.050588347017765045\n",
            "Iteration 5743/6000, G loss: 0.5338610410690308, D loss: 0.009553211741149426\n",
            "Iteration 5744/6000, G loss: 0.5361387133598328, D loss: 0.007898155599832535\n",
            "Iteration 5745/6000, G loss: 0.37699341773986816, D loss: 0.01282145082950592\n",
            "Iteration 5746/6000, G loss: 0.46902427077293396, D loss: 0.09978213161230087\n",
            "Iteration 5747/6000, G loss: 0.493071585893631, D loss: 0.8724274635314941\n",
            "Iteration 5748/6000, G loss: 0.46017590165138245, D loss: 0.04582323879003525\n",
            "Iteration 5749/6000, G loss: 0.514643669128418, D loss: 1.241447925567627\n",
            "Iteration 5750/6000, G loss: 0.4632900357246399, D loss: 0.0010427667293697596\n",
            "Iteration 5751/6000, G loss: 0.48280400037765503, D loss: 0.00024412086349911988\n",
            "Iteration 5752/6000, G loss: 0.4059898853302002, D loss: 3.642378330230713\n",
            "Iteration 5753/6000, G loss: 0.4845670759677887, D loss: 0.012085928581655025\n",
            "Iteration 5754/6000, G loss: 0.608535885810852, D loss: 0.018088430166244507\n",
            "Iteration 5755/6000, G loss: 0.4976126253604889, D loss: 0.0014211987145245075\n",
            "Iteration 5756/6000, G loss: 0.5599595308303833, D loss: 0.00043753779027611017\n",
            "Iteration 5757/6000, G loss: 0.4828622341156006, D loss: 0.00043474300764501095\n",
            "Iteration 5758/6000, G loss: 0.379080593585968, D loss: 0.01441752165555954\n",
            "Iteration 5759/6000, G loss: 0.5521034598350525, D loss: 0.00022600704687647521\n",
            "Iteration 5760/6000, G loss: 0.5070191025733948, D loss: 0.0015722652897238731\n",
            "Iteration 5761/6000, G loss: 0.614563524723053, D loss: 0.0011766136158257723\n",
            "Iteration 5762/6000, G loss: 0.5405415296554565, D loss: 0.0010435752337798476\n",
            "Iteration 5763/6000, G loss: 0.4665480852127075, D loss: 0.001114946324378252\n",
            "Iteration 5764/6000, G loss: 0.5244149565696716, D loss: 0.003560183569788933\n",
            "Iteration 5765/6000, G loss: 0.47084444761276245, D loss: 0.027243267744779587\n",
            "Iteration 5766/6000, G loss: 0.5146226286888123, D loss: 0.1597251296043396\n",
            "Iteration 5767/6000, G loss: 0.4142223000526428, D loss: 0.4733009934425354\n",
            "Iteration 5768/6000, G loss: 0.5691462159156799, D loss: 0.0007619931129738688\n",
            "Iteration 5769/6000, G loss: 0.5082474946975708, D loss: 0.00012738080113194883\n",
            "Iteration 5770/6000, G loss: 0.49131637811660767, D loss: 0.00012785618309862912\n",
            "Iteration 5771/6000, G loss: 0.4696270227432251, D loss: 0.9300225377082825\n",
            "Iteration 5772/6000, G loss: 0.4643661677837372, D loss: 0.007563898805528879\n",
            "Iteration 5773/6000, G loss: 0.5747958421707153, D loss: 0.002795421751216054\n",
            "Iteration 5774/6000, G loss: 0.5055597424507141, D loss: 0.014097040519118309\n",
            "Iteration 5775/6000, G loss: 0.4815915524959564, D loss: 0.00018924276810139418\n",
            "Iteration 5776/6000, G loss: 0.49644362926483154, D loss: 3.9743215893395245e-05\n",
            "Iteration 5777/6000, G loss: 0.5583378076553345, D loss: 0.1979949027299881\n",
            "Iteration 5778/6000, G loss: 0.3978008031845093, D loss: 0.01945401355624199\n",
            "Iteration 5779/6000, G loss: 0.43712905049324036, D loss: 5.216979026794434\n",
            "Iteration 5780/6000, G loss: 0.5611295700073242, D loss: 0.007421523332595825\n",
            "Iteration 5781/6000, G loss: 0.477582722902298, D loss: 0.0002707949315663427\n",
            "Iteration 5782/6000, G loss: 0.599299430847168, D loss: 0.005945994984358549\n",
            "Iteration 5783/6000, G loss: 0.5467070937156677, D loss: 0.00029042549431324005\n",
            "Iteration 5784/6000, G loss: 0.39612048864364624, D loss: 0.009860278107225895\n",
            "Iteration 5785/6000, G loss: 0.4986291527748108, D loss: 0.004048836417496204\n",
            "Iteration 5786/6000, G loss: 0.41603562235832214, D loss: 0.030794966965913773\n",
            "Iteration 5787/6000, G loss: 0.4863560199737549, D loss: 0.007536170072853565\n",
            "Iteration 5788/6000, G loss: 0.42964160442352295, D loss: 0.01067129522562027\n",
            "Iteration 5789/6000, G loss: 0.46848657727241516, D loss: 0.009447487071156502\n",
            "Iteration 5790/6000, G loss: 0.5150430202484131, D loss: 0.008329415693879128\n",
            "Iteration 5791/6000, G loss: 0.493312805891037, D loss: 0.0056690070778131485\n",
            "Iteration 5792/6000, G loss: 0.41190701723098755, D loss: 0.01929793879389763\n",
            "Iteration 5793/6000, G loss: 0.4569835662841797, D loss: 0.04773373156785965\n",
            "Iteration 5794/6000, G loss: 0.47140252590179443, D loss: 0.006692325696349144\n",
            "Iteration 5795/6000, G loss: 0.4081047773361206, D loss: 0.08283863961696625\n",
            "Iteration 5796/6000, G loss: 0.4513304829597473, D loss: 0.07306262850761414\n",
            "Iteration 5797/6000, G loss: 0.3922053575515747, D loss: 0.00583068560808897\n",
            "Iteration 5798/6000, G loss: 0.5229243040084839, D loss: 0.0019006384536623955\n",
            "Iteration 5799/6000, G loss: 0.48196685314178467, D loss: 0.0012016566470265388\n",
            "Iteration 5800/6000, G loss: 0.5212497711181641, D loss: 0.00019118150521535426\n",
            "Iteration 5801/6000, G loss: 0.40205392241477966, D loss: 0.0057578980922698975\n",
            "Iteration 5802/6000, G loss: 0.42163270711898804, D loss: 0.04806976765394211\n",
            "Iteration 5803/6000, G loss: 0.5184253454208374, D loss: 0.0768725723028183\n",
            "Iteration 5804/6000, G loss: 0.43794068694114685, D loss: 0.006295525468885899\n",
            "Iteration 5805/6000, G loss: 0.5074747204780579, D loss: 0.007502609398216009\n",
            "Iteration 5806/6000, G loss: 0.397663950920105, D loss: 0.009954545646905899\n",
            "Iteration 5807/6000, G loss: 0.48389822244644165, D loss: 0.004465815611183643\n",
            "Iteration 5808/6000, G loss: 0.4115787446498871, D loss: 0.03610201179981232\n",
            "Iteration 5809/6000, G loss: 0.5075864791870117, D loss: 0.0009107509977184236\n",
            "Iteration 5810/6000, G loss: 0.4553227126598358, D loss: 0.015483211725950241\n",
            "Iteration 5811/6000, G loss: 0.5189096331596375, D loss: 0.000573814264498651\n",
            "Iteration 5812/6000, G loss: 0.5307102203369141, D loss: 0.0011515732621774077\n",
            "Iteration 5813/6000, G loss: 0.4753447473049164, D loss: 0.0007718103006482124\n",
            "Iteration 5814/6000, G loss: 0.5259724259376526, D loss: 0.0176701657474041\n",
            "Iteration 5815/6000, G loss: 0.5207211375236511, D loss: 0.0007574105402454734\n",
            "Iteration 5816/6000, G loss: 0.5640836954116821, D loss: 0.003971263766288757\n",
            "Iteration 5817/6000, G loss: 0.5496956706047058, D loss: 0.0016703060828149319\n",
            "Iteration 5818/6000, G loss: 0.4294547438621521, D loss: 0.008186280727386475\n",
            "Iteration 5819/6000, G loss: 0.42197078466415405, D loss: 0.01011602021753788\n",
            "Iteration 5820/6000, G loss: 0.5671827793121338, D loss: 0.0010447187814861536\n",
            "Iteration 5821/6000, G loss: 0.4363180100917816, D loss: 0.007016396149992943\n",
            "Iteration 5822/6000, G loss: 0.4916974604129791, D loss: 0.0005261760670691729\n",
            "Iteration 5823/6000, G loss: 0.48986226320266724, D loss: 0.0002743039221968502\n",
            "Iteration 5824/6000, G loss: 0.5884674787521362, D loss: 0.007432260550558567\n",
            "Iteration 5825/6000, G loss: 0.401380330324173, D loss: 0.0015020614955574274\n",
            "Iteration 5826/6000, G loss: 0.4380891025066376, D loss: 0.0026896747294813395\n",
            "Iteration 5827/6000, G loss: 0.478148490190506, D loss: 0.007019568234682083\n",
            "Iteration 5828/6000, G loss: 0.4808152914047241, D loss: 0.007245965767651796\n",
            "Iteration 5829/6000, G loss: 0.47470104694366455, D loss: 0.01674097590148449\n",
            "Iteration 5830/6000, G loss: 0.45409098267555237, D loss: 0.0072080516256392\n",
            "Iteration 5831/6000, G loss: 0.47043025493621826, D loss: 0.016934793442487717\n",
            "Iteration 5832/6000, G loss: 0.4859465956687927, D loss: 0.0045584505423903465\n",
            "Iteration 5833/6000, G loss: 0.40627023577690125, D loss: 0.04655345529317856\n",
            "Iteration 5834/6000, G loss: 0.4933316111564636, D loss: 0.0016188719309866428\n",
            "Iteration 5835/6000, G loss: 0.42365339398384094, D loss: 0.04597403481602669\n",
            "Iteration 5836/6000, G loss: 0.47185415029525757, D loss: 0.0008488928433507681\n",
            "Iteration 5837/6000, G loss: 0.5752943754196167, D loss: 0.005772063508629799\n",
            "Iteration 5838/6000, G loss: 0.4614056944847107, D loss: 0.02237304300069809\n",
            "Iteration 5839/6000, G loss: 0.45665398240089417, D loss: 0.03744836896657944\n",
            "Iteration 5840/6000, G loss: 0.4323539733886719, D loss: 0.11714107543230057\n",
            "Iteration 5841/6000, G loss: 0.40202873945236206, D loss: 0.012104718945920467\n",
            "Iteration 5842/6000, G loss: 0.49208658933639526, D loss: 0.01564529910683632\n",
            "Iteration 5843/6000, G loss: 0.6015452742576599, D loss: 0.0005593870300799608\n",
            "Iteration 5844/6000, G loss: 0.4054052531719208, D loss: 0.0062294891104102135\n",
            "Iteration 5845/6000, G loss: 0.5779350996017456, D loss: 0.0008261059410870075\n",
            "Iteration 5846/6000, G loss: 0.5458816885948181, D loss: 0.024481944739818573\n",
            "Iteration 5847/6000, G loss: 0.4672575891017914, D loss: 0.004734416492283344\n",
            "Iteration 5848/6000, G loss: 0.4915798604488373, D loss: 0.04918333888053894\n",
            "Iteration 5849/6000, G loss: 0.48686620593070984, D loss: 0.0068443757481873035\n",
            "Iteration 5850/6000, G loss: 0.4092619717121124, D loss: 0.001084909774363041\n",
            "Iteration 5851/6000, G loss: 0.4368184506893158, D loss: 0.019657883793115616\n",
            "Iteration 5852/6000, G loss: 0.4590832591056824, D loss: 0.12363415956497192\n",
            "Iteration 5853/6000, G loss: 0.5251662135124207, D loss: 0.001801246078684926\n",
            "Iteration 5854/6000, G loss: 0.44981929659843445, D loss: 0.007410629652440548\n",
            "Iteration 5855/6000, G loss: 0.5852341055870056, D loss: 0.090674489736557\n",
            "Iteration 5856/6000, G loss: 0.46013882756233215, D loss: 0.024111811071634293\n",
            "Iteration 5857/6000, G loss: 0.5553462505340576, D loss: 0.0006142860511317849\n",
            "Iteration 5858/6000, G loss: 0.45775270462036133, D loss: 0.002784379292279482\n",
            "Iteration 5859/6000, G loss: 0.5359459519386292, D loss: 0.0014475779607892036\n",
            "Iteration 5860/6000, G loss: 0.5277484655380249, D loss: 0.035630423575639725\n",
            "Iteration 5861/6000, G loss: 0.3793531358242035, D loss: 0.41481900215148926\n",
            "Iteration 5862/6000, G loss: 0.44576919078826904, D loss: 0.14277613162994385\n",
            "Iteration 5863/6000, G loss: 0.5735073089599609, D loss: 0.08262085914611816\n",
            "Iteration 5864/6000, G loss: 0.6428013443946838, D loss: 0.056775689125061035\n",
            "Iteration 5865/6000, G loss: 0.4835631549358368, D loss: 2.440751632093452e-05\n",
            "Iteration 5866/6000, G loss: 0.5197665095329285, D loss: 0.0001891313586384058\n",
            "Iteration 5867/6000, G loss: 0.4284123480319977, D loss: 2.890089511871338\n",
            "Iteration 5868/6000, G loss: 0.5978373885154724, D loss: 0.03126196563243866\n",
            "Iteration 5869/6000, G loss: 0.4728492200374603, D loss: 0.8814333081245422\n",
            "Iteration 5870/6000, G loss: 0.3322303593158722, D loss: 0.02146013081073761\n",
            "Iteration 5871/6000, G loss: 0.3994530737400055, D loss: 0.0016639834502711892\n",
            "Iteration 5872/6000, G loss: 0.39008504152297974, D loss: 2.563164710998535\n",
            "Iteration 5873/6000, G loss: 0.4343545436859131, D loss: 0.00128357601352036\n",
            "Iteration 5874/6000, G loss: 0.47972410917282104, D loss: 0.010137935169041157\n",
            "Iteration 5875/6000, G loss: 0.4557300806045532, D loss: 0.0006735475617460907\n",
            "Iteration 5876/6000, G loss: 0.4633077383041382, D loss: 0.014704206958413124\n",
            "Iteration 5877/6000, G loss: 0.49653011560440063, D loss: 9.428428893443197e-05\n",
            "Iteration 5878/6000, G loss: 0.5097890496253967, D loss: 0.0006795841036364436\n",
            "Iteration 5879/6000, G loss: 0.4409828186035156, D loss: 0.0038717431016266346\n",
            "Iteration 5880/6000, G loss: 0.4376010298728943, D loss: 0.6086543798446655\n",
            "Iteration 5881/6000, G loss: 0.49592605233192444, D loss: 0.08590099215507507\n",
            "Iteration 5882/6000, G loss: 0.5092343091964722, D loss: 1.0550966262817383\n",
            "Iteration 5883/6000, G loss: 0.5462246537208557, D loss: 0.04169628769159317\n",
            "Iteration 5884/6000, G loss: 0.3771560788154602, D loss: 0.011053720489144325\n",
            "Iteration 5885/6000, G loss: 0.4840545058250427, D loss: 0.02392859011888504\n",
            "Iteration 5886/6000, G loss: 0.5027545690536499, D loss: 0.008416062220931053\n",
            "Iteration 5887/6000, G loss: 0.3964945077896118, D loss: 0.0019433568231761456\n",
            "Iteration 5888/6000, G loss: 0.44507330656051636, D loss: 0.0016819965094327927\n",
            "Iteration 5889/6000, G loss: 0.49236753582954407, D loss: 0.0004724198952317238\n",
            "Iteration 5890/6000, G loss: 0.4547669291496277, D loss: 0.002072556409984827\n",
            "Iteration 5891/6000, G loss: 0.4617190957069397, D loss: 0.0115604093298316\n",
            "Iteration 5892/6000, G loss: 0.4481329917907715, D loss: 0.001284051570110023\n",
            "Iteration 5893/6000, G loss: 0.6030430197715759, D loss: 0.0007779343286529183\n",
            "Iteration 5894/6000, G loss: 0.4432941675186157, D loss: 0.0037801596336066723\n",
            "Iteration 5895/6000, G loss: 0.4235076308250427, D loss: 0.0006428657798096538\n",
            "Iteration 5896/6000, G loss: 0.3771095871925354, D loss: 0.003385547548532486\n",
            "Iteration 5897/6000, G loss: 0.5339658856391907, D loss: 0.0007326052291318774\n",
            "Iteration 5898/6000, G loss: 0.484266072511673, D loss: 0.0022953315638005733\n",
            "Iteration 5899/6000, G loss: 0.45173799991607666, D loss: 0.024819165468215942\n",
            "Iteration 5900/6000, G loss: 0.4316134452819824, D loss: 0.0006256407359614968\n",
            "Iteration 5901/6000, G loss: 0.47045475244522095, D loss: 0.011637959629297256\n",
            "Iteration 5902/6000, G loss: 0.4884375333786011, D loss: 0.010927222669124603\n",
            "Iteration 5903/6000, G loss: 0.5180125832557678, D loss: 0.10372761636972427\n",
            "Iteration 5904/6000, G loss: 0.5848062038421631, D loss: 0.007832772098481655\n",
            "Iteration 5905/6000, G loss: 0.4281652271747589, D loss: 0.10660980641841888\n",
            "Iteration 5906/6000, G loss: 0.47024205327033997, D loss: 0.0030350431334227324\n",
            "Iteration 5907/6000, G loss: 0.5087875723838806, D loss: 0.017765481024980545\n",
            "Iteration 5908/6000, G loss: 0.5393798351287842, D loss: 4.59705843240954e-05\n",
            "Iteration 5909/6000, G loss: 0.5336242318153381, D loss: 0.00033433857606723905\n",
            "Iteration 5910/6000, G loss: 0.4714966118335724, D loss: 0.01622837409377098\n",
            "Iteration 5911/6000, G loss: 0.46638983488082886, D loss: 0.015117805451154709\n",
            "Iteration 5912/6000, G loss: 0.5006400942802429, D loss: 0.005516733042895794\n",
            "Iteration 5913/6000, G loss: 0.49756374955177307, D loss: 0.0029793945141136646\n",
            "Iteration 5914/6000, G loss: 0.4439426064491272, D loss: 0.0030271364375948906\n",
            "Iteration 5915/6000, G loss: 0.48746258020401, D loss: 0.005992372520267963\n",
            "Iteration 5916/6000, G loss: 0.46045219898223877, D loss: 0.0014606539625674486\n",
            "Iteration 5917/6000, G loss: 0.44590309262275696, D loss: 0.005808604881167412\n",
            "Iteration 5918/6000, G loss: 0.49306708574295044, D loss: 0.054808929562568665\n",
            "Iteration 5919/6000, G loss: 0.5117163062095642, D loss: 0.019904932007193565\n",
            "Iteration 5920/6000, G loss: 0.5366524457931519, D loss: 0.0019528024131432176\n",
            "Iteration 5921/6000, G loss: 0.44673725962638855, D loss: 0.009319807402789593\n",
            "Iteration 5922/6000, G loss: 0.4993302822113037, D loss: 0.00587583240121603\n",
            "Iteration 5923/6000, G loss: 0.3862656056880951, D loss: 0.08604812622070312\n",
            "Iteration 5924/6000, G loss: 0.5136784315109253, D loss: 0.0021650618873536587\n",
            "Iteration 5925/6000, G loss: 0.44216907024383545, D loss: 0.006280425004661083\n",
            "Iteration 5926/6000, G loss: 0.4808201491832733, D loss: 0.011043861508369446\n",
            "Iteration 5927/6000, G loss: 0.4363117814064026, D loss: 0.29964184761047363\n",
            "Iteration 5928/6000, G loss: 0.4563600420951843, D loss: 1.5502240657806396\n",
            "Iteration 5929/6000, G loss: 0.46010735630989075, D loss: 0.10327626019716263\n",
            "Iteration 5930/6000, G loss: 0.415531724691391, D loss: 0.0013938173651695251\n",
            "Iteration 5931/6000, G loss: 0.5061062574386597, D loss: 0.0035453978925943375\n",
            "Iteration 5932/6000, G loss: 0.4150061011314392, D loss: 0.0020307418890297413\n",
            "Iteration 5933/6000, G loss: 0.5591325759887695, D loss: 0.000123805963085033\n",
            "Iteration 5934/6000, G loss: 0.47265633940696716, D loss: 0.11411518603563309\n",
            "Iteration 5935/6000, G loss: 0.5424844026565552, D loss: 0.48594075441360474\n",
            "Iteration 5936/6000, G loss: 0.5040704011917114, D loss: 0.004647685214877129\n",
            "Iteration 5937/6000, G loss: 0.4585533142089844, D loss: 0.00010764804756036028\n",
            "Iteration 5938/6000, G loss: 0.5097907185554504, D loss: 0.0016029654070734978\n",
            "Iteration 5939/6000, G loss: 0.4676266610622406, D loss: 0.002047548536211252\n",
            "Iteration 5940/6000, G loss: 0.3895154595375061, D loss: 0.0010326452320441604\n",
            "Iteration 5941/6000, G loss: 0.5304644107818604, D loss: 0.40761643648147583\n",
            "Iteration 5942/6000, G loss: 0.49208417534828186, D loss: 0.0025185015983879566\n",
            "Iteration 5943/6000, G loss: 0.5581036806106567, D loss: 0.0019270863849669695\n",
            "Iteration 5944/6000, G loss: 0.5768494606018066, D loss: 7.593615009682253e-06\n",
            "Iteration 5945/6000, G loss: 0.5155051946640015, D loss: 0.0011930752079933882\n",
            "Iteration 5946/6000, G loss: 0.4248122572898865, D loss: 0.00011565117165446281\n",
            "Iteration 5947/6000, G loss: 0.4419235289096832, D loss: 0.5197398066520691\n",
            "Iteration 5948/6000, G loss: 0.532451868057251, D loss: 0.1046660989522934\n",
            "Iteration 5949/6000, G loss: 0.5972358584403992, D loss: 6.890279564686352e-06\n",
            "Iteration 5950/6000, G loss: 0.5182114243507385, D loss: 0.012260040268301964\n",
            "Iteration 5951/6000, G loss: 0.42833322286605835, D loss: 0.0017382112564519048\n",
            "Iteration 5952/6000, G loss: 0.39080166816711426, D loss: 6.369822222040966e-05\n",
            "Iteration 5953/6000, G loss: 0.503401517868042, D loss: 0.46012306213378906\n",
            "Iteration 5954/6000, G loss: 0.3713144063949585, D loss: 0.005130358040332794\n",
            "Iteration 5955/6000, G loss: 0.6162807941436768, D loss: 0.0008775106398388743\n",
            "Iteration 5956/6000, G loss: 0.4660576581954956, D loss: 0.000760941649787128\n",
            "Iteration 5957/6000, G loss: 0.5332725644111633, D loss: 1.6122918168548495e-05\n",
            "Iteration 5958/6000, G loss: 0.49995851516723633, D loss: 0.004394111223518848\n",
            "Iteration 5959/6000, G loss: 0.46984514594078064, D loss: 4.410736437421292e-06\n",
            "Iteration 5960/6000, G loss: 0.5132567882537842, D loss: 3.59828227374237e-05\n",
            "Iteration 5961/6000, G loss: 0.5483350157737732, D loss: 0.01405940018594265\n",
            "Iteration 5962/6000, G loss: 0.5289934873580933, D loss: 0.3526495695114136\n",
            "Iteration 5963/6000, G loss: 0.42517346143722534, D loss: 0.4049312174320221\n",
            "Iteration 5964/6000, G loss: 0.5384567975997925, D loss: 0.0003729119198396802\n",
            "Iteration 5965/6000, G loss: 0.4758703112602234, D loss: 3.0219287509680726e-05\n",
            "Iteration 5966/6000, G loss: 0.4106425344944, D loss: 0.063921719789505\n",
            "Iteration 5967/6000, G loss: 0.4630575180053711, D loss: 0.0012989114038646221\n",
            "Iteration 5968/6000, G loss: 0.5665004849433899, D loss: 0.0008074482320807874\n",
            "Iteration 5969/6000, G loss: 0.5029574632644653, D loss: 6.900719017721713e-05\n",
            "Iteration 5970/6000, G loss: 0.44419851899147034, D loss: 0.5728827714920044\n",
            "Iteration 5971/6000, G loss: 0.5197314620018005, D loss: 0.0015364335849881172\n",
            "Iteration 5972/6000, G loss: 0.5548028945922852, D loss: 7.13238914613612e-05\n",
            "Iteration 5973/6000, G loss: 0.462224543094635, D loss: 0.00017054349882528186\n",
            "Iteration 5974/6000, G loss: 0.37580224871635437, D loss: 0.0034012203104794025\n",
            "Iteration 5975/6000, G loss: 0.573575496673584, D loss: 0.36024338006973267\n",
            "Iteration 5976/6000, G loss: 0.38173797726631165, D loss: 5.855034828186035\n",
            "Iteration 5977/6000, G loss: 0.5789766311645508, D loss: 0.0033943194430321455\n",
            "Iteration 5978/6000, G loss: 0.47078993916511536, D loss: 0.0026336894370615482\n",
            "Iteration 5979/6000, G loss: 0.4786703884601593, D loss: 0.00763721251860261\n",
            "Iteration 5980/6000, G loss: 0.5645647048950195, D loss: 0.0003394132072571665\n",
            "Iteration 5981/6000, G loss: 0.4542228579521179, D loss: 0.014036307111382484\n",
            "Iteration 5982/6000, G loss: 0.491779625415802, D loss: 0.08185562491416931\n",
            "Iteration 5983/6000, G loss: 0.4591102600097656, D loss: 0.030930515378713608\n",
            "Iteration 5984/6000, G loss: 0.5429502129554749, D loss: 0.009421244263648987\n",
            "Iteration 5985/6000, G loss: 0.5091946125030518, D loss: 0.006990490481257439\n",
            "Iteration 5986/6000, G loss: 0.42180734872817993, D loss: 0.16024845838546753\n",
            "Iteration 5987/6000, G loss: 0.45732641220092773, D loss: 0.02198806032538414\n",
            "Iteration 5988/6000, G loss: 0.4378509521484375, D loss: 0.004186203703284264\n",
            "Iteration 5989/6000, G loss: 0.48677849769592285, D loss: 0.018809806555509567\n",
            "Iteration 5990/6000, G loss: 0.4570169150829315, D loss: 0.10362206399440765\n",
            "Iteration 5991/6000, G loss: 0.44894638657569885, D loss: 0.07125023007392883\n",
            "Iteration 5992/6000, G loss: 0.41724586486816406, D loss: 0.021126270294189453\n",
            "Iteration 5993/6000, G loss: 0.5061333179473877, D loss: 0.016351846978068352\n",
            "Iteration 5994/6000, G loss: 0.5530784130096436, D loss: 0.002903927583247423\n",
            "Iteration 5995/6000, G loss: 0.408263623714447, D loss: 0.0477747842669487\n",
            "Iteration 5996/6000, G loss: 0.40355926752090454, D loss: 0.003624288598075509\n",
            "Iteration 5997/6000, G loss: 0.6311366558074951, D loss: 0.0027553988620638847\n",
            "Iteration 5998/6000, G loss: 0.4163576662540436, D loss: 0.0845763310790062\n",
            "Iteration 5999/6000, G loss: 0.539710283279419, D loss: 0.01974065974354744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:342: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_ESRGAN(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "csc413_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}